{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m7jN0d3qJZZK","outputId":"c7305625-8dc5-44aa-9671-460a3e24d715","executionInfo":{"status":"ok","timestamp":1750460819053,"user_tz":-60,"elapsed":26621,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Importing drive model from the google.colab package\n","from google.colab import drive\n","\n","#Mounting the google drive to a specific path\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_8kLZ9NLAgy","outputId":"d2d8ddd4-8ddf-4d66-bf7f-1a9d30aefb33","executionInfo":{"status":"ok","timestamp":1750460837100,"user_tz":-60,"elapsed":1391,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection\n"]}]},{"cell_type":"code","source":["# Configuring Git user details\n","!git config --global user.email \"dorothy.sarpongk@gmail.com\"\n","!git config --global user.name \"01DorSarpong\""],"metadata":{"id":"sdbbjCzE0tWT","executionInfo":{"status":"ok","timestamp":1750460858230,"user_tz":-60,"elapsed":4025,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Importing libraries for code\n","\n","import pandas as pd\n","import numpy  as np\n","import yfinance as yf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import IsolationForest\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.cluster import DBSCAN\n","from tqdm import tqdm\n","from typing import Tuple, Union\n"],"metadata":{"id":"jQ_wBKNkmV25","executionInfo":{"status":"ok","timestamp":1750461969860,"user_tz":-60,"elapsed":41,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# A function to download and save FTSE 100 and FTSE 250 stocks\n","\n","def download_and_save_FTSE_stocks(tickers: list, start_date: str, end_date: str, directory: str):\n","\n"," \"\"\" This function downloads historical stock data for a list of tickers and saves it to CSV files.\n","\n","  Args:\n","    tickers_list (list): A list of stock ticker symbols (e.g., ['TSCO.L', 'BARC.L']).\n","    start_date (str): The start date for data download in 'YYYY-MM-DD' format.\n","    end_date (str): The end date for data download in 'YYYY-MM-DD' format.\n","    directory (str): The path to the directory where CSV files will be saved.\n","  \"\"\"\n","\n","  # Ensure the save directory exists\n"," if not os.path.exists(directory):\n","    os.makedirs(directory)\n","    print(f\"Created directory: {directory}\")\n","\n"," print(f\"Starting download for {len(tickers)} tickers from {start_date} to {end_date}...\")\n","\n"," for ticker in tqdm(tickers, desc=\"Downloading Stocks\"):\n","    # Format the filename: remove '.L' and add date range for clarity\n","    cleaned_ticker = ticker.replace('.L', '')\n","    file_name = f\"{cleaned_ticker}_{start_date.replace('-', '')}_{end_date.replace('-', '')}.csv\"\n","    full_file_path = os.path.join(directory, file_name)\n","\n","    try:\n","      df = yf.download(ticker, start=start_date, end=end_date, auto_adjust=False)\n","\n","      if not df.empty:\n","        df.to_csv(full_file_path)\n","        # print(f\"✅ Saved data for {ticker} to {full_file_path}\") # Optional: uncomment for more verbose output\n","      else:\n","        print(f\"⚠️ No data available for {ticker} for the specified period.\")\n","    except Exception as e:\n","      print(f\"❌ Error downloading or saving data for {ticker}: {e}\")\n","\n"," print(\"Download process completed.\")\n"],"metadata":{"id":"6Iby76j7PtZR","executionInfo":{"status":"ok","timestamp":1750379813375,"user_tz":-60,"elapsed":88,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#Creating a list of FTSE 100 and FTSE 250 tickers\n","\n","FTSE_100_tickers = [\"AZN.L\", \"HSBA.L\", \"ULVR.L\", \"REL.L\", \"BATS.L\", \"BP.L\", \"GSK.L\", \"DGE.L\",\n","                   \"RR.L\", \"NG.L\", \"BARC.L\", \"TSCO.L\", \"PRU.L\", \"BHP.L\", \"BT-A.L\",]\n","\n","FTSE_250_tickers = [\"BWY.L\", \"EMG.L\", \"JUST.L\", \"SXS.L\", \"CKN.L\", \"LRE.L\", \"RAT.L\", \"THG.L\",\n","                    \"JDW.L\", \"SCT.L\", \"DOM.L\", \"SRE.L\", \"HIK.L\", \"ICGT.L\", \"HSX.L\"]"],"metadata":{"id":"2L4EdTEsuCu8","executionInfo":{"status":"ok","timestamp":1750460885353,"user_tz":-60,"elapsed":43,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#Defining the period for stocks range\n","start_date = \"2014-01-01\"\n","end_date = \"2024-12-31\""],"metadata":{"id":"yESfV6rzUuQ1","executionInfo":{"status":"ok","timestamp":1750460887813,"user_tz":-60,"elapsed":6,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Defining the path to save the CSVs\n","\n","ftse_100_path = '/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection/FTSE_100'\n","ftse_250_path = '/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection/FTSE_250'\n"],"metadata":{"id":"w9mFm0wLVH6v","executionInfo":{"status":"ok","timestamp":1750460889190,"user_tz":-60,"elapsed":31,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Calling the function for FTSE 100 and FTSE 250 tickers\n","\n","download_and_save_FTSE_stocks(\n","    tickers=FTSE_100_tickers,\n","    start_date=start_date,\n","    end_date=end_date,\n","    directory=ftse_100_path\n",")\n","\n","download_and_save_FTSE_stocks(\n","    tickers=FTSE_250_tickers,\n","    start_date=start_date,\n","    end_date=end_date,\n","    directory=ftse_250_path\n",")"],"metadata":{"id":"dnZIuB7oXMSA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a function to load stocks and pre-process into a dataframe\n","\n","def load_and_structure_stock_data(folder_path: str) -> pd.DataFrame:\n","    \"\"\"\n","    Loads historical stock data from CSV files in a specified folder,\n","    cleans, processes, filters to weekdays, fills NaNs/gaps, and\n","    consolidates them into a single structured DataFrame with a MultiIndex.\n","\n","    Args:\n","        folder_path (str): The path to the directory containing stock data CSVs.\n","\n","    Returns:\n","        pd.DataFrame: A single DataFrame containing data for all tickers,\n","                      indexed by 'Date' and 'Ticker', sorted by Date and then Ticker.\n","                      Returns an empty DataFrame if no data is loaded or processed.\n","    \"\"\"\n","    all_dfs = []\n","\n","    if not os.path.exists(folder_path):\n","        print(f\"❌ Error: Folder not found at {folder_path}\")\n","        return pd.DataFrame() # Return empty DataFrame if folder doesn't exist\n","\n","    # Get list of CSV files to process\n","    file_list = [f.name for f in os.scandir(folder_path) if f.name.endswith(\".csv\")]\n","\n","    if not file_list:\n","        print(f\"⚠️ No CSV files found in {folder_path}\")\n","        return pd.DataFrame()\n","\n","    print(f\"Loading and processing data from {len(file_list)} CSV files in {folder_path}...\")\n","\n","    # Define columns that typically contain numerical stock data to be filled\n","    numerical_cols_to_fill = ['open', 'high', 'low', 'close', 'adj close', 'volume']\n","\n","    for filename in tqdm(file_list, desc=\"Processing Stock Files\"):\n","        file_path = os.path.join(folder_path, filename)\n","\n","        try:\n","            # Read CSV: use first row as header, skip second row (often contains ticker name repeated)\n","            df = pd.read_csv(file_path, header=0, skiprows=[1], encoding='utf-8-sig')\n","\n","            # --- Initial Cleaning and Date Conversion ---\n","            # Rename the first column to 'Date' if it's not already\n","            if df.columns[0].strip().lower() != 'date':\n","                df.rename(columns={df.columns[0]: 'Date'}, inplace=True)\n","\n","            # Normalize all column names to lowercase and remove leading/trailing spaces\n","            df.columns = [col.strip().lower() for col in df.columns]\n","\n","            # Clean and prepare the 'date' column\n","            df['date'] = df['date'].astype(str).str.strip()\n","            df = df[df['date'].str.lower() != 'date'] # Drop any rows where the date column contains the string 'Date'\n","\n","            # Convert to datetime, coercing errors will turn unparseable dates into NaT\n","            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n","            df.dropna(subset=['date'], inplace=True) # Drop rows where date conversion failed (NaT)\n","\n","            # --- Add Ticker Information ---\n","            # Extract ticker from filename (e.g., 'TSCO_20140101_20241231.csv' -> 'TSCO')\n","            ticker_symbol = filename.split('_')[0]\n","            df['ticker'] = ticker_symbol\n","\n","            # --- Set Date as Index (temporarily for filtering/filling) ---\n","            df.set_index('date', inplace=True)\n","\n","            # --- Filter for Weekdays Only ---\n","            # .dayofweek returns Monday=0, ..., Sunday=6. Keep only 0 to 4.\n","            df = df[df.index.dayofweek < 5]\n","\n","            # --- Fill NaNs/Gaps for Numerical Columns ---\n","            # Identify columns to fill that actually exist in the current DataFrame\n","            existing_numerical_cols = [col for col in numerical_cols_to_fill if col in df.columns]\n","\n","            if not existing_numerical_cols:\n","                # print(f\"⚠️ No numerical columns found for {filename}. Skipping NaN filling for this file.\")\n","                pass # This is fine, just means the file might only have non-numerical data or very specific columns\n","\n","            # Apply forward-fill then backward-fill for numerical columns within this ticker's data\n","            # This handles gaps for individual stock series\n","            df[existing_numerical_cols] = df[existing_numerical_cols].ffill().bfill()\n","\n","            # --- Final Check for essential data after filling ---\n","            # If 'open' column is vital and still has NaNs (e.g., entire series was NaN), drop those rows\n","            if 'open' in df.columns:\n","                df.dropna(subset=['open'], inplace=True)\n","            else:\n","                print(f\"⚠️ 'open' column not found in {filename}. Skipping this file as essential data is missing.\")\n","                continue # Skip this file if 'open' is genuinely missing\n","\n","            if df.empty:\n","                print(f\"⚠️ No valid weekday data remaining for {filename} after filtering. Skipping.\")\n","                continue\n","\n","            all_dfs.append(df)\n","\n","        except pd.errors.EmptyDataError:\n","            print(f\"⚠️ {filename} is empty. Skipping.\")\n","        except pd.errors.ParserError as e:\n","            print(f\"❌ Error parsing {filename}: {type(e).__name__} - {e}. Skipping.\")\n","        except Exception as e:\n","            print(f\"❌ Error processing {filename}: {type(e).__name__} - {e}. Skipping.\")\n","\n","    if not all_dfs:\n","        print(\"No valid stock data loaded after processing. Returning empty DataFrame.\")\n","        return pd.DataFrame()\n","\n","    # --- Consolidate all DataFrames into one structured DataFrame ---\n","    combined_df = pd.concat(all_dfs)\n","\n","    # Set a MultiIndex: primary index is 'Date', secondary is 'ticker'\n","    # 'ticker' was added as a regular column inside the loop, now it becomes part of the index\n","    combined_df.set_index('ticker', append=True, inplace=True)\n","    combined_df.index.names = ['Date', 'Ticker']\n","\n","    # Sort the MultiIndex for better performance and consistency\n","    combined_df.sort_index(inplace=True)\n","\n","    print(\"All stock data loaded, structured, filtered, and filled successfully.\")\n","    return combined_df\n","\n"],"metadata":{"id":"oQt_Z87EH2L9","executionInfo":{"status":"ok","timestamp":1750460907704,"user_tz":-60,"elapsed":52,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#Calling the function to load and structure df for FTSE 100 stocks\n","\n","ready_ftse100_data = load_and_structure_stock_data(ftse_100_path)\n","\n","if not ready_ftse100_data.empty:\n","  print(\"\\n--- Final Structured and Cleaned DataFrame ---\")\n","  print(ready_ftse100_data.head(15)) # Show more rows to see multiple dates/tickers\n","  print(\"\\nDataFrame Info:\")\n","  ready_ftse100_data.info()\n","  print(\"\\nSample of weekdays (should only be Mon-Fri):\")\n","  print(ready_ftse100_data.index.get_level_values('Date').day_name().value_counts())\n","  print(\"\\nNaNs after processing (should be very few or none in numerical columns):\")\n","  print(ready_ftse100_data.isnull().sum())\n","else:\n"," print(\"No data was successfully loaded and processed.\")"],"metadata":{"id":"gK9gOL3vIK_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calling the function to load and structure df for FTSE 250 stocks\n","\n","ready_ftse250_data = load_and_structure_stock_data(ftse_250_path)\n","\n","if not ready_ftse250_data.empty:\n","  print(\"\\n--- Final Structured and Cleaned DataFrame ---\")\n","  print(ready_ftse250_data.head(15)) # Show more rows to see multiple dates/tickers\n","  print(\"\\nDataFrame Info:\")\n","  ready_ftse250_data.info()\n","  print(\"\\nSample of weekdays (should only be Mon-Fri):\")\n","  print(ready_ftse250_data.index.get_level_values('Date').day_name().value_counts())\n","  print(\"\\nNaNs after processing (should be very few or none in numerical columns):\")\n","  print(ready_ftse250_data.isnull().sum())\n","else:\n"," print(\"No data was successfully loaded and processed.\")"],"metadata":{"id":"XktAAHheAscj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to genereate the stock features needed to build ML model\n","\n","def generate_all_stock_features(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Generates a comprehensive set of time-series and cross-sectional numerical features\n","    for stock data, suitable for unsupervised anomaly detection models.\n","\n","    This function assumes the input DataFrame has a MultiIndex (Date, Ticker)\n","    and contains cleaned base columns like 'open', 'high', 'low', 'close',\n","    'adj close', and 'volume', with no critical NaNs.\n","\n","    Args:\n","        df (pd.DataFrame): Your clean DataFrame with MultiIndex (Date, Ticker)\n","                           and base stock price/volume data.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with all original and newly engineered numerical\n","                      features. NaNs introduced by calculations (e.g., at the start\n","                      of rolling windows) will be present.\n","    \"\"\"\n","    if df.empty:\n","        print(\"Input DataFrame is empty for feature generation. Returning empty DataFrame.\")\n","        return pd.DataFrame()\n","\n","    processed_df = df.copy() # Always work on a copy to keep the original untouched\n","\n","    # --- Ensure critical columns are numerical for calculations ---\n","    for col in ['open', 'high', 'low', 'close', 'adj close', 'volume']:\n","        if col in processed_df.columns:\n","            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')\n","    processed_df.dropna(subset=['open','close','volume'], inplace=True)\n","    if processed_df.empty:\n","        print(\"No data remaining after ensuring essential columns are numerical and not NaN.\")\n","        return pd.DataFrame()\n","\n","\n","    grouped_by_ticker = processed_df.groupby(level='Ticker')\n","    grouped_by_date = processed_df.groupby(level='Date')\n","    epsilon = 1e-9\n","\n","    print(\"Generating Time-Series Based Features (per stock ticker)...\")\n","\n","    # 1. Return-Based Features:\n","    processed_df['log_return'] = grouped_by_ticker['close'].transform(\n","        lambda x: np.log(x / x.shift(1).replace(0, epsilon)).replace([-np.inf, np.inf], np.nan)\n","    )\n","    processed_df['simple_return'] = grouped_by_ticker['close'].transform(\n","        lambda x: (x / x.shift(1).replace(0, epsilon)) - 1\n","    )\n","    if 'adj close' in processed_df.columns:\n","        processed_df['log_adj_close_return'] = grouped_by_ticker['adj close'].transform(\n","            lambda x: np.log(x / x.shift(1).replace(0, epsilon)).replace([-np.inf, np.inf], np.nan)\n","        )\n","    processed_df['return_5d'] = grouped_by_ticker['close'].transform(lambda x: x.pct_change(periods=5))\n","    processed_df['return_20d'] = grouped_by_ticker['close'].transform(lambda x: x.pct_change(periods=20))\n","\n","    print(\"Generating Volatility Measures...\")\n","    # 2. Volatility Measures:\n","    processed_df['rolling_std_5d_log_return'] = grouped_by_ticker['log_return'].transform(lambda x: x.rolling(window=5).std())\n","    processed_df['rolling_std_20d_log_return'] = grouped_by_ticker['log_return'].transform(lambda x: x.rolling(window=20).std())\n","\n","    if 'high' in processed_df.columns and 'low' in processed_df.columns:\n","        processed_df['daily_range_norm'] = (processed_df['high'] - processed_df['low']) / (processed_df['close'] + epsilon)\n","\n","        log_high_div_low = np.log((processed_df['high'] / processed_df['low'].replace(0, epsilon)).clip(lower=epsilon))\n","        log_close_div_open = np.log((processed_df['close'] / processed_df['open'].replace(0, epsilon)).clip(lower=epsilon))\n","\n","        gk_term = 0.5 * (log_high_div_low)**2 - (2 * np.log(2) - 1) * (log_close_div_open)**2\n","        gk_term[gk_term < 0] = np.nan # Set negative values before sqrt to NaN\n","        processed_df['garman_klass_vol'] = np.sqrt(gk_term)\n","\n","        # Explicitly replace infs/NaNs that might result from sqrt or division by zero, and fill\n","        processed_df['garman_klass_vol'] = processed_df['garman_klass_vol'].replace([-np.inf, np.inf], np.nan)\n","        processed_df['garman_klass_vol'].fillna(0, inplace=True) # Fill with 0 for Garman-Klass specific NaNs\n","    else:\n","        print(\"⚠️ Missing 'high' or 'low' columns for some volatility features. Skipping.\")\n","\n","    print(\"Generating Volume-Based Features...\")\n","    # 3. Volume-Based Features:\n","    processed_df['volume_change'] = grouped_by_ticker['volume'].transform(lambda x: x.pct_change(periods=1))\n","    processed_df['avg_volume_20d'] = grouped_by_ticker['volume'].transform(lambda x: x.rolling(window=20).mean())\n","    processed_df['relative_volume'] = processed_df['volume'] / (processed_df['avg_volume_20d'] + epsilon)\n","\n","    print(\"Generating Momentum/Trend Indicators...\")\n","    # 4. Momentum/Trend Indicators:\n","    processed_df['sma_5d'] = grouped_by_ticker['close'].transform(lambda x: x.rolling(window=5).mean())\n","    processed_df['sma_20d'] = grouped_by_ticker['close'].transform(lambda x: x.rolling(window=20).mean())\n","    processed_df['deviation_from_sma_20d'] = (processed_df['close'] - processed_df['sma_20d']) / (processed_df['sma_20d'] + epsilon)\n","\n","    print(\"Generating Price-Volume Interaction Features...\")\n","    # 5. Price-Volume Interaction Features:\n","    if 'high' in processed_df.columns and 'low' in processed_df.columns:\n","        processed_df['typical_price'] = (processed_df['high'] + processed_df['low'] + processed_df['close']) / 3\n","    else:\n","        print(\"⚠️ Missing 'high' or 'low' columns. Skipping 'typical_price'.\")\n","\n","    print(\"Generating Cross-Sectional Features (comparing stocks on the same day)...\")\n","    # 6. Cross-Sectional Features:\n","    if 'log_return' in processed_df.columns:\n","        processed_df['daily_market_mean_log_return'] = grouped_by_date['log_return'].transform('mean')\n","        processed_df['daily_market_median_log_return'] = grouped_by_date['log_return'].transform('median')\n","\n","        processed_df['deviation_from_daily_mean_return'] = processed_df['log_return'] - processed_df['daily_market_mean_log_return']\n","        processed_df['deviation_from_daily_median_return'] = processed_df['log_return'] - processed_df['daily_market_median_log_return']\n","        processed_df['daily_return_rank_pct'] = grouped_by_date['log_return'].rank(pct=True, method='average')\n","    else:\n","        print(\"⚠️ 'log_return' column not available for cross-sectional feature generation. Skipping these features.\")\n","\n","    # --- FINAL CLEANUP: Replace any remaining inf/-inf with NaN across all numerical columns ---\n","    print(\"Finalizing features: cleaning up any remaining inf/-inf values...\")\n","    numerical_cols_after_gen = processed_df.select_dtypes(include=np.number).columns\n","    for col in numerical_cols_after_gen:\n","        processed_df[col] = processed_df[col].replace([np.inf, -np.inf], np.nan)\n","\n","    print(\"Feature generation complete. NaNs from calculations are present and will need further handling.\")\n","    return processed_df.sort_index()"],"metadata":{"id":"CX47oDUYDIPX","executionInfo":{"status":"ok","timestamp":1750460963850,"user_tz":-60,"elapsed":28,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["\n","# Call the feature generation function on the FTSE 100 stock\n","\n","df_with_all_100_features = generate_all_stock_features(ready_ftse100_data)\n","\n","if not df_with_all_100_features.empty:\n","    print(\"\\n--- NEWLY GENERATED DataFrame with ALL Engineered Features (Head) ---\")\n","    # To ensure you see all columns, even if there are many:\n","    pd.set_option('display.max_columns', None) # Display all columns\n","    pd.set_option('display.width', 1000)      # Ensure wide display in terminal/Colab output\n","\n","    # Now, print the head of the *NEW* DataFrame\n","    print(df_with_all_100_features.head(15))\n","\n","    print(\"\\n--- NEWLY GENERATED DataFrame Info (should show many more columns) ---\")\n","    # Now, print the info of the *NEW* DataFrame\n","    df_with_all_100_features.info()\n","\n","    print(\"\\n--- ALL Column Names in the NEW DataFrame ---\")\n","    # This will explicitly list ALL column names, proving they are there\n","    print(df_with_all_100_features.columns.tolist())\n","\n","    print(\"\\n--- Count of NaNs per column (expect some NaNs from rolling/shifting, esp. at start of series) ---\")\n","    print(df_with_all_100_features.isnull().sum().sort_values(ascending=False).head(20))\n","else:\n","    print(\"The feature generation function returned an empty DataFrame.\")"],"metadata":{"id":"GNB55xrl1RAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Call the feature generation function on the FTSE 100 stock\n","\n","df_with_all_250_features = generate_all_stock_features(ready_ftse250_data)\n","\n","if not df_with_all_250_features.empty:\n","    print(\"\\n--- NEWLY GENERATED DataFrame with ALL Engineered Features (Head) ---\")\n","    # To ensure you see all columns, even if there are many:\n","    pd.set_option('display.max_columns', None) # Display all columns\n","    pd.set_option('display.width', 1000)      # Ensure wide display in terminal/Colab output\n","\n","    # Now, print the head of the *NEW* DataFrame\n","    print(df_with_all_250_features.head(15))\n","\n","    print(\"\\n--- NEWLY GENERATED DataFrame Info (should show many more columns) ---\")\n","    # Now, print the info of the *NEW* DataFrame\n","    df_with_all_250_features.info()\n","\n","    print(\"\\n--- ALL Column Names in the NEW DataFrame ---\")\n","    # This will explicitly list ALL column names, proving they are there\n","    print(df_with_all_250_features.columns.tolist())\n","\n","    print(\"\\n--- Count of NaNs per column (expect some NaNs from rolling/shifting, esp. at start of series) ---\")\n","    print(df_with_all_250_features.isnull().sum().sort_values(ascending=False).head(20))\n","else:\n","    print(\"The feature generation function returned an empty DataFrame.\")"],"metadata":{"id":"pkgU-aLTsJr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to remove all NaNs from features and scales values using standard scaler\n","#from sklearn.preprocessing import StandardScaler\n","\n","def preprocess_features_for_model(\n","    df_with_all_features: pd.DataFrame,\n","    columns_to_exclude_from_features: list = None,\n","    apply_scaling: bool = True,\n","    scaler_obj: StandardScaler = None # Optional: provide a pre-fitted scaler for consistent scaling\n",") -> Tuple[pd.DataFrame, Union[StandardScaler, None]]:\n","    \"\"\"\n","    Handles NaN removal, feature selection, and feature scaling for a DataFrame\n","    containing engineered stock features. This prepares the data for anomaly\n","    detection models.\n","\n","    Args:\n","        df_with_engineered_features (pd.DataFrame): The DataFrame containing\n","                                                    all engineered features, with a MultiIndex.\n","                                                    (Output of `generate_all_stock_features`).\n","        columns_to_exclude_from_features (list, optional): A list of column names\n","                                                        that should NOT be treated as features\n","                                                        for the model (e.g., raw 'open', 'close',\n","                                                        or non-numeric helper columns like 'weekday').\n","                                                        If None, a default list is used.\n","        apply_scaling (bool): Whether to apply StandardScaler to numerical features.\n","                              Defaults to True. Highly recommended for most ML models.\n","        scaler_obj (StandardScaler, optional): An pre-fitted StandardScaler object.\n","                                               If `apply_scaling` is True and `scaler_obj` is None,\n","                                               a new scaler will be fitted. Useful for consistent\n","                                               scaling between train/test datasets.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - pd.DataFrame: The DataFrame with selected, cleaned, and optionally scaled\n","                            numerical features, ready for an anomaly detection model.\n","                            Retains the MultiIndex.\n","            - StandardScaler or None: The fitted or used StandardScaler object if\n","                                      scaling was applied, else None.\n","    \"\"\"\n","    if df_with_all_features.empty:\n","        print(\"Input DataFrame is empty for preprocessing. Returning empty DataFrame.\")\n","        return pd.DataFrame(), None\n","\n","    processed_df = df_with_all_features.copy()\n","    print(\"\\n--- Starting Feature Preprocessing for Model ---\")\n","\n","    # --- 1. Feature Selection (Identify Numerical Features to Use) ---\n","    # Default list of columns that are typically not features, but base data or helpers\n","    if columns_to_exclude_from_features is None:\n","        columns_to_exclude_from_features = ['open', 'high', 'low', 'close', 'adj close', 'volume', 'weekday']\n","\n","    # Get all numerical columns from the DataFrame\n","    all_numerical_cols = processed_df.select_dtypes(include=np.number).columns.tolist()\n","\n","    # Filter out the columns that should be excluded\n","    features_for_model_names = [\n","        col for col in all_numerical_cols\n","        if col not in columns_to_exclude_from_features\n","    ]\n","\n","    if not features_for_model_names:\n","        print(\"⚠️ No valid numerical features identified after exclusion. Using all numerical original columns.\")\n","        features_for_model_names = all_numerical_cols # Fallback to all if custom exclusion leads to empty list\n","\n","    print(f\"Selected {len(features_for_model_names)} features for the model.\")\n","    df_features_only = processed_df[features_for_model_names].copy()\n","\n","\n","    # --- 2. NaN Removal (Final Handling for Model Input) ---\n","    # Drop rows where any of the *selected features* have NaNs.\n","    # This is critical as most ML models cannot handle NaNs.\n","    original_rows_count = df_features_only.shape[0]\n","    df_features_only.dropna(inplace=True)\n","    rows_after_nan_drop = df_features_only.shape[0]\n","\n","    if original_rows_count > rows_after_nan_drop:\n","        print(f\"Dropped {original_rows_count - rows_after_nan_drop} rows due to NaNs in selected features.\")\n","    if df_features_only.empty:\n","        print(\"DataFrame is empty after NaN removal. Cannot proceed with preprocessing.\")\n","        return pd.DataFrame(), None\n","    print(f\"Data shape after NaN removal: {df_features_only.shape}\")\n","\n","\n","    # --- 3. Feature Scaling ---\n","    scaler = None\n","    if apply_scaling:\n","        print(\"Applying StandardScaler to features...\")\n","        scaler = scaler_obj if scaler_obj is not None else StandardScaler()\n","\n","        # Fit and/or transform the features\n","        X_scaled = scaler.fit_transform(df_features_only) if scaler_obj is None else scaler.transform(df_features_only)\n","\n","        # Convert scaled array back to DataFrame, retaining index and column names\n","        df_scaled_features = pd.DataFrame(X_scaled, index=df_features_only.index, columns=df_features_only.columns)\n","        print(f\"Features scaled. Scaler: {'New' if scaler_obj is None else 'Existing'}.\")\n","    else:\n","        df_scaled_features = df_features_only.copy()\n","        print(\"Skipping feature scaling.\")\n","\n","    print(\"--- Feature Preprocessing for Model Complete ---\")\n","    return df_scaled_features, scaler"],"metadata":{"id":"IULteCpM6AMz","executionInfo":{"status":"ok","timestamp":1750461004270,"user_tz":-60,"elapsed":25,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#Calling the preprocess_features_for_model function on the FTSE 100 and FTSE 250 stocks\n","\n","final_processed_FTSE100_df, fitted_scaler = preprocess_features_for_model(\n","    df_with_all_100_features.copy(), # Pass a copy to avoid modifying the original df_with_all_features\n","    columns_to_exclude_from_features=None, # Use default exclusion (removes original price/volume/weekday)\n","    apply_scaling=True,                     # Apply StandardScaler (best for all 3 models)\n","    scaler_obj=None                         # Fit a new scaler\n",")\n","if not final_processed_FTSE100_df.empty:\n","    print(\"\\n--- Processed DataFrame (Head) after fixing infs ---\")\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    print(final_processed_FTSE100_df.head(15))\n","\n","    print(\"\\n--- Processed DataFrame (Info) after fixing infs ---\")\n","    final_processed_FTSE100_df.info()\n","\n","    print(\"\\n--- NaNs in Processed DataFrame (should be 0) ---\")\n","    print(final_processed_FTSE100_df.isnull().sum().sum()) # Should be 0 NaNs if successful\n","else:\n","    print(\"Preprocessing returned an empty DataFrame.\")\n","\n","\n","\n","final_processed_FTSE250_df, fitted_scaler = preprocess_features_for_model(\n","    df_with_all_250_features.copy(), # Pass a copy to avoid modifying the original df_with_all_features\n","    columns_to_exclude_from_features=None, # Use default exclusion (removes original price/volume/weekday)\n","    apply_scaling=True,                     # Apply StandardScaler (best for all 3 models)\n","    scaler_obj=None                         # Fit a new scaler\n",")\n","if not final_processed_FTSE250_df.empty:\n","    print(\"\\n--- Processed DataFrame (Head) after fixing infs ---\")\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    print(final_processed_FTSE100_df.head(15))\n","\n","    print(\"\\n--- Processed DataFrame (Info) after fixing infs ---\")\n","    final_processed_FTSE250_df.info()\n","\n","    print(\"\\n--- NaNs in Processed DataFrame (should be 0) ---\")\n","    print(final_processed_FTSE250_df.isnull().sum().sum()) # Should be 0 NaNs if successful\n","else:\n","    print(\"Preprocessing returned an empty DataFrame.\")"],"metadata":{"id":"W9dD2nPsraNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data visualization\n","\n","\n","def visualize_engineered_features(df: pd.DataFrame, num_sample_tickers: int = 3, num_top_features_corr: int = 15):\n","    \"\"\"\n","    Generates various visualizations for the engineered stock features.\n","\n","    Assumes input DataFrame has a MultiIndex (Date, Ticker) and contains\n","    numerical features (optionally scaled).\n","\n","    Args:\n","        df (pd.DataFrame): The DataFrame with engineered features, MultiIndex (Date, Ticker).\n","        num_sample_tickers (int): Number of random tickers to sample for time-series plots.\n","                                  Defaults to 3.\n","        num_top_features_corr (int): Number of top correlated features to display in the\n","                                     correlation heatmap. Defaults to 15.\n","    \"\"\"\n","    if df.empty:\n","        print(\"DataFrame is empty. No visualizations to generate.\")\n","        return\n","\n","    print(\"\\n--- Starting Data Visualization for Engineered Features ---\")\n","\n","    # Set plot style\n","    sns.set_style(\"whitegrid\")\n","    plt.rcParams['figure.figsize'] = (12, 6) # Default figure size\n","\n","    # --- 1. Overall DataFrame Information ---\n","    print(\"\\n1. DataFrame Overview:\")\n","    print(f\"Total entries: {df.shape[0]}\")\n","    print(f\"Number of features: {df.shape[1]}\")\n","    print(f\"Number of unique tickers: {df.index.get_level_values('Ticker').nunique()}\")\n","    print(f\"Date range: {df.index.get_level_values('Date').min().date()} to {df.index.get_level_values('Date').max().date()}\")\n","\n","    # --- 2. Distribution of Key Features ---\n","    print(\"\\n2. Distribution of Selected Key Features (Histograms/KDE):\")\n","    # Select a few representative features to visualize their distribution\n","    # Adjust these feature names based on your actual generated features\n","    key_features = [\n","        'log_return', 'rolling_std_20d_log_return', 'relative_volume',\n","        'deviation_from_daily_median_return', 'daily_return_rank_pct'\n","    ]\n","\n","    # Filter for features that actually exist in the DataFrame\n","    existing_key_features = [f for f in key_features if f in df.columns]\n","\n","    if existing_key_features:\n","        fig, axes = plt.subplots(len(existing_key_features), 1, figsize=(10, 4 * len(existing_key_features)))\n","        if len(existing_key_features) == 1: axes = [axes] # Ensure axes is iterable for single plot\n","\n","        for i, feature in enumerate(existing_key_features):\n","            sns.histplot(df[feature], kde=True, ax=axes[i], bins=50)\n","            axes[i].set_title(f'Distribution of {feature}')\n","            axes[i].set_xlabel(feature)\n","            axes[i].set_ylabel('Frequency')\n","        plt.tight_layout()\n","        plt.show()\n","    else:\n","        print(\"⚠️ No key features found to plot distributions. Ensure feature names are correct.\")\n","\n","    # --- 3. Time Series Plots for Sample Tickers ---\n","    print(f\"\\n3. Time Series Plots for {num_sample_tickers} Sample Tickers:\")\n","    unique_tickers = df.index.get_level_values('Ticker').unique()\n","    if len(unique_tickers) > num_sample_tickers:\n","        sample_tickers = np.random.choice(unique_tickers, num_sample_tickers, replace=False)\n","    else:\n","        sample_tickers = unique_tickers # Use all if fewer than requested samples\n","\n","    features_to_plot_ts = ['log_return', 'rolling_std_20d_log_return', 'relative_volume']\n","    existing_features_to_plot_ts = [f for f in features_to_plot_ts if f in df.columns]\n","\n","    if existing_features_to_plot_ts and sample_tickers.size > 0:\n","        for ticker in sample_tickers:\n","            ticker_df = df.loc[(slice(None), ticker), :] # Select all dates for this ticker\n","            ticker_df = ticker_df.droplevel('Ticker') # Drop Ticker level from index for cleaner plot\n","\n","            fig, axes = plt.subplots(len(existing_features_to_plot_ts), 1, figsize=(15, 3 * len(existing_features_to_plot_ts)), sharex=True)\n","            if len(existing_features_to_plot_ts) == 1: axes = [axes] # Ensure axes is iterable\n","\n","            for i, feature in enumerate(existing_features_to_plot_ts):\n","                if feature in ticker_df.columns:\n","                    axes[i].plot(ticker_df.index, ticker_df[feature], label=f'{ticker} - {feature}')\n","                    axes[i].set_title(f'{ticker} - {feature} over Time')\n","                    axes[i].set_ylabel(feature)\n","                    axes[i].legend()\n","            plt.tight_layout()\n","            plt.show()\n","    else:\n","        print(\"⚠️ Not enough data or features to plot time series for sample tickers.\")\n","\n","    # --- 4. Correlation Matrix of Features ---\n","    print(f\"\\n4. Correlation Matrix (Top {num_top_features_corr} Most Correlated Features):\")\n","    # Calculate the correlation matrix\n","    corr_matrix = df.corr()\n","\n","    # Find the top N most correlated features (by sum of absolute correlations)\n","    # Exclude self-correlation (diagonal) when summing\n","    np.fill_diagonal(corr_matrix.values, np.nan) # Temporarily set diagonal to NaN for sum\n","    sum_abs_corr = corr_matrix.abs().sum().sort_values(ascending=False)\n","\n","    # Restore diagonal for display\n","    np.fill_diagonal(corr_matrix.values, 1.0)\n","\n","    top_features = sum_abs_corr.head(num_top_features_corr).index.tolist()\n","\n","    if len(top_features) > 1:\n","        plt.figure(figsize=(num_top_features_corr * 0.7, num_top_features_corr * 0.7))\n","        sns.heatmap(corr_matrix.loc[top_features, top_features], annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n","        plt.title(f'Correlation Matrix of Top {num_top_features_corr} Features')\n","        plt.show()\n","    else:\n","        print(\"⚠️ Not enough features or no strong correlations to plot heatmap.\")\n","\n","    # --- 5. Box Plots of Features (Overall) ---\n","    print(\"\\n5. Box Plots of All Features:\")\n","    plt.figure(figsize=(15, 8))\n","    # It's better to melt the DataFrame for box plots if you want all features on one plot\n","    # Or plot each feature individually if the number of features is small\n","    df_melted = df.reset_index().melt(id_vars=['Date', 'Ticker'], var_name='Feature', value_name='Value')\n","\n","    # Plotting for numerical features only\n","    numerical_features_only = df_melted[df_melted['Feature'].isin(df.select_dtypes(include=np.number).columns)]\n","\n","    if not numerical_features_only.empty:\n","        sns.boxplot(data=numerical_features_only, x='Feature', y='Value', orient='v')\n","        plt.title('Box Plot of Engineered Features (Overall)')\n","        plt.xticks(rotation=90)\n","        plt.tight_layout()\n","        plt.show()\n","    else:\n","        print(\"⚠️ No numerical features to plot box plots.\")\n","\n","    # --- 6. Pair Plots (for selected features - can be very slow for many features) ---\n","    print(\"\\n6. Pair Plots for a subset of features (Warning: Can be very slow for many features/rows):\")\n","    # It's impractical to plot all features. Select a very small, representative subset.\n","    pair_plot_features = [\n","        'log_return', 'rolling_std_5d_log_return', 'daily_return_rank_pct'\n","    ]\n","    existing_pair_plot_features = [f for f in pair_plot_features if f in df.columns]\n","\n","    if len(existing_pair_plot_features) >= 2 and df.shape[0] < 5000: # Limit for performance\n","        print(f\"Plotting pair plots for: {', '.join(existing_pair_plot_features)}. This may take a while.\")\n","        sns.pairplot(df[existing_pair_plot_features].sample(min(500, df.shape[0]))) # Sample a subset for faster plotting\n","        plt.suptitle('Pair Plots of Selected Features', y=1.02)\n","        plt.show()\n","    elif len(existing_pair_plot_features) < 2:\n","        print(\"⚠️ Not enough selected features for pair plot.\")\n","    else:\n","        print(\"⚠️ Too many rows for pair plot (sampling for speed). Skipped pair plot due to potentially large data.\")\n","\n","\n","    print(\"\\n--- Data Visualization Complete ---\")\n","\n","# Call the visualization function\n","visualize_engineered_features(final_processed_FTSE100_df)"],"metadata":{"id":"4jQhYp7IKmZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to split data into training and validation\n","\n","def perform_chronological_split(df: pd.DataFrame, split_date: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","    \"\"\"\n","    Splits the input DataFrame into training and evaluation sets based on a chronological date.\n","\n","    The training set will contain data up to and including the split_date.\n","    The evaluation set will contain data strictly after the split_date.\n","\n","    Args:\n","        df (pd.DataFrame): The input DataFrame with a MultiIndex (Date, Ticker),\n","                           and numerical features. This is your 'processed_features_df'.\n","        split_date (str): The date string in 'YYYY-MM-DD' format to use as the split point.\n","                          Data up to this date goes into the training set.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - pd.DataFrame: The training data (X_train_normal).\n","            - pd.DataFrame: The evaluation data (X_eval).\n","            Returns empty DataFrames if the input is empty or split is not possible.\n","    \"\"\"\n","    if df.empty:\n","        print(\"Input DataFrame is empty for chronological split. Returning empty DataFrames.\")\n","        return pd.DataFrame(), pd.DataFrame()\n","\n","    try:\n","        parsed_split_date = pd.to_datetime(split_date)\n","    except ValueError:\n","        print(f\"❌ Error: Invalid split_date format '{split_date}'. Please use 'YYYY-MM-DD'.\")\n","        return pd.DataFrame(), pd.DataFrame()\n","\n","    print(f\"\\n--- Performing Chronological Data Split at {parsed_split_date.date()} ---\")\n","\n","    # X_train_normal: Data from the beginning up to the split_date (inclusive)\n","    X_train_normal = df.loc[df.index.get_level_values('Date') <= parsed_split_date].copy()\n","\n","    # X_eval: Data strictly after the split_date\n","    X_eval = df.loc[df.index.get_level_values('Date') > parsed_split_date].copy()\n","\n","    print(f\"Data split complete:\")\n","    print(f\"  Training data (X_train_normal) shape: {X_train_normal.shape} (Dates up to {parsed_split_date.date()})\")\n","    print(f\"  Evaluation data (X_eval) shape: {X_eval.shape} (Dates after {parsed_split_date.date()})\")\n","\n","    # Basic verification of the split\n","    if not X_train_normal.empty and not X_eval.empty:\n","        max_train_date = X_train_normal.index.get_level_values('Date').max()\n","        min_eval_date = X_eval.index.get_level_values('Date').min()\n","        print(f\"  Verification: Max train date: {max_train_date.date()}, Min eval date: {min_eval_date.date()}\")\n","        if max_train_date >= parsed_split_date and min_eval_date <= parsed_split_date: # This condition might be too strict if split_date is not present in data\n","            pass # The check is more about ensuring non-overlap in the general sense.\n","    elif X_train_normal.empty:\n","        print(\"⚠️ Warning: Training data is empty after split. Check split_date or input data.\")\n","    elif X_eval.empty:\n","        print(\"⚠️ Warning: Evaluation data is empty after split. Check split_date or input data.\")\n","\n","\n","    return X_train_normal, X_eval\n"],"metadata":{"id":"dzKR4yIIAwlk","executionInfo":{"status":"ok","timestamp":1750461085511,"user_tz":-60,"elapsed":8,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Calling the function to split the data into training and testing\n","\n","# Define your desired chronological split point (e.g., end of 2020)\n","# A common split is 70-80% for training. For 2014-2024 data, 2020-12-31 is a good point.\n","my_split_date = \"2020-12-31\"\n","\n","# Call the function to perform the split on the FTSE 100 stocks\n","X1_train_normal, X1_eval = perform_chronological_split(final_processed_FTSE100_df, my_split_date)\n","\n","#Call the function to perform the split on the FTSE 250 stocks\n","X2_train_normal, X2_eval = perform_chronological_split(final_processed_FTSE250_df, my_split_date)\n","\n"],"metadata":{"id":"4n-YDY4hJBRx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to train the data using the Isolation Forest Model\n","\n","def train_and_score_isolation_forest(\n","    X_train_normal: pd.DataFrame,\n","    X_eval: pd.DataFrame,\n","    contamination: float = 0.01,\n","    random_state: int = 42\n",") -> Tuple[pd.DataFrame, pd.DataFrame, IsolationForest]:\n","    \"\"\"\n","    Trains an Isolation Forest model on X_train_normal and applies it to\n","    both X_train_normal and X_eval to generate anomaly scores and labels.\n","\n","    Args:\n","        X_train_normal (pd.DataFrame): DataFrame containing the features for training\n","                                       the Isolation Forest model (assumed to be scaled).\n","        X_eval (pd.DataFrame): DataFrame containing the features for evaluating\n","                               the trained model (assumed to be scaled).\n","        contamination (float): The proportion of outliers in the data set.\n","                               Used to define the threshold for anomaly prediction.\n","                               Defaults to 0.01 (1%).\n","        random_state (int): Seed for reproducibility of the Isolation Forest model.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - pd.DataFrame: X_train_normal with 'anomaly_score' and 'anomaly_label' columns added.\n","            - pd.DataFrame: X_eval with 'anomaly_score' and 'anomaly_label' columns added.\n","            - IsolationForest: The trained Isolation Forest model.\n","        Returns empty DataFrames and None if input is empty.\n","    \"\"\"\n","    if X_train_normal.empty:\n","        print(\"X_train_normal is empty. Cannot train Isolation Forest.\")\n","        return pd.DataFrame(), pd.DataFrame(), None\n","    if X_eval.empty:\n","        print(\"X_eval is empty. Cannot score Isolation Forest on evaluation set.\")\n","        # We can still train and score on train, but will return empty X_eval_scored\n","\n","    print(\"\\n--- Training and Scoring Isolation Forest Model ---\")\n","\n","    # Initialize and train the Isolation Forest model\n","    # Isolation Forest does not technically need scaled data, but it's good practice\n","    # as it's often followed by other models sensitive to scaling.\n","    print(f\"Training Isolation Forest with contamination={contamination} and random_state={random_state}...\")\n","    iso_forest_model = IsolationForest(contamination=contamination, random_state=random_state)\n","    iso_forest_model.fit(X_train_normal)\n","    print(\"Isolation Forest model trained successfully.\")\n","\n","    # --- Apply the model to generate scores and labels ---\n","\n","    # Score and label X_train_normal\n","    print(\"Generating anomaly scores and labels for X_train_normal...\")\n","    X_train_normal_scored = X_train_normal.copy()\n","    X_train_normal_scored['iso_forest_anomaly_score'] = iso_forest_model.decision_function(X_train_normal)\n","    # predict returns -1 for outliers and 1 for inliers\n","    X_train_normal_scored['iso_forest_anomaly_label'] = iso_forest_model.predict(X_train_normal)\n","    # Map to 0 (normal) and 1 (anomaly) for easier interpretation\n","    X_train_normal_scored['iso_forest_is_anomaly'] = X_train_normal_scored['iso_forest_anomaly_label'].map({1: 0, -1: 1})\n","    print(\"Anomaly scores and labels generated for X_train_normal.\")\n","\n","    # Score and label X_eval\n","    X_eval_scored = X_eval.copy()\n","    if not X_eval.empty:\n","        print(\"Generating anomaly scores and labels for X_eval...\")\n","        X_eval_scored['iso_forest_anomaly_score'] = iso_forest_model.decision_function(X_eval)\n","        X_eval_scored['iso_forest_anomaly_label'] = iso_forest_model.predict(X_eval)\n","        X_eval_scored['iso_forest_is_anomaly'] = X_eval_scored['iso_forest_anomaly_label'].map({1: 0, -1: 1})\n","        print(\"Anomaly scores and labels generated for X_eval.\")\n","    else:\n","        print(\"X_eval is empty, skipping anomaly scoring for evaluation set.\")\n","\n","    print(\"--- Isolation Forest Training and Scoring Complete ---\")\n","\n","    return X_train_normal_scored, X_eval_scored, iso_forest_model\n","\n"],"metadata":{"id":"chuPEFGKonvf","executionInfo":{"status":"ok","timestamp":1750461111912,"user_tz":-60,"elapsed":9,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Calling the Train and score the Isolation Forest model on the FTSE 100 stocks\n","\n","X100_train_normal_iso_scored, X100_eval_iso_scored, isolation_model = \\\n","    train_and_score_isolation_forest(X1_train_normal.copy(), X1_eval.copy(), contamination=0.01)\n","\n","# Display results\n","if not X100_train_normal_iso_scored.empty:\n","    print(\"\\n--- X_train_normal (Isolation Forest Scored) Head ---\")\n","    print(X100_train_normal_iso_scored.head())\n","    print(\"\\nAnomaly counts in X_train_normal:\")\n","    print(X100_train_normal_iso_scored['iso_forest_is_anomaly'].value_counts())\n","\n","if not X100_eval_iso_scored.empty:\n","    print(\"\\n--- X_eval (Isolation Forest Scored) Head ---\")\n","    print(X100_eval_iso_scored.head())\n","    print(\"\\nAnomaly counts in X_eval:\")\n","    print(X100_eval_iso_scored['iso_forest_is_anomaly'].value_counts())\n","\n","print(\"\\nIsolation Forest Model Summary:\")\n","print(isolation_model)"],"metadata":{"id":"oTeQrqqFsF9Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter the DataFrame to show only the anomalous data points\n","anomalies_df = X100_eval_iso_scored[X100_eval_iso_scored['iso_forest_is_anomaly'] == 1]\n","\n","print(f\"\\n--- Detected Anomalies (Total: {anomalies_df.shape[0]}) ---\")\n","pd.set_option('display.max_columns', None) # Show all columns\n","pd.set_option('display.width', 1000)      # Ensure wide display\n","print(anomalies_df.sort_values(by='iso_forest_anomaly_score').head(20)) # Show the top 10 most anomalous (lowest score)\n"],"metadata":{"id":"q_T-KZr7GcQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# --- NEW MERGE LOGIC FOR ULTIMATE ROBUSTNESS (using index-based merge) ---\n","\n","# 1. Prepare original_eval_data: filter by date range and ensure unique MultiIndex\n","eval_start_date = X100_eval_iso_scored.index.get_level_values('Date').min()\n","eval_end_date = X100_eval_iso_scored.index.get_level_values('Date').max()\n","\n","original_eval_data_for_merge = ready_ftse100_data.loc[\n","    (ready_ftse100_data.index.get_level_values('Date') >= eval_start_date) &\n","    (ready_ftse100_data.index.get_level_values('Date') <= eval_end_date)\n","].copy()\n","\n","# Crucial: Drop duplicates from the MultiIndex if any exist, before any merge operations.\n","# The .index.duplicated() check is robust for MultiIndex.\n","if not original_eval_data_for_merge.index.is_unique:\n","    print(\"Warning: Duplicates found in original_eval_data_for_merge index. Dropping them.\")\n","    original_eval_data_for_merge = original_eval_data_for_merge[~original_eval_data_for_merge.index.duplicated(keep='first')].copy()\n","\n","# 2. Prepare X100_eval_iso_scored (features and scores): ensure unique MultiIndex\n","if not X100_eval_iso_scored.index.is_unique:\n","    print(\"Warning: Duplicates found in X100_eval_iso_scored index. Dropping them.\")\n","    X100_eval_iso_scored_unique = X100_eval_iso_scored[~X100_eval_iso_scored.index.duplicated(keep='first')].copy()\n","else:\n","    X100_eval_iso_scored_unique = X100_eval_iso_scored.copy()\n","\n","\n","# Ensure that both DataFrames have the same index for a clean merge (intersection)\n","# This will align them perfectly based on (Date, Ticker)\n","common_indices = original_eval_data_for_merge.index.intersection(X100_eval_iso_scored_unique.index)\n","\n","original_eval_data_aligned = original_eval_data_for_merge.loc[common_indices]\n","eval_scores_features_aligned = X100_eval_iso_scored_unique.loc[common_indices]\n","\n","\n","# 3. Perform the merge using `left_index=True` and `right_index=True`\n","#    This merges directly on the MultiIndex, which is the most stable approach.\n","#    We are merging the features and anomaly data from eval_scores_features_aligned\n","#    onto the original raw data from original_eval_data_aligned.\n","\n","# Columns to take from eval_scores_features_aligned (all columns except base data)\n","cols_to_merge_from_scores = [col for col in eval_scores_features_aligned.columns\n","                             if col not in original_eval_data_aligned.columns or col in ['iso_forest_anomaly_score', 'iso_forest_anomaly_label', 'iso_forest_is_anomaly']]\n","\n","full_eval_data_with_anomalies = original_eval_data_aligned.merge(\n","    eval_scores_features_aligned[cols_to_merge_from_scores],\n","    left_index=True,  # Merge on the index of the left DataFrame\n","    right_index=True, # Merge on the index of the right DataFrame\n","    how='left'        # Keep all rows from the left DataFrame (original data)\n",")\n","\n","print(f\"Full eval data with anomalies shape: {full_eval_data_with_anomalies.shape}\")\n","print(f\"Is index truly unique in full_eval_data_with_anomalies? {full_eval_data_with_anomalies.index.is_unique}\") # Should be True\n","\n","# --- Step 1: Filter to See Only the Anomalies from the NEW Merged DataFrame ---\n","# This filtering now happens on a DataFrame guaranteed to have a unique MultiIndex.\n","anomalies_df = full_eval_data_with_anomalies[full_eval_data_with_anomalies['iso_forest_is_anomaly'] == 1].copy()\n","\n","print(f\"\\n--- Detected Anomalies with Original Context (Total: {anomalies_df.shape[0]}) ---\")\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', 1000)\n","print(anomalies_df.sort_values(by='iso_forest_anomaly_score').head(10))\n","\n","\n","# --- Step 2: Inspect Anomaly Characteristics ---\n","# Review anomalies_df to understand specific values.\n","\n","\n","# --- Step 3: Visualize Individual Anomalies in Context ---\n","print(\"\\n--- Visualizing Individual Anomalies with Original Close Price Context ---\")\n","\n","window_days = 30\n","\n","if not anomalies_df.empty:\n","    sample_anomaly_indices = anomalies_df.sort_values(by='iso_forest_anomaly_score').head(3).index.tolist()\n","else:\n","    sample_anomaly_indices = []\n","    print(\"No anomalies detected to visualize.\")\n","\n","for anomaly_idx in sample_anomaly_indices:\n","    anomaly_date = anomaly_idx[0]\n","    anomaly_ticker = anomaly_idx[1]\n","\n","    print(f\"\\nVisualizing anomaly for {anomaly_ticker} on {anomaly_date.date()}\")\n","\n","    # Select data for this specific ticker across a time window from the full_eval_data_with_anomalies\n","    # This DataFrame should already have its MultiIndex intact and unique.\n","    ticker_data_for_plot = full_eval_data_with_anomalies.loc[(slice(None), anomaly_ticker), :].droplevel('Ticker')\n","\n","    start_window = anomaly_date - pd.Timedelta(days=window_days)\n","    end_window = anomaly_date + pd.Timedelta(days=window_days)\n","\n","    plot_data = ticker_data_for_plot.loc[start_window:end_window]\n","\n","    if plot_data.empty:\n","        print(f\"No data available for {anomaly_ticker} around {anomaly_date.date()} for plotting window.\")\n","        continue\n","\n","    plt.figure(figsize=(15, 8))\n","\n","    ax1 = plt.subplot(3, 1, 1)\n","    ax1.plot(plot_data.index, plot_data['close'], label=f'{anomaly_ticker} Close Price', color='blue')\n","\n","    anomalies_in_window = plot_data[plot_data['iso_forest_is_anomaly'] == 1]\n","    if not anomalies_in_window.empty:\n","        ax1.scatter(anomalies_in_window.index, anomalies_in_window['close'],\n","                    color='red', s=100, zorder=5, label='Detected Anomaly')\n","\n","    ax1.set_title(f'Close Price for {anomaly_ticker} around {anomaly_date.date()}')\n","    ax1.set_ylabel('Close Price')\n","    ax1.legend()\n","    ax1.grid(True)\n","\n","    ax2 = plt.subplot(3, 1, 2, sharex=ax1)\n","    if 'log_return' in plot_data.columns:\n","        ax2.plot(plot_data.index, plot_data['log_return'], label=f'{anomaly_ticker} Log Return', color='green')\n","        if not anomalies_in_window.empty:\n","            ax2.scatter(anomalies_in_window.index, anomalies_in_window['log_return'], color='red', s=100, zorder=5)\n","        ax2.set_ylabel('Log Return')\n","        ax2.set_title(f'Log Return for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax2.legend()\n","        ax2.grid(True)\n","    elif 'relative_volume' in plot_data.columns:\n","        ax2.plot(plot_data.index, plot_data['relative_volume'], label=f'{anomaly_ticker} Relative Volume', color='purple')\n","        if not anomalies_in_window.empty:\n","            ax2.scatter(anomalies_in_window.index, anomalies_in_window['relative_volume'], color='red', s=100, zorder=5)\n","        ax2.set_ylabel('Relative Volume')\n","        ax2.set_title(f'Relative Volume for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax2.legend()\n","        ax2.grid(True)\n","    else:\n","        ax2.text(0.5, 0.5, \"No suitable feature for second plot.\", horizontalalignment='center', verticalalignment='center', transform=ax2.transAxes)\n","\n","\n","    ax3 = plt.subplot(3, 1, 3, sharex=ax1)\n","    if 'relative_volume' in plot_data.columns:\n","        ax3.plot(plot_data.index, plot_data['relative_volume'], label=f'{anomaly_ticker} Relative Volume', color='purple')\n","        if not anomalies_in_window.empty:\n","            ax3.scatter(anomalies_in_window.index, anomalies_in_window['relative_volume'], color='red', s=100, zorder=5)\n","        ax3.set_ylabel('Relative Volume')\n","        ax3.set_title(f'Relative Volume for {anomaly_ticker} around {anomaly_date.date()}')\n","    elif 'deviation_from_daily_median_return' in plot_data.columns:\n","         ax3.plot(plot_data.index, plot_data['deviation_from_daily_median_return'], label=f'{anomaly_ticker} Deviation from Median Daily Return', color='orange')\n","         if not anomalies_in_window.empty:\n","            ax3.scatter(anomalies_in_window.index, anomalies_in_window['deviation_from_daily_median_return'], color='red', s=100, zorder=5)\n","         ax3.set_ylabel('Deviation from Median Daily Return')\n","         ax3.set_title(f'Deviation from Median Daily Return for {anomaly_ticker} around {anomaly_date.date()}')\n","    else:\n","        ax3.text(0.5, 0.5, \"No suitable feature for third plot.\", horizontalalignment='center', verticalalignment='center', transform=ax3.transAxes)\n","\n","    ax3.set_xlabel('Date')\n","    ax3.legend()\n","    ax3.grid(True)\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"ie31OZnyXXZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def analyze_and_visualize_anomalies(\n","    anomalies_df_multiindex: pd.DataFrame, # Now explicitly named as MultiIndex input\n","    full_eval_data_with_anomalies_multiindex: pd.DataFrame, # Now explicitly named as MultiIndex input\n","    num_top_anomalies_to_plot: int = 5,\n","    window_days_for_plot: int = 40 # Days before and after anomaly for time-series plot\n","):\n","    \"\"\"\n","    Performs detailed analysis and visualization of detected anomalies.\n","\n","    Args:\n","        anomalies_df_multiindex (pd.DataFrame): DataFrame containing only the data points\n","                                               identified as anomalies (from Isolation Forest).\n","                                               Must have MultiIndex (Date, Ticker).\n","        full_eval_data_with_anomalies_multiindex (pd.DataFrame): The complete evaluation DataFrame\n","                                                                 with all features, original data,\n","                                                                 and anomaly scores/labels (both normal and anomalous points).\n","                                                                 Must have MultiIndex (Date, Ticker).\n","        num_top_anomalies_to_plot (int): Number of top (most anomalous) events to plot in detail.\n","        window_days_for_plot (int): Number of calendar days before and after an anomaly\n","                                    to show in time-series plots for context.\n","    \"\"\"\n","    if anomalies_df_multiindex.empty:\n","        print(\"No anomalies detected. Skipping analysis and visualization.\")\n","        return\n","\n","    print(\"\\n--- Starting Anomaly Analysis and Visualization ---\")\n","\n","    # Set display options for full column view in print statements\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    sns.set_style(\"whitegrid\")\n","\n","    # --- CRITICAL FIX: Flatten DataFrames to have Date/Ticker as regular columns ---\n","    # This makes subsequent operations (feature selection, plotting) much more consistent.\n","    full_eval_data_flat = full_eval_data_with_anomalies_multiindex.reset_index().copy()\n","    anomalies_df_flat = anomalies_df_multiindex.reset_index().copy()\n","\n","    # --- 1. Overall Anomaly Summary ---\n","    total_anomalies = anomalies_df_flat.shape[0] # Use the flattened version here\n","    print(f\"\\n1. Overall Anomaly Summary:\")\n","    print(f\"Total anomalies detected: {total_anomalies}\")\n","\n","    print(\"\\nTop 10 Tickers with Most Anomalies:\")\n","    print(anomalies_df_flat['Ticker'].value_counts().head(10)) # Use Ticker column from flat DF\n","\n","    print(\"\\nMost Anomalous Events (Top 10 by Score - Lower is More Anomalous):\")\n","    # Sort on the flat DF, but we'll print using the MultiIndexed df for consistency with previous user output.\n","    # So, use anomalies_df_multiindex here.\n","    print(anomalies_df_multiindex.sort_values(by='iso_forest_anomaly_score').head(10))\n","\n","    # --- 2. Distribution of Anomaly Scores (for Anomalies Only) ---\n","    plt.figure(figsize=(10, 6))\n","    sns.histplot(anomalies_df_flat['iso_forest_anomaly_score'], kde=True, bins=30, color='red')\n","    plt.title('Distribution of Anomaly Scores for Detected Anomalies')\n","    plt.xlabel('Anomaly Score (Lower = More Anomalous)')\n","    plt.ylabel('Count')\n","    plt.show()\n","\n","    # --- 3. Feature Comparison: Anomalies vs. Normal Data (Box Plots) ---\n","    print(\"\\n3. Feature Comparison: Anomalies vs. Normal Data (Box Plots):\")\n","\n","    # Corrected way to get feature columns when DataFrame has a flat index and Date/Ticker are columns\n","    # Get all numerical columns that are NOT Date, Ticker, or anomaly scores/labels, or original base columns.\n","    feature_cols = [col for col in full_eval_data_flat.columns\n","                    if pd.api.types.is_numeric_dtype(full_eval_data_flat[col]) # Ensure numerical\n","                    and col not in ['Date', 'Ticker', 'iso_forest_anomaly_score', 'iso_forest_anomaly_label', 'iso_forest_is_anomaly']\n","                    and col not in ['open', 'high', 'low', 'close', 'adj close', 'volume']] # Exclude raw original data if desired\n","\n","    # Select a diverse sample of actual features for comparison\n","    comparison_features = ['log_return', 'rolling_std_20d_log_return', 'relative_volume', 'deviation_from_daily_median_return']\n","    existing_comparison_features = [f for f in comparison_features if f in feature_cols]\n","\n","    if not existing_comparison_features:\n","        # Fallback: if specific features aren't found, pick first few numerical features\n","        existing_comparison_features = feature_cols[:min(5, len(feature_cols))]\n","        print(f\"⚠️ Using a fallback list of {len(existing_comparison_features)} features for comparison plots.\")\n","\n","    if not existing_comparison_features:\n","        print(\"⚠️ No suitable features found for comparison plots after all filters. Skipping.\")\n","    else:\n","        # Use the flattened DataFrame directly for plotting box plots\n","        fig, axes = plt.subplots(len(existing_comparison_features), 1, figsize=(12, 5 * len(existing_comparison_features)))\n","        if len(existing_comparison_features) == 1: axes = [axes] # Ensure axes is iterable for single plot\n","\n","        for i, feature in enumerate(existing_comparison_features):\n","            sns.boxplot(x='iso_forest_is_anomaly', y=feature, data=full_eval_data_flat, ax=axes[i])\n","            axes[i].set_title(f'Distribution of {feature} for Normal (0) vs. Anomaly (1)')\n","            axes[i].set_xlabel('Is Anomaly (0=Normal, 1=Anomaly)')\n","            axes[i].set_ylabel(feature)\n","        plt.tight_layout()\n","        plt.show()\n","\n","    # --- 4. Anomaly Frequency Over Time ---\n","    print(\"\\n4. Anomaly Frequency Over Time:\")\n","    # Use the flat DataFrame for value_counts on the 'Date' column\n","    anomalies_per_day = anomalies_df_flat['Date'].value_counts().sort_index()\n","    if not anomalies_per_day.empty:\n","        plt.figure(figsize=(15, 6))\n","        anomalies_per_day.plot(kind='line', marker='o', linestyle='-', color='red')\n","        plt.title('Number of Anomalies Detected Per Day')\n","        plt.xlabel('Date')\n","        plt.ylabel('Number of Anomalies')\n","        plt.grid(True)\n","        plt.tight_layout()\n","        plt.show()\n","    else:\n","        print(\"No daily anomaly counts to plot.\")\n","\n","    # --- 5. Detailed Time-Series Plots for Top N Anomalies ---\n","    print(f\"\\n5. Detailed Time-Series Plots for Top {num_top_anomalies_to_plot} Anomalies:\")\n","\n","    # Get the indices (Date, Ticker) of the top N most anomalous points from the original MultiIndex anomalies_df\n","    top_anomaly_indices = anomalies_df_multiindex.sort_values(by='iso_forest_anomaly_score').head(num_top_anomalies_to_plot).index.tolist()\n","\n","    if not top_anomaly_indices:\n","        print(\"No top anomalies to plot in detail.\")\n","        return\n","\n","    # Use the flattened full_eval_data_flat for plotting, as it has Date/Ticker as columns\n","    plot_source_df_flat = full_eval_data_flat.copy()\n","\n","    # Ensure essential plot columns exist\n","    required_plot_cols = ['close', 'log_return', 'relative_volume']\n","    if not all(col in plot_source_df_flat.columns for col in required_plot_cols):\n","        print(f\"Error: Missing one or more required plotting columns ({', '.join(required_plot_cols)}) in the main DataFrame. Skipping detailed plots.\")\n","        return\n","\n","    for anomaly_idx in tqdm(top_anomaly_indices, desc=\"Plotting Top Anomalies\"):\n","        anomaly_date = anomaly_idx[0]\n","        anomaly_ticker = anomaly_idx[1]\n","\n","        print(f\"\\nVisualizing anomaly for {anomaly_ticker} on {anomaly_date.date()}\")\n","\n","        # Filter the flat DataFrame for plotting by Date and Ticker columns\n","        ticker_data_for_plot_flat = plot_source_df_flat[\n","            (plot_source_df_flat['Ticker'] == anomaly_ticker) &\n","            (plot_source_df_flat['Date'] >= (anomaly_date - pd.Timedelta(days=window_days_for_plot))) &\n","            (plot_source_df_flat['Date'] <= (anomaly_date + pd.Timedelta(days=window_days_for_plot)))\n","        ].copy()\n","\n","        # Temporarily set index to Date for plotting, as plot functions usually prefer this\n","        plot_data = ticker_data_for_plot_flat.set_index('Date').sort_index()\n","\n","        if plot_data.empty:\n","            print(f\"No data available for {anomaly_ticker} around {anomaly_date.date()} for plotting window. Skipping.\")\n","            continue\n","\n","        plt.figure(figsize=(15, 10))\n","\n","        # Subplot 1: Close Price with Anomaly Highlight\n","        ax1 = plt.subplot(3, 1, 1)\n","        ax1.plot(plot_data.index, plot_data['close'], label=f'{anomaly_ticker} Close Price', color='blue')\n","\n","        anomalies_in_window = plot_data[plot_data['iso_forest_is_anomaly'] == 1]\n","        if not anomalies_in_window.empty:\n","            ax1.scatter(anomalies_in_window.index, anomalies_in_window['close'],\n","                        color='red', s=100, zorder=5, label='Detected Anomaly')\n","\n","        ax1.set_title(f'Close Price for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax1.set_ylabel('Close Price')\n","        ax1.legend()\n","        ax1.grid(True)\n","\n","        # Subplot 2: Log Return with Anomaly Highlight\n","        ax2 = plt.subplot(3, 1, 2, sharex=ax1) # Share x-axis to align dates\n","        ax2.plot(plot_data.index, plot_data['log_return'], label=f'{anomaly_ticker} Log Return', color='green')\n","        if not anomalies_in_window.empty:\n","            ax2.scatter(anomalies_in_window.index, anomalies_in_window['log_return'], color='red', s=100, zorder=5)\n","        ax2.set_ylabel('Log Return')\n","        ax2.set_title(f'Log Return for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax2.legend()\n","        ax2.grid(True)\n","\n","        # Subplot 3: Relative Volume with Anomaly Highlight\n","        ax3 = plt.subplot(3, 1, 3, sharex=ax1) # Share x-axis to align dates\n","        ax3.plot(plot_data.index, plot_data['relative_volume'], label=f'{anomaly_ticker} Relative Volume', color='purple')\n","        if not anomalies_in_window.empty:\n","            ax3.scatter(anomalies_in_window.index, anomalies_in_window['relative_volume'], color='red', s=100, zorder=5)\n","        ax3.set_ylabel('Relative Volume')\n","        ax3.set_title(f'Relative Volume for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax3.set_xlabel('Date')\n","        ax3.legend()\n","        ax3.grid(True)\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    print(\"\\n--- Anomaly Analysis and Visualization Complete ---\")\n","\n","\n","\n","# --- Call the analysis function ---\n","# Pass the correct variables here\n","analyze_and_visualize_anomalies(anomalies_df, full_eval_data_with_anomalies, num_top_anomalies_to_plot=5)"],"metadata":{"id":"Yryc-MJERQff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add ."],"metadata":{"id":"nWfQYpdbvvvt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a function to get the optimal eps parameter from the k-means graph\n","\n","def plot_k_distance_graph(df_features: pd.DataFrame, min_samples_val: int):\n","    \"\"\"\n","    Plots the k-distance graph to help determine the optimal 'eps' parameter for DBSCAN.\n","\n","    Args:\n","        df_features (pd.DataFrame): The DataFrame of features (e.g., X_eval from your split),\n","                                    assumed to be scaled.\n","        min_samples_val (int): The chosen 'min_samples' value for DBSCAN, which determines 'k'\n","                                for the k-distance calculation (k = min_samples_val - 1).\n","    \"\"\"\n","    if df_features.empty:\n","        print(\"Input DataFrame is empty. Cannot plot k-distance graph.\")\n","        return\n","\n","    print(f\"\\n--- Plotting K-Distance Graph for eps Tuning (k = {min_samples_val-1}) ---\")\n","\n","    # Use NearestNeighbors to find the distance to the (min_samples_val - 1)-th nearest neighbor\n","    # (k in k-distance is min_samples - 1 because min_samples includes the point itself)\n","    neighbors = NearestNeighbors(n_neighbors=min_samples_val)\n","    neighbors_fit = neighbors.fit(df_features)\n","\n","    # Distances to the n_neighbors-th nearest point for each data point\n","    distances, indices = neighbors_fit.kneighbors(df_features)\n","\n","    # Sort distances by the distance to the (k-1)th nearest neighbor (which is min_samples_val - 1)\n","    # The last column of 'distances' array contains the distances to the n_neighbors-th point.\n","    distances = np.sort(distances[:, min_samples_val-1], axis=0)\n","\n","    plt.figure(figsize=(15, 7))\n","    plt.plot(distances)\n","    plt.title(f'K-Distance Graph for eps Tuning (k = {min_samples_val-1})')\n","    plt.xlabel('Data Points Sorted by Distance')\n","    plt.ylabel(f'Distance to {min_samples_val-1}-th Nearest Neighbor')\n","    plt.grid(True)\n","    plt.axhline(y=0.1, color='r', linestyle='--', label='Example Threshold (0.1)') # Example threshold\n","    plt.legend()\n","    plt.show()"],"metadata":{"id":"6JeursaH3xEj","executionInfo":{"status":"ok","timestamp":1750461604023,"user_tz":-60,"elapsed":43,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["#Calling the function to create k-means graph\n","\n","# Step 1: Determine initial min_samples\n","# num_features = X_eval.shape[1]\n","# min_samples_for_dbscan = 2 * num_features # Common heuristic\n","min_samples_for_dbscan = 2 * X1_eval.shape[1] # Using X_eval's column count\n","\n","print(f\"Recommended min_samples for DBSCAN based on features: {min_samples_for_dbscan}\")\n","\n","# Step 2: Plot the k-distance graph to help choose 'eps'\n","plot_k_distance_graph(X1_eval, min_samples_for_dbscan)"],"metadata":{"id":"dGl1Zz0PkqK_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a function that trains and scores the data using the DBSCAN model\n","\n","def train_and_score_dbscan(\n","    X_eval: pd.DataFrame,\n","    eps: float,\n","    min_samples: int\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Applies the DBSCAN clustering algorithm to the evaluation data,\n","    identifying clusters and marking noise points as anomalies.\n","\n","    Args:\n","        X_eval (pd.DataFrame): DataFrame containing the features for evaluation\n","                               (assumed to be scaled and cleaned).\n","        eps (float): The maximum distance between two samples for one to be considered\n","                     as in the neighborhood of the other (epsilon).\n","        min_samples (int): The number of samples (or total weight) in a neighborhood\n","                           for a point to be considered as a core point.\n","\n","    Returns:\n","        pd.DataFrame: X_eval with 'dbscan_cluster_label' and 'dbscan_is_anomaly' columns added.\n","                      Returns an empty DataFrame if input is empty.\n","    \"\"\"\n","    if X_eval.empty:\n","        print(\"X_eval is empty. Cannot apply DBSCAN.\")\n","        return pd.DataFrame()\n","\n","    print(f\"\\n--- Applying DBSCAN Model with eps={eps}, min_samples={min_samples} ---\")\n","\n","    # Initialize and apply the DBSCAN model\n","    # DBSCAN does not have a 'fit' phase like supervised models; it directly clusters.\n","    # It also does not typically use a separate X_train_normal for its clustering\n","    # of X_eval unless you're trying a very specific semi-supervised approach.\n","    dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n","\n","    # fit_predict returns the cluster labels for each sample (-1 for noise)\n","    dbscan_labels = dbscan_model.fit_predict(X_eval)\n","\n","    # Add the cluster labels to the DataFrame\n","    X_eval_scored_dbscan = X_eval.copy()\n","    X_eval_scored_dbscan['dbscan_cluster_label'] = dbscan_labels\n","\n","    # Identify anomalies: In DBSCAN, points with label -1 are considered noise/outliers.\n","    X_eval_scored_dbscan['dbscan_is_anomaly'] = (dbscan_labels == -1).astype(int) # 1 if anomaly, 0 if normal cluster\n","\n","    print(\"DBSCAN application complete.\")\n","    print(f\"Number of clusters found (excluding noise): {len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)}\")\n","    print(f\"Number of noise points (anomalies) detected: {np.sum(dbscan_labels == -1)}\")\n","\n","    return X_eval_scored_dbscan"],"metadata":{"id":"sp_ZciqkqD1D","executionInfo":{"status":"ok","timestamp":1750462237380,"user_tz":-60,"elapsed":43,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Calling the function to train and score data using the DBSCAN model\n","\n","# Step 1: Use your chosen eps value\n","my_eps_value = 7 # As determined from your k-distance graph\n","\n","# Step 2: Use your determined min_samples value\n","# This was typically 2 * number_of_features from plot_k_distance_graph\n","my_min_samples_value = 2 * X1_eval.shape[1]\n","print(f\"Using min_samples={my_min_samples_value}\")\n","\n","\n","# Step 3: Call the DBSCAN scoring function\n","X_eval_dbscan_scored = train_and_score_dbscan(X1_eval.copy(), my_eps_value, my_min_samples_value)\n","\n","# Display results\n","if not X_eval_dbscan_scored.empty:\n","    print(\"\\n--- X_eval (DBSCAN Scored) Head ---\")\n","    print(X_eval_dbscan_scored.head())\n","    print(\"\\nAnomaly counts in X_eval (DBSCAN):\")\n","    print(X_eval_dbscan_scored['dbscan_is_anomaly'].value_counts())\n","    print(\"\\nCluster Label counts (DBSCAN):\")\n","    print(X_eval_dbscan_scored['dbscan_cluster_label'].value_counts())"],"metadata":{"id":"7DxwaBpkq6t_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analyze_and_visualize_dbscan_anomalies(\n","    X_eval_dbscan_scored: pd.DataFrame,\n","    my_clean_base_df: pd.DataFrame, # Original clean data with raw prices/volumes\n","    num_top_anomalies_to_plot: int = 5,\n","    window_days_for_plot: int = 40 # Days before and after anomaly for time-series plot\n","):\n","    \"\"\"\n","    Performs detailed analysis and visualization of anomalies detected by DBSCAN.\n","\n","    Args:\n","        X_eval_dbscan_scored (pd.DataFrame): DataFrame with features and DBSCAN labels\n","                                            (assumed to have MultiIndex 'Date', 'Ticker').\n","        my_clean_base_df (pd.DataFrame): The original, cleaned DataFrame with raw prices/volumes\n","                                         (MultiIndex 'Date', 'Ticker'). Used for context.\n","        num_top_anomalies_to_plot (int): Number of top anomalous events to plot in detail.\n","        window_days_for_plot (int): Number of calendar days before and after anomaly for time-series plot.\n","    \"\"\"\n","    if X_eval_dbscan_scored.empty:\n","        print(\"X_eval_dbscan_scored DataFrame is empty. No analysis to perform.\")\n","        return\n","\n","    print(\"\\n--- Starting DBSCAN Anomaly Analysis and Visualization ---\")\n","\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    sns.set_style(\"whitegrid\")\n","\n","    # --- 1. Prepare Merged Data for Analysis ---\n","    # This part merges DBSCAN results with original price data for context\n","    eval_start_date = X_eval_dbscan_scored.index.get_level_values('Date').min()\n","    eval_end_date = X_eval_dbscan_scored.index.get_level_values('Date').max()\n","\n","    original_eval_data_base = my_clean_base_df.loc[\n","        (my_clean_base_df.index.get_level_values('Date') >= eval_start_date) &\n","        (my_clean_base_df.index.get_level_values('Date') <= eval_end_date)\n","    ].copy()\n","\n","    original_eval_data_base = original_eval_data_base[~original_eval_data_base.index.duplicated(keep='first')].copy()\n","    original_eval_data_base['unique_id'] = original_eval_data_base.index.get_level_values('Date').astype(str) + '_' + \\\n","                                          original_eval_data_base.index.get_level_values('Ticker').astype(str)\n","    original_eval_data_base.set_index('unique_id', inplace=True)\n","\n","    eval_scores_features_base = X_eval_dbscan_scored.copy()\n","    eval_scores_features_base = eval_scores_features_base[~eval_scores_features_base.index.duplicated(keep='first')].copy()\n","\n","    eval_scores_features_base['unique_id'] = eval_scores_features_base.index.get_level_values('Date').astype(str) + '_' + \\\n","                                           eval_scores_features_base.index.get_level_values('Ticker').astype(str)\n","    eval_scores_features_base.set_index('unique_id', inplace=True)\n","\n","    cols_to_merge_from_dbscan = [col for col in eval_scores_features_base.columns\n","                                 if col not in original_eval_data_base.columns or col in ['dbscan_cluster_label', 'dbscan_is_anomaly']]\n","\n","    full_eval_data_merged_for_analysis = original_eval_data_base.merge(\n","        eval_scores_features_base[cols_to_merge_from_dbscan],\n","        left_index=True,\n","        right_index=True,\n","        how='left'\n","    )\n","\n","    full_eval_data_merged_for_analysis = full_eval_data_merged_for_analysis.reset_index().copy()\n","    full_eval_data_merged_for_analysis[['Date', 'Ticker']] = full_eval_data_merged_for_analysis['unique_id'].str.split('_', expand=True)\n","    full_eval_data_merged_for_analysis['Date'] = pd.to_datetime(full_eval_data_merged_for_analysis['Date'])\n","    full_eval_data_merged_for_analysis.drop(columns=['unique_id'], inplace=True)\n","\n","    anomalies_df_flat = full_eval_data_merged_for_analysis[full_eval_data_merged_for_analysis['dbscan_is_anomaly'] == 1].copy()\n","\n","    if not anomalies_df_flat.empty:\n","        anomalies_df = anomalies_df_flat.set_index(['Date', 'Ticker']).sort_index()\n","    else:\n","        anomalies_df = pd.DataFrame()\n","\n","\n","    # --- 2. Overall Anomaly Summary ---\n","    total_anomalies = anomalies_df.shape[0]\n","    print(f\"\\n2. Overall Anomaly Summary (DBSCAN):\")\n","    print(f\"Total anomalies detected: {total_anomalies}\")\n","\n","    if total_anomalies > 0:\n","        print(\"\\nTop 10 Tickers with Most Anomalies (DBSCAN):\")\n","        print(anomalies_df.index.get_level_values('Ticker').value_counts().head(10))\n","\n","        print(\"\\nFirst 10 Detected Anomalies (DBSCAN):\")\n","        print(anomalies_df.head(10))\n","    else:\n","        print(\"No anomalies detected by DBSCAN.\")\n","        return\n","\n","\n","    # --- 3. Cluster Label Distribution ---\n","    print(\"\\n3. DBSCAN Cluster Label Distribution:\")\n","    print(full_eval_data_merged_for_analysis['dbscan_cluster_label'].value_counts().sort_index())\n","\n","    # --- 4. Feature Comparison: Anomalies vs. Normal Data (Box Plots) ---\n","    print(\"\\n4. Feature Comparison: Anomalies vs. Normal Data (Box Plots):\")\n","\n","    feature_cols = [col for col in full_eval_data_merged_for_analysis.columns\n","                    if pd.api.types.is_numeric_dtype(full_eval_data_merged_for_analysis[col])\n","                    and col not in ['Date', 'Ticker', 'dbscan_cluster_label', 'dbscan_is_anomaly']\n","                    and col not in ['open', 'high', 'low', 'close', 'adj close', 'volume']]\n","\n","    comparison_features = ['log_return', 'rolling_std_20d_log_return', 'relative_volume', 'deviation_from_daily_median_return']\n","    existing_comparison_features = [f for f in comparison_features if f in feature_cols]\n","\n","    if not existing_comparison_features:\n","        existing_comparison_features = feature_cols[:min(5, len(feature_cols))]\n","        print(f\"⚠️ Using a fallback list of {len(existing_comparison_features)} features for comparison plots.\")\n","\n","    if not existing_comparison_features:\n","        print(\"⚠️ No suitable features found for comparison plots after all filters. Skipping.\")\n","    else:\n","        fig, axes = plt.subplots(len(existing_comparison_features), 1, figsize=(12, 5 * len(existing_comparison_features)))\n","        if len(existing_comparison_features) == 1: axes = [axes]\n","\n","        for i, feature in enumerate(existing_comparison_features):\n","            sns.boxplot(x='dbscan_is_anomaly', y=feature, data=full_eval_data_merged_for_analysis, ax=axes[i])\n","            axes[i].set_title(f'Distribution of {feature} for Normal (0) vs. Anomaly (1) (DBSCAN)')\n","            axes[i].set_xlabel('Is Anomaly (0=Normal, 1=Anomaly)')\n","            axes[i].set_ylabel(feature)\n","        plt.tight_layout()\n","        plt.show()\n","\n","    # --- 5. Anomaly Frequency Over Time ---\n","    print(\"\\n5. Anomaly Frequency Over Time (DBSCAN):\")\n","    anomalies_per_day = anomalies_df_flat['Date'].value_counts().sort_index()\n","    if not anomalies_per_day.empty:\n","        plt.figure(figsize=(15, 6))\n","        anomalies_per_day.plot(kind='line', marker='o', linestyle='-', color='blue')\n","        plt.title('Number of DBSCAN Anomalies Detected Per Day')\n","        plt.xlabel('Date')\n","        plt.ylabel('Number of Anomalies')\n","        plt.grid(True)\n","        plt.tight_layout()\n","        plt.show()\n","    else:\n","        print(\"No daily anomaly counts to plot for DBSCAN.\")\n","\n","    # --- NEW: 6. DBSCAN Cluster Visualization (using PCA) ---\n","    print(\"\\n6. DBSCAN Cluster Visualization (2D PCA Projection):\")\n","\n","    # Get the features used for DBSCAN (excluding labels, Date, Ticker, original prices)\n","    # This assumes these features were already scaled.\n","    features_for_pca = [col for col in X_eval_dbscan_scored.columns # Use the original scored DF for feature list\n","                        if pd.api.types.is_numeric_dtype(X_eval_dbscan_scored[col]) # Ensure numerical\n","                        and col not in ['dbscan_cluster_label', 'dbscan_is_anomaly']\n","                        and col not in ['open', 'high', 'low', 'close', 'adj close', 'volume']] # Exclude original prices/volumes\n","\n","    # Ensure there are enough features for PCA\n","    if len(features_for_pca) < 2:\n","        print(\"⚠️ Not enough numerical features (less than 2) for PCA visualization. Skipping cluster plot.\")\n","    else:\n","        # Create a DataFrame with just the features and ensure no NaNs\n","        X_for_pca = X_eval_dbscan_scored[features_for_pca].dropna()\n","\n","        if X_for_pca.empty:\n","            print(\"⚠️ No data for PCA after dropping NaNs. Skipping cluster plot.\")\n","        else:\n","            # OPTIONAL: Sample data for faster plotting if dataset is very large\n","            max_plot_points = 50000 # Limit to avoid extremely slow plots\n","            if X_for_pca.shape[0] > max_plot_points:\n","                print(f\"Sampling {max_plot_points} points for PCA plot due to large dataset size...\")\n","                X_for_pca_sampled = X_for_pca.sample(max_plot_points, random_state=42)\n","            else:\n","                X_for_pca_sampled = X_for_pca\n","\n","            # Perform PCA\n","            pca = PCA(n_components=2)\n","            principal_components = pca.fit_transform(X_for_pca_sampled)\n","\n","            # Create a DataFrame for plotting PCA results, retaining index for labels\n","            pca_df = pd.DataFrame(data = principal_components, columns = ['principal_component_1', 'principal_component_2'], index=X_for_pca_sampled.index)\n","\n","            # Merge back DBSCAN labels for plotting\n","            # Use X_eval_dbscan_scored (which has the MultiIndex intact)\n","            pca_df['cluster_label'] = X_eval_dbscan_scored.loc[pca_df.index, 'dbscan_cluster_label']\n","            pca_df['is_anomaly'] = X_eval_dbscan_scored.loc[pca_df.index, 'dbscan_is_anomaly']\n","\n","            plt.figure(figsize=(12, 10))\n","\n","            # Plot clusters\n","            sns.scatterplot(\n","                x='principal_component_1',\n","                y='principal_component_2',\n","                hue='cluster_label', # Color by cluster label\n","                palette='tab10',     # Use a distinct color palette\n","                data=pca_df[pca_df['is_anomaly'] == 0], # Plot normal points\n","                s=20, # Size of points\n","                alpha=0.7, # Transparency\n","                legend='full'\n","            )\n","\n","            # Plot anomalies separately (label -1)\n","            anomalies_pca = pca_df[pca_df['is_anomaly'] == 1]\n","            if not anomalies_pca.empty:\n","                sns.scatterplot(\n","                    x='principal_component_1',\n","                    y='principal_component_2',\n","                    data=anomalies_pca,\n","                    color='red', # Anomalies in red\n","                    marker='X', # Use 'X' marker for anomalies\n","                    s=100, # Larger size for anomalies\n","                    label='Anomalies (Noise)',\n","                    zorder=5 # Ensure anomalies are on top\n","                )\n","\n","            plt.title('DBSCAN Clusters and Anomalies (2D PCA Projection)')\n","            plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)')\n","            plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)')\n","            plt.legend(title='Cluster Label')\n","            plt.grid(True)\n","            plt.show()\n","\n","    # --- 7. Detailed Time-Series Plots for Sample Anomalies (Same as before) ---\n","    print(f\"\\n7. Detailed Time-Series Plots for Sample DBSCAN Anomalies:\")\n","\n","    sample_anomaly_indices = anomalies_df.sample(min(anomalies_df.shape[0], num_top_anomalies_to_plot), random_state=42).index.tolist()\n","\n","    if not sample_anomaly_indices:\n","        print(\"No sample anomalies to plot in detail.\")\n","        return\n","\n","    plot_source_df_flat = full_eval_data_merged_for_analysis.copy()\n","\n","    required_plot_cols = ['close', 'log_return', 'relative_volume']\n","    if not all(col in plot_source_df_flat.columns for col in required_plot_cols):\n","        print(f\"Error: Missing one or more required plotting columns ({', '.join(required_plot_cols)}) in the main DataFrame. Skipping detailed plots.\")\n","        return\n","\n","    for anomaly_idx in tqdm(sample_anomaly_indices, desc=\"Plotting Sample DBSCAN Anomalies\"):\n","        anomaly_date = anomaly_idx[0]\n","        anomaly_ticker = anomaly_idx[1]\n","\n","        print(f\"\\nVisualizing anomaly for {anomaly_ticker} on {anomaly_date.date()}\")\n","\n","        ticker_data_for_plot_flat = plot_source_df_flat[\n","            (plot_source_df_flat['Ticker'] == anomaly_ticker) &\n","            (plot_source_df_flat['Date'] >= (anomaly_date - pd.Timedelta(days=window_days_for_plot))) &\n","            (plot_source_df_flat['Date'] <= (anomaly_date + pd.Timedelta(days=window_days_for_plot)))\n","        ].copy()\n","\n","        plot_data = ticker_data_for_plot_flat.set_index('Date').sort_index()\n","\n","        if plot_data.empty:\n","            print(f\"No data available for {anomaly_ticker} around {anomaly_date.date()} for plotting window. Skipping.\")\n","            continue\n","\n","        plt.figure(figsize=(15, 10))\n","\n","        ax1 = plt.subplot(3, 1, 1)\n","        ax1.plot(plot_data.index, plot_data['close'], label=f'{anomaly_ticker} Close Price', color='blue')\n","\n","        anomalies_in_window = plot_data[plot_data['dbscan_is_anomaly'] == 1] # Use dbscan_is_anomaly\n","        if not anomalies_in_window.empty:\n","            ax1.scatter(anomalies_in_window.index, anomalies_in_window['close'],\n","                        color='red', s=100, zorder=5, label='Detected Anomaly')\n","\n","        ax1.set_title(f'Close Price for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax1.set_ylabel('Close Price')\n","        ax1.legend()\n","        ax1.grid(True)\n","\n","        ax2 = plt.subplot(3, 1, 2, sharex=ax1)\n","        ax2.plot(plot_data.index, plot_data['log_return'], label=f'{anomaly_ticker} Log Return', color='green')\n","        if not anomalies_in_window.empty:\n","            ax2.scatter(anomalies_in_window.index, anomalies_in_window['log_return'], color='red', s=100, zorder=5)\n","        ax2.set_ylabel('Log Return')\n","        ax2.set_title(f'Log Return for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax2.legend()\n","        ax2.grid(True)\n","\n","        ax3 = plt.subplot(3, 1, 3, sharex=ax1)\n","        ax3.plot(plot_data.index, plot_data['relative_volume'], label=f'{anomaly_ticker} Relative Volume', color='purple')\n","        if not anomalies_in_window.empty:\n","            ax3.scatter(anomalies_in_window.index, anomalies_in_window['relative_volume'], color='red', s=100, zorder=5)\n","        ax3.set_ylabel('Relative Volume')\n","        ax3.set_title(f'Relative Volume for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax3.set_xlabel('Date')\n","        ax3.legend()\n","        ax3.grid(True)\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    print(\"\\n--- DBSCAN Anomaly Analysis and Visualization Complete ---\")"],"metadata":{"id":"pEJChjAfuUQO","executionInfo":{"status":"ok","timestamp":1750463001368,"user_tz":-60,"elapsed":22,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["\n","# Call the DBSCAN anomaly analysis function\n","analyze_and_visualize_dbscan_anomalies(\n","    X_eval_dbscan_scored.copy(), # Pass a copy if you don't want the original altered\n","    ready_ftse100_data.copy(),     # Pass a copy for the original data\n","    num_top_anomalies_to_plot=5\n",")"],"metadata":{"id":"nTwmqfhosha4"},"execution_count":null,"outputs":[]}]}