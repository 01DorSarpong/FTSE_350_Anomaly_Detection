{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m7jN0d3qJZZK","outputId":"4d932567-c5b7-40de-c549-f46d8eb130e8","executionInfo":{"status":"ok","timestamp":1751931892904,"user_tz":-60,"elapsed":26874,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Importing drive model from the google.colab package\n","from google.colab import drive\n","\n","#Mounting the google drive to a specific path\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_8kLZ9NLAgy","outputId":"f33c7eb8-71bc-4f4b-9536-e6a85662d6c4","executionInfo":{"status":"ok","timestamp":1751931897616,"user_tz":-60,"elapsed":1960,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection\n"]}]},{"cell_type":"code","source":["# Configuring Git user details\n","!git config --global user.email \"dorothy.sarpongk@gmail.com\"\n","!git config --global user.name \"01DorSarpong\""],"metadata":{"id":"sdbbjCzE0tWT","executionInfo":{"status":"ok","timestamp":1751931902424,"user_tz":-60,"elapsed":2420,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"G-nzEs8x2I93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importing libraries for code\n","\n","import pandas as pd\n","import numpy  as np\n","import yfinance as yf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import IsolationForest\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.cluster import DBSCAN\n","from sklearn.preprocessing import MinMaxScaler\n","from tqdm import tqdm\n","from typing import Tuple, Union, Dict, Any\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, RepeatVector, TimeDistributed\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.model_selection import ParameterGrid\n"],"metadata":{"id":"jQ_wBKNkmV25","executionInfo":{"status":"ok","timestamp":1751931969000,"user_tz":-60,"elapsed":2087,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# A function to download and save FTSE 100 and FTSE 250 stocks\n","\n","def download_and_save_FTSE_stocks(tickers: list, start_date: str, end_date: str, directory: str):\n","\n"," \"\"\" This function downloads historical stock data for a list of tickers and saves it to CSV files.\n","\n","  Args:\n","    tickers_list (list): A list of stock ticker symbols (e.g., ['TSCO.L', 'BARC.L']).\n","    start_date (str): The start date for data download in 'YYYY-MM-DD' format.\n","    end_date (str): The end date for data download in 'YYYY-MM-DD' format.\n","    directory (str): The path to the directory where CSV files will be saved.\n","  \"\"\"\n","\n","  # Ensure the save directory exists\n"," if not os.path.exists(directory):\n","    os.makedirs(directory)\n","    print(f\"Created directory: {directory}\")\n","\n"," print(f\"Starting download for {len(tickers)} tickers from {start_date} to {end_date}...\")\n","\n"," for ticker in tqdm(tickers, desc=\"Downloading Stocks\"):\n","    # Format the filename: remove '.L' and add date range for clarity\n","    cleaned_ticker = ticker.replace('.L', '')\n","    file_name = f\"{cleaned_ticker}_{start_date.replace('-', '')}_{end_date.replace('-', '')}.csv\"\n","    full_file_path = os.path.join(directory, file_name)\n","\n","    try:\n","      df = yf.download(ticker, start=start_date, end=end_date, auto_adjust=False)\n","\n","      if not df.empty:\n","        df.to_csv(full_file_path)\n","        # print(f\"✅ Saved data for {ticker} to {full_file_path}\") # Optional: uncomment for more verbose output\n","      else:\n","        print(f\"⚠️ No data available for {ticker} for the specified period.\")\n","    except Exception as e:\n","      print(f\"❌ Error downloading or saving data for {ticker}: {e}\")\n","\n"," print(\"Download process completed.\")\n"],"metadata":{"id":"6Iby76j7PtZR","executionInfo":{"status":"ok","timestamp":1751932065618,"user_tz":-60,"elapsed":7,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#Creating a list of FTSE 100 and FTSE 250 tickers\n","\n","FTSE_100_tickers = [\"AZN.L\", \"HSBA.L\", \"ULVR.L\", \"REL.L\", \"BATS.L\", \"BP.L\", \"GSK.L\", \"DGE.L\",\n","                   \"RR.L\", \"NG.L\", \"BARC.L\", \"TSCO.L\", \"PRU.L\", \"BHP.L\", \"BT-A.L\",]\n","\n","FTSE_250_tickers = [\"BWY.L\", \"EMG.L\", \"JUST.L\", \"SXS.L\", \"CKN.L\", \"LRE.L\", \"RAT.L\", \"THG.L\",\n","                    \"JDW.L\", \"SCT.L\", \"DOM.L\", \"SRE.L\", \"HIK.L\", \"ICGT.L\", \"HSX.L\"]"],"metadata":{"id":"2L4EdTEsuCu8","executionInfo":{"status":"ok","timestamp":1751932076040,"user_tz":-60,"elapsed":18,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#Defining the period for stocks range\n","start_date = \"2014-01-01\"\n","end_date = \"2024-12-31\""],"metadata":{"id":"yESfV6rzUuQ1","executionInfo":{"status":"ok","timestamp":1751932079954,"user_tz":-60,"elapsed":34,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Defining the path to save the CSVs\n","\n","ftse_100_path = '/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection/FTSE_100'\n","ftse_250_path = '/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection/FTSE_250'\n"],"metadata":{"id":"w9mFm0wLVH6v","executionInfo":{"status":"ok","timestamp":1751932081480,"user_tz":-60,"elapsed":9,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Calling the function for FTSE 100 and FTSE 250 tickers\n","\n","download_and_save_FTSE_stocks(\n","    tickers=FTSE_100_tickers,\n","    start_date=start_date,\n","    end_date=end_date,\n","    directory=ftse_100_path\n",")\n","\n","download_and_save_FTSE_stocks(\n","    tickers=FTSE_250_tickers,\n","    start_date=start_date,\n","    end_date=end_date,\n","    directory=ftse_250_path\n",")"],"metadata":{"id":"dnZIuB7oXMSA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a function to load stocks and pre-process into a dataframe\n","\n","def load_and_structure_stock_data(folder_path: str) -> pd.DataFrame:\n","    \"\"\"\n","    Loads historical stock data from CSV files in a specified folder,\n","    cleans, processes, filters to weekdays, fills NaNs/gaps, and\n","    consolidates them into a single structured DataFrame with a MultiIndex.\n","\n","    Args:\n","        folder_path (str): The path to the directory containing stock data CSVs.\n","\n","    Returns:\n","        pd.DataFrame: A single DataFrame containing data for all tickers,\n","                      indexed by 'Date' and 'Ticker', sorted by Date and then Ticker.\n","                      Returns an empty DataFrame if no data is loaded or processed.\n","    \"\"\"\n","    all_dfs = []\n","\n","    if not os.path.exists(folder_path):\n","        print(f\"❌ Error: Folder not found at {folder_path}\")\n","        return pd.DataFrame() # Return empty DataFrame if folder doesn't exist\n","\n","    # Get list of CSV files to process\n","    file_list = [f.name for f in os.scandir(folder_path) if f.name.endswith(\".csv\")]\n","\n","    if not file_list:\n","        print(f\"⚠️ No CSV files found in {folder_path}\")\n","        return pd.DataFrame()\n","\n","    print(f\"Loading and processing data from {len(file_list)} CSV files in {folder_path}...\")\n","\n","    # Define columns that typically contain numerical stock data to be filled\n","    numerical_cols_to_fill = ['open', 'high', 'low', 'close', 'adj close', 'volume']\n","\n","    for filename in tqdm(file_list, desc=\"Processing Stock Files\"):\n","        file_path = os.path.join(folder_path, filename)\n","\n","        try:\n","            # Read CSV: use first row as header, skip second row (often contains ticker name repeated)\n","            df = pd.read_csv(file_path, header=0, skiprows=[1], encoding='utf-8-sig')\n","\n","            # --- Initial Cleaning and Date Conversion ---\n","            # Rename the first column to 'Date' if it's not already\n","            if df.columns[0].strip().lower() != 'date':\n","                df.rename(columns={df.columns[0]: 'Date'}, inplace=True)\n","\n","            # Normalize all column names to lowercase and remove leading/trailing spaces\n","            df.columns = [col.strip().lower() for col in df.columns]\n","\n","            # Clean and prepare the 'date' column\n","            df['date'] = df['date'].astype(str).str.strip()\n","            df = df[df['date'].str.lower() != 'date'] # Drop any rows where the date column contains the string 'Date'\n","\n","            # Convert to datetime, coercing errors will turn unparseable dates into NaT\n","            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n","            df.dropna(subset=['date'], inplace=True) # Drop rows where date conversion failed (NaT)\n","\n","            # --- Add Ticker Information ---\n","            # Extract ticker from filename (e.g., 'TSCO_20140101_20241231.csv' -> 'TSCO')\n","            ticker_symbol = filename.split('_')[0]\n","            df['ticker'] = ticker_symbol\n","\n","            # --- Set Date as Index (temporarily for filtering/filling) ---\n","            df.set_index('date', inplace=True)\n","\n","            # --- Filter for Weekdays Only ---\n","            # .dayofweek returns Monday=0, ..., Sunday=6. Keep only 0 to 4.\n","            df = df[df.index.dayofweek < 5]\n","\n","            # --- Fill NaNs/Gaps for Numerical Columns ---\n","            # Identify columns to fill that actually exist in the current DataFrame\n","            existing_numerical_cols = [col for col in numerical_cols_to_fill if col in df.columns]\n","\n","            if not existing_numerical_cols:\n","                # print(f\"⚠️ No numerical columns found for {filename}. Skipping NaN filling for this file.\")\n","                pass # This is fine, just means the file might only have non-numerical data or very specific columns\n","\n","            # Apply forward-fill then backward-fill for numerical columns within this ticker's data\n","            # This handles gaps for individual stock series\n","            df[existing_numerical_cols] = df[existing_numerical_cols].ffill().bfill()\n","\n","            # --- Final Check for essential data after filling ---\n","            # If 'open' column is vital and still has NaNs (e.g., entire series was NaN), drop those rows\n","            if 'open' in df.columns:\n","                df.dropna(subset=['open'], inplace=True)\n","            else:\n","                print(f\"⚠️ 'open' column not found in {filename}. Skipping this file as essential data is missing.\")\n","                continue # Skip this file if 'open' is genuinely missing\n","\n","            if df.empty:\n","                print(f\"⚠️ No valid weekday data remaining for {filename} after filtering. Skipping.\")\n","                continue\n","\n","            all_dfs.append(df)\n","\n","        except pd.errors.EmptyDataError:\n","            print(f\"⚠️ {filename} is empty. Skipping.\")\n","        except pd.errors.ParserError as e:\n","            print(f\"❌ Error parsing {filename}: {type(e).__name__} - {e}. Skipping.\")\n","        except Exception as e:\n","            print(f\"❌ Error processing {filename}: {type(e).__name__} - {e}. Skipping.\")\n","\n","    if not all_dfs:\n","        print(\"No valid stock data loaded after processing. Returning empty DataFrame.\")\n","        return pd.DataFrame()\n","\n","    # --- Consolidate all DataFrames into one structured DataFrame ---\n","    combined_df = pd.concat(all_dfs)\n","\n","    # Set a MultiIndex: primary index is 'Date', secondary is 'ticker'\n","    # 'ticker' was added as a regular column inside the loop, now it becomes part of the index\n","    combined_df.set_index('ticker', append=True, inplace=True)\n","    combined_df.index.names = ['Date', 'Ticker']\n","\n","    # Sort the MultiIndex for better performance and consistency\n","    combined_df.sort_index(inplace=True)\n","\n","    print(\"All stock data loaded, structured, filtered, and filled successfully.\")\n","    return combined_df\n","\n"],"metadata":{"id":"oQt_Z87EH2L9","executionInfo":{"status":"ok","timestamp":1751932087474,"user_tz":-60,"elapsed":27,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["#Calling the function to load and structure df for FTSE 100 stocks\n","\n","ready_ftse100_data = load_and_structure_stock_data(ftse_100_path)\n","\n","if not ready_ftse100_data.empty:\n","  print(\"\\n--- Final Structured and Cleaned DataFrame ---\")\n","  print(ready_ftse100_data.head(15)) # Show more rows to see multiple dates/tickers\n","  print(\"\\nDataFrame Info:\")\n","  ready_ftse100_data.info()\n","  print(\"\\nSample of weekdays (should only be Mon-Fri):\")\n","  print(ready_ftse100_data.index.get_level_values('Date').day_name().value_counts())\n","  print(\"\\nNaNs after processing (should be very few or none in numerical columns):\")\n","  print(ready_ftse100_data.isnull().sum())\n","else:\n"," print(\"No data was successfully loaded and processed.\")"],"metadata":{"id":"gK9gOL3vIK_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calling the function to load and structure df for FTSE 250 stocks\n","\n","ready_ftse250_data = load_and_structure_stock_data(ftse_250_path)\n","\n","if not ready_ftse250_data.empty:\n","  print(\"\\n--- Final Structured and Cleaned DataFrame ---\")\n","  print(ready_ftse250_data.head(15)) # Show more rows to see multiple dates/tickers\n","  print(\"\\nDataFrame Info:\")\n","  ready_ftse250_data.info()\n","  print(\"\\nSample of weekdays (should only be Mon-Fri):\")\n","  print(ready_ftse250_data.index.get_level_values('Date').day_name().value_counts())\n","  print(\"\\nNaNs after processing (should be very few or none in numerical columns):\")\n","  print(ready_ftse250_data.isnull().sum())\n","else:\n"," print(\"No data was successfully loaded and processed.\")"],"metadata":{"id":"XktAAHheAscj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to genereate features needed to build the Isolation Forest model\n","def generate_all_stock_features(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Generates a comprehensive set of time-series and cross-sectional numerical features\n","    for stock data, suitable for unsupervised anomaly detection models.\n","\n","    This function assumes the input DataFrame has a MultiIndex (Date, Ticker)\n","    and contains cleaned base columns like 'open', 'high', 'low', 'close',\n","    'adj close', and 'volume', with no critical NaNs.\n","\n","    Args:\n","        df (pd.DataFrame): Your clean DataFrame with MultiIndex (Date, Ticker)\n","                           and base stock price/volume data.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with all original and newly engineered numerical\n","                      features. NaNs introduced by calculations (e.g., at the start\n","                      of rolling windows) will be present.\n","    \"\"\"\n","    if df.empty:\n","        print(\"Input DataFrame is empty for feature generation. Returning empty DataFrame.\")\n","        return pd.DataFrame()\n","\n","    processed_df = df.copy() # Always work on a copy to keep the original untouched\n","\n","    # --- Ensure critical columns are numerical for calculations ---\n","    for col in ['open', 'high', 'low', 'close', 'adj close', 'volume']:\n","        if col in processed_df.columns:\n","            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')\n","    processed_df.dropna(subset=['open','close','volume'], inplace=True)\n","    if processed_df.empty:\n","        print(\"No data remaining after ensuring essential columns are numerical and not NaN.\")\n","        return pd.DataFrame()\n","\n","\n","    grouped_by_ticker = processed_df.groupby(level='Ticker')\n","    grouped_by_date = processed_df.groupby(level='Date')\n","    epsilon = 1e-9 # Small constant to avoid division by zero\n","\n","    print(\"Generating Time-Series Based Features (per stock ticker)...\")\n","\n","    # 1. Return-Based Features:\n","    processed_df['log_return'] = grouped_by_ticker['close'].transform(\n","        lambda x: np.log(x / x.shift(1).replace(0, epsilon)).replace([-np.inf, np.inf], np.nan)\n","    )\n","    processed_df['simple_return'] = grouped_by_ticker['close'].transform(\n","        lambda x: (x / x.shift(1).replace(0, epsilon)) - 1\n","    )\n","    if 'adj close' in processed_df.columns:\n","        processed_df['log_adj_close_return'] = grouped_by_ticker['adj close'].transform(\n","            lambda x: np.log(x / x.shift(1).replace(0, epsilon)).replace([-np.inf, np.inf], np.nan)\n","        )\n","    processed_df['return_5d'] = grouped_by_ticker['close'].transform(lambda x: x.pct_change(periods=5))\n","    processed_df['return_20d'] = grouped_by_ticker['close'].transform(lambda x: x.pct_change(periods=20))\n","\n","    print(\"Generating Volatility Measures...\")\n","    # 2. Volatility Measures:\n","    # Added min_periods=1 for rolling calculations\n","    processed_df['rolling_std_5d_log_return'] = grouped_by_ticker['log_return'].transform(lambda x: x.rolling(window=5, min_periods=1).std())\n","    processed_df['rolling_std_20d_log_return'] = grouped_by_ticker['log_return'].transform(lambda x: x.rolling(window=20, min_periods=1).std())\n","\n","    if 'high' in processed_df.columns and 'low' in processed_df.columns:\n","        processed_df['daily_range_norm'] = (processed_df['high'] - processed_df['low']) / (processed_df['close'] + epsilon)\n","\n","        log_high_div_low = np.log((processed_df['high'] / processed_df['low'].replace(0, epsilon)).clip(lower=epsilon))\n","        log_close_div_open = np.log((processed_df['close'] / processed_df['open'].replace(0, epsilon)).clip(lower=epsilon))\n","\n","        gk_term = 0.5 * (log_high_div_low)**2 - (2 * np.log(2) - 1) * (log_close_div_open)**2\n","        gk_term[gk_term < 0] = np.nan # Set negative values before sqrt to NaN\n","        processed_df['garman_klass_vol'] = np.sqrt(gk_term)\n","\n","        # Explicitly replace infs/NaNs that might result from sqrt or division by zero, and fill\n","        processed_df['garman_klass_vol'] = processed_df['garman_klass_vol'].replace([-np.inf, np.inf], np.nan)\n","        processed_df['garman_klass_vol'].fillna(0, inplace=True) # Fill with 0 for Garman-Klass specific NaNs\n","    else:\n","        print(\"⚠️ Missing 'high' or 'low' columns for some volatility features. Skipping.\")\n","\n","    print(\"Generating Volume-Based Features...\")\n","    # 3. Volume-Based Features:\n","    processed_df['volume_change'] = grouped_by_ticker['volume'].transform(lambda x: x.pct_change(periods=1))\n","    # Added min_periods=1 for rolling calculations\n","    processed_df['avg_volume_20d'] = grouped_by_ticker['volume'].transform(lambda x: x.rolling(window=20, min_periods=1).mean())\n","    processed_df['relative_volume'] = processed_df['volume'] / (processed_df['avg_volume_20d'] + epsilon)\n","\n","    print(\"Generating Momentum/Trend Indicators...\")\n","    # 4. Momentum/Trend Indicators:\n","    # Added min_periods=1 for rolling calculations\n","    processed_df['sma_5d'] = grouped_by_ticker['close'].transform(lambda x: x.rolling(window=5, min_periods=1).mean())\n","    processed_df['sma_20d'] = grouped_by_ticker['close'].transform(lambda x: x.rolling(window=20, min_periods=1).mean())\n","    processed_df['deviation_from_sma_20d'] = (processed_df['close'] - processed_df['sma_20d']) / (processed_df['sma_20d'] + epsilon)\n","\n","    print(\"Generating Price-Volume Interaction Features...\")\n","    # 5. Price-Volume Interaction Features:\n","    if 'high' in processed_df.columns and 'low' in processed_df.columns:\n","        processed_df['typical_price'] = (processed_df['high'] + processed_df['low'] + processed_df['close']) / 3\n","    else:\n","        print(\"⚠️ Missing 'high' or 'low' columns. Skipping 'typical_price'.\")\n","\n","    print(\"Generating Cross-Sectional Features (comparing stocks on the same day)...\")\n","    # 6. Cross-Sectional Features:\n","    if 'log_return' in processed_df.columns:\n","        processed_df['daily_market_mean_log_return'] = grouped_by_date['log_return'].transform('mean')\n","        processed_df['daily_market_median_log_return'] = grouped_by_date['log_return'].transform('median')\n","\n","        processed_df['deviation_from_daily_mean_return'] = processed_df['log_return'] - processed_df['daily_market_mean_log_return']\n","        processed_df['deviation_from_daily_median_return'] = processed_df['log_return'] - processed_df['daily_market_median_log_return']\n","        processed_df['daily_return_rank_pct'] = grouped_by_date['log_return'].rank(pct=True, method='average')\n","    else:\n","        print(\"⚠️ 'log_return' column not available for cross-sectional feature generation. Skipping these features.\")\n","\n","    # --- FINAL CLEANUP: Replace any remaining inf/-inf with NaN across all numerical columns ---\n","    print(\"Finalizing features: cleaning up any remaining inf/-inf values...\")\n","    numerical_cols_after_gen = processed_df.select_dtypes(include=np.number).columns\n","    for col in numerical_cols_after_gen:\n","        processed_df[col] = processed_df[col].replace([np.inf, -np.inf], np.nan)\n","\n","    print(\"Feature generation complete. NaNs from calculations are present and will need further handling.\")\n","    return processed_df.sort_index()"],"metadata":{"id":"JxKQF6HGuYIH","executionInfo":{"status":"ok","timestamp":1751932183560,"user_tz":-60,"elapsed":71,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Calling the feature generation function on the FTSE 100 stock\n","\n","df_with_all_100_features = generate_all_stock_features(ready_ftse100_data)\n","\n","if not df_with_all_100_features.empty:\n","    print(\"\\n--- NEWLY GENERATED DataFrame with ALL Engineered Features (Head) ---\")\n","    # To ensure you see all columns, even if there are many:\n","    pd.set_option('display.max_columns', None) # Display all columns\n","    pd.set_option('display.width', 1000)      # Ensure wide display in terminal/Colab output\n","\n","    # Now, print the head of the *NEW* DataFrame\n","    print(df_with_all_100_features.head(15))\n","\n","    print(\"\\n--- NEWLY GENERATED DataFrame Info (should show many more columns) ---\")\n","    # Now, print the info of the *NEW* DataFrame\n","    df_with_all_100_features.info()\n","\n","    print(\"\\n--- ALL Column Names in the NEW DataFrame ---\")\n","    # This will explicitly list ALL column names, proving they are there\n","    print(df_with_all_100_features.columns.tolist())\n","\n","    print(\"\\n--- Count of NaNs per column (expect some NaNs from rolling/shifting, esp. at start of series) ---\")\n","    print(df_with_all_100_features.isnull().sum().sort_values(ascending=False).head(20))\n","else:\n","    print(\"The feature generation function returned an empty DataFrame.\")"],"metadata":{"id":"GNB55xrl1RAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Call the feature generation function on the FTSE 100 stock\n","\n","df_with_all_250_features = generate_all_stock_features(ready_ftse250_data)\n","\n","if not df_with_all_250_features.empty:\n","    print(\"\\n--- NEWLY GENERATED DataFrame with ALL Engineered Features (Head) ---\")\n","    # To ensure you see all columns, even if there are many:\n","    pd.set_option('display.max_columns', None) # Display all columns\n","    pd.set_option('display.width', 1000)      # Ensure wide display in terminal/Colab output\n","\n","    # Now, print the head of the *NEW* DataFrame\n","    print(df_with_all_250_features.head(15))\n","\n","    print(\"\\n--- NEWLY GENERATED DataFrame Info (should show many more columns) ---\")\n","    # Now, print the info of the *NEW* DataFrame\n","    df_with_all_250_features.info()\n","\n","    print(\"\\n--- ALL Column Names in the NEW DataFrame ---\")\n","    # This will explicitly list ALL column names, proving they are there\n","    print(df_with_all_250_features.columns.tolist())\n","\n","    print(\"\\n--- Count of NaNs per column (expect some NaNs from rolling/shifting, esp. at start of series) ---\")\n","    print(df_with_all_250_features.isnull().sum().sort_values(ascending=False).head(20))\n","else:\n","    print(\"The feature generation function returned an empty DataFrame.\")"],"metadata":{"id":"pkgU-aLTsJr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to remove all NaNs from features and scales values using standard scaler\n","#from sklearn.preprocessing import StandardScaler\n","\n","def preprocess_features_for_model(\n","    df_with_all_features: pd.DataFrame,\n","    columns_to_exclude_from_features: list = None,\n","    apply_scaling: bool = True,\n","    scaler_obj: StandardScaler = None # Optional: provide a pre-fitted scaler for consistent scaling\n",") -> Tuple[pd.DataFrame, Union[StandardScaler, None]]:\n","    \"\"\"\n","    Handles NaN removal, feature selection, and feature scaling for a DataFrame\n","    containing engineered stock features. This prepares the data for anomaly\n","    detection models.\n","\n","    Args:\n","        df_with_engineered_features (pd.DataFrame): The DataFrame containing\n","                                                    all engineered features, with a MultiIndex.\n","                                                    (Output of `generate_all_stock_features`).\n","        columns_to_exclude_from_features (list, optional): A list of column names\n","                                                        that should NOT be treated as features\n","                                                        for the model (e.g., raw 'open', 'close',\n","                                                        or non-numeric helper columns like 'weekday').\n","                                                        If None, a default list is used.\n","        apply_scaling (bool): Whether to apply StandardScaler to numerical features.\n","                              Defaults to True. Highly recommended for most ML models.\n","        scaler_obj (StandardScaler, optional): An pre-fitted StandardScaler object.\n","                                               If `apply_scaling` is True and `scaler_obj` is None,\n","                                               a new scaler will be fitted. Useful for consistent\n","                                               scaling between train/test datasets.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - pd.DataFrame: The DataFrame with selected, cleaned, and optionally scaled\n","                            numerical features, ready for an anomaly detection model.\n","                            Retains the MultiIndex.\n","            - StandardScaler or None: The fitted or used StandardScaler object if\n","                                      scaling was applied, else None.\n","    \"\"\"\n","    if df_with_all_features.empty:\n","        print(\"Input DataFrame is empty for preprocessing. Returning empty DataFrame.\")\n","        return pd.DataFrame(), None\n","\n","    processed_df = df_with_all_features.copy()\n","    print(\"\\n--- Starting Feature Preprocessing for Model ---\")\n","\n","    # --- 1. Feature Selection (Identify Numerical Features to Use) ---\n","    # Default list of columns that are typically not features, but base data or helpers\n","    if columns_to_exclude_from_features is None:\n","        columns_to_exclude_from_features = ['open', 'high', 'low', 'close', 'adj close', 'volume', 'weekday']\n","\n","    # Get all numerical columns from the DataFrame\n","    all_numerical_cols = processed_df.select_dtypes(include=np.number).columns.tolist()\n","\n","    # Filter out the columns that should be excluded\n","    features_for_model_names = [\n","        col for col in all_numerical_cols\n","        if col not in columns_to_exclude_from_features\n","    ]\n","\n","    if not features_for_model_names:\n","        print(\"⚠️ No valid numerical features identified after exclusion. Using all numerical original columns.\")\n","        features_for_model_names = all_numerical_cols # Fallback to all if custom exclusion leads to empty list\n","\n","    print(f\"Selected {len(features_for_model_names)} features for the model.\")\n","    df_features_only = processed_df[features_for_model_names].copy()\n","\n","\n","    # --- 2. NaN Removal (Final Handling for Model Input) ---\n","    # Drop rows where any of the *selected features* have NaNs.\n","    # This is critical as most ML models cannot handle NaNs.\n","    original_rows_count = df_features_only.shape[0]\n","    df_features_only.dropna(inplace=True)\n","    rows_after_nan_drop = df_features_only.shape[0]\n","\n","    if original_rows_count > rows_after_nan_drop:\n","        print(f\"Dropped {original_rows_count - rows_after_nan_drop} rows due to NaNs in selected features.\")\n","    if df_features_only.empty:\n","        print(\"DataFrame is empty after NaN removal. Cannot proceed with preprocessing.\")\n","        return pd.DataFrame(), None\n","    print(f\"Data shape after NaN removal: {df_features_only.shape}\")\n","\n","\n","    # --- 3. Feature Scaling ---\n","    scaler = None\n","    if apply_scaling:\n","        print(\"Applying StandardScaler to features...\")\n","        scaler = scaler_obj if scaler_obj is not None else StandardScaler()\n","\n","        # Fit and/or transform the features\n","        X_scaled = scaler.fit_transform(df_features_only) if scaler_obj is None else scaler.transform(df_features_only)\n","\n","        # Convert scaled array back to DataFrame, retaining index and column names\n","        df_scaled_features = pd.DataFrame(X_scaled, index=df_features_only.index, columns=df_features_only.columns)\n","        print(f\"Features scaled. Scaler: {'New' if scaler_obj is None else 'Existing'}.\")\n","    else:\n","        df_scaled_features = df_features_only.copy()\n","        print(\"Skipping feature scaling.\")\n","\n","    print(\"--- Feature Preprocessing for Model Complete ---\")\n","    return df_scaled_features, scaler"],"metadata":{"id":"IULteCpM6AMz","executionInfo":{"status":"ok","timestamp":1751932237417,"user_tz":-60,"elapsed":42,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["#Calling the preprocess_features_for_model function on the FTSE 100 stocks\n","\n","final_processed_FTSE100_df, fitted_scaler = preprocess_features_for_model(\n","    df_with_all_100_features.copy(), # Pass a copy to avoid modifying the original df_with_all_features\n","    columns_to_exclude_from_features=None, # Use default exclusion (removes original price/volume/weekday)\n","    apply_scaling=True,                     # Apply StandardScaler (best for all 3 models)\n","    scaler_obj=None                         # Fit a new scaler\n",")\n","if not final_processed_FTSE100_df.empty:\n","    print(\"\\n--- Processed DataFrame (Head) after fixing infs ---\")\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    print(final_processed_FTSE100_df.head(15))\n","\n","    print(\"\\n--- Processed DataFrame (Info) after fixing infs ---\")\n","    final_processed_FTSE100_df.info()\n","\n","    print(\"\\n--- NaNs in Processed DataFrame (should be 0) ---\")\n","    print(final_processed_FTSE100_df.isnull().sum().sum()) # Should be 0 NaNs if successful\n","else:\n","    print(\"Preprocessing returned an empty DataFrame.\")\n","\n","\n","#Calling the preprocess_features_for_model function on the FTSE 250 stocks\n","final_processed_FTSE250_df, fitted_scaler = preprocess_features_for_model(\n","    df_with_all_250_features.copy(), # Pass a copy to avoid modifying the original df_with_all_features\n","    columns_to_exclude_from_features=None, # Use default exclusion (removes original price/volume/weekday)\n","    apply_scaling=True,                     # Apply StandardScaler (best for all 3 models)\n","    scaler_obj=None                         # Fit a new scaler\n",")\n","if not final_processed_FTSE250_df.empty:\n","    print(\"\\n--- Processed DataFrame (Head) after fixing infs ---\")\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    print(final_processed_FTSE250_df.head(15))\n","\n","    print(\"\\n--- Processed DataFrame (Info) after fixing infs ---\")\n","    final_processed_FTSE250_df.info()\n","\n","    print(\"\\n--- NaNs in Processed DataFrame (should be 0) ---\")\n","    print(final_processed_FTSE250_df.isnull().sum().sum()) # Should be 0 NaNs if successful\n","else:\n","    print(\"Preprocessing returned an empty DataFrame.\")"],"metadata":{"id":"W9dD2nPsraNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data visualization\n","\n","\n","def visualize_engineered_features(df: pd.DataFrame, num_sample_tickers: int = 3, num_top_features_corr: int = 15):\n","    \"\"\"\n","    Generates various visualizations for the engineered stock features.\n","\n","    Assumes input DataFrame has a MultiIndex (Date, Ticker) and contains\n","    numerical features (optionally scaled).\n","\n","    Args:\n","        df (pd.DataFrame): The DataFrame with engineered features, MultiIndex (Date, Ticker).\n","        num_sample_tickers (int): Number of random tickers to sample for time-series plots.\n","                                  Defaults to 3.\n","        num_top_features_corr (int): Number of top correlated features to display in the\n","                                     correlation heatmap. Defaults to 15.\n","    \"\"\"\n","    if df.empty:\n","        print(\"DataFrame is empty. No visualizations to generate.\")\n","        return\n","\n","    print(\"\\n--- Starting Data Visualization for Engineered Features ---\")\n","\n","    # Set plot style\n","    sns.set_style(\"whitegrid\")\n","    plt.rcParams['figure.figsize'] = (12, 6) # Default figure size\n","\n","    # --- 1. Overall DataFrame Information ---\n","    print(\"\\n1. DataFrame Overview:\")\n","    print(f\"Total entries: {df.shape[0]}\")\n","    print(f\"Number of features: {df.shape[1]}\")\n","    print(f\"Number of unique tickers: {df.index.get_level_values('Ticker').nunique()}\")\n","    print(f\"Date range: {df.index.get_level_values('Date').min().date()} to {df.index.get_level_values('Date').max().date()}\")\n","\n","    # --- 2. Distribution of Key Features ---\n","    print(\"\\n2. Distribution of Selected Key Features (Histograms/KDE):\")\n","    # Select a few representative features to visualize their distribution\n","    # Adjust these feature names based on your actual generated features\n","    key_features = [\n","        'log_return', 'rolling_std_20d_log_return', 'relative_volume',\n","        'deviation_from_daily_median_return', 'daily_return_rank_pct'\n","    ]\n","\n","    # Filter for features that actually exist in the DataFrame\n","    existing_key_features = [f for f in key_features if f in df.columns]\n","\n","    if existing_key_features:\n","        fig, axes = plt.subplots(len(existing_key_features), 1, figsize=(10, 4 * len(existing_key_features)))\n","        if len(existing_key_features) == 1: axes = [axes] # Ensure axes is iterable for single plot\n","\n","        for i, feature in enumerate(existing_key_features):\n","            sns.histplot(df[feature], kde=True, ax=axes[i], bins=50)\n","            axes[i].set_title(f'Distribution of {feature}')\n","            axes[i].set_xlabel(feature)\n","            axes[i].set_ylabel('Frequency')\n","        plt.tight_layout()\n","        plt.show()\n","    else:\n","        print(\"⚠️ No key features found to plot distributions. Ensure feature names are correct.\")\n","\n","    # --- 3. Time Series Plots for Sample Tickers ---\n","    print(f\"\\n3. Time Series Plots for {num_sample_tickers} Sample Tickers:\")\n","    unique_tickers = df.index.get_level_values('Ticker').unique()\n","    if len(unique_tickers) > num_sample_tickers:\n","        sample_tickers = np.random.choice(unique_tickers, num_sample_tickers, replace=False)\n","    else:\n","        sample_tickers = unique_tickers # Use all if fewer than requested samples\n","\n","    features_to_plot_ts = ['log_return', 'rolling_std_20d_log_return', 'relative_volume']\n","    existing_features_to_plot_ts = [f for f in features_to_plot_ts if f in df.columns]\n","\n","    if existing_features_to_plot_ts and sample_tickers.size > 0:\n","        for ticker in sample_tickers:\n","            ticker_df = df.loc[(slice(None), ticker), :] # Select all dates for this ticker\n","            ticker_df = ticker_df.droplevel('Ticker') # Drop Ticker level from index for cleaner plot\n","\n","            fig, axes = plt.subplots(len(existing_features_to_plot_ts), 1, figsize=(15, 3 * len(existing_features_to_plot_ts)), sharex=True)\n","            if len(existing_features_to_plot_ts) == 1: axes = [axes] # Ensure axes is iterable\n","\n","            for i, feature in enumerate(existing_features_to_plot_ts):\n","                if feature in ticker_df.columns:\n","                    axes[i].plot(ticker_df.index, ticker_df[feature], label=f'{ticker} - {feature}')\n","                    axes[i].set_title(f'{ticker} - {feature} over Time')\n","                    axes[i].set_ylabel(feature)\n","                    axes[i].legend()\n","            plt.tight_layout()\n","            plt.show()\n","    else:\n","        print(\"⚠️ Not enough data or features to plot time series for sample tickers.\")\n","\n","    # --- 4. Correlation Matrix of Features ---\n","    print(f\"\\n4. Correlation Matrix (Top {num_top_features_corr} Most Correlated Features):\")\n","    # Calculate the correlation matrix\n","    corr_matrix = df.corr()\n","\n","    # Find the top N most correlated features (by sum of absolute correlations)\n","    # Exclude self-correlation (diagonal) when summing\n","    np.fill_diagonal(corr_matrix.values, np.nan) # Temporarily set diagonal to NaN for sum\n","    sum_abs_corr = corr_matrix.abs().sum().sort_values(ascending=False)\n","\n","    # Restore diagonal for display\n","    np.fill_diagonal(corr_matrix.values, 1.0)\n","\n","    top_features = sum_abs_corr.head(num_top_features_corr).index.tolist()\n","\n","    if len(top_features) > 1:\n","        plt.figure(figsize=(num_top_features_corr * 0.7, num_top_features_corr * 0.7))\n","        sns.heatmap(corr_matrix.loc[top_features, top_features], annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n","        plt.title(f'Correlation Matrix of Top {num_top_features_corr} Features')\n","        plt.show()\n","    else:\n","        print(\"⚠️ Not enough features or no strong correlations to plot heatmap.\")\n","\n","    # --- 5. Box Plots of Features (Overall) ---\n","    print(\"\\n5. Box Plots of All Features:\")\n","    plt.figure(figsize=(15, 8))\n","    # It's better to melt the DataFrame for box plots if you want all features on one plot\n","    # Or plot each feature individually if the number of features is small\n","    df_melted = df.reset_index().melt(id_vars=['Date', 'Ticker'], var_name='Feature', value_name='Value')\n","\n","    # Plotting for numerical features only\n","    numerical_features_only = df_melted[df_melted['Feature'].isin(df.select_dtypes(include=np.number).columns)]\n","\n","    if not numerical_features_only.empty:\n","        sns.boxplot(data=numerical_features_only, x='Feature', y='Value', orient='v')\n","        plt.title('Box Plot of Engineered Features (Overall)')\n","        plt.xticks(rotation=90)\n","        plt.tight_layout()\n","        plt.show()\n","    else:\n","        print(\"⚠️ No numerical features to plot box plots.\")\n","\n","    # --- 6. Pair Plots (for selected features - can be very slow for many features) ---\n","    print(\"\\n6. Pair Plots for a subset of features (Warning: Can be very slow for many features/rows):\")\n","    # It's impractical to plot all features. Select a very small, representative subset.\n","    pair_plot_features = [\n","        'log_return', 'rolling_std_5d_log_return', 'daily_return_rank_pct'\n","    ]\n","    existing_pair_plot_features = [f for f in pair_plot_features if f in df.columns]\n","\n","    if len(existing_pair_plot_features) >= 2 and df.shape[0] < 5000: # Limit for performance\n","        print(f\"Plotting pair plots for: {', '.join(existing_pair_plot_features)}. This may take a while.\")\n","        sns.pairplot(df[existing_pair_plot_features].sample(min(500, df.shape[0]))) # Sample a subset for faster plotting\n","        plt.suptitle('Pair Plots of Selected Features', y=1.02)\n","        plt.show()\n","    elif len(existing_pair_plot_features) < 2:\n","        print(\"⚠️ Not enough selected features for pair plot.\")\n","    else:\n","        print(\"⚠️ Too many rows for pair plot (sampling for speed). Skipped pair plot due to potentially large data.\")\n","\n","\n","    print(\"\\n--- Data Visualization Complete ---\")\n","\n","# Call the visualization function on FTSE 100 and FTSE 250 stocks\n","visualize_engineered_features(final_processed_FTSE100_df)\n","visualize_engineered_features(final_processed_FTSE250_df)"],"metadata":{"id":"4jQhYp7IKmZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to split data into training, validation and testing data\n","\n","def split_time_series_data(df: pd.DataFrame, train_end_date: str, val_end_date: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n","    \"\"\"\n","    Splits a MultiIndex DataFrame (Date, Ticker) into training, validation, and test sets\n","    chronologically based on explicit end dates.\n","\n","    Args:\n","        df (pd.DataFrame): The input DataFrame with a MultiIndex (Date, Ticker),\n","                           already processed with features and sorted by Date.\n","        train_end_date (str): The end date (inclusive) for the training set (e.g., '2019-12-31').\n","        val_end_date (str): The end date (inclusive) for the validation set.\n","                            Data after this date will be the test set (e.g., '2021-12-31').\n","\n","    Returns:\n","        tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: A tuple containing (df_train, df_val, df_test).\n","    \"\"\"\n","    if not isinstance(df.index, pd.MultiIndex) or 'Date' not in df.index.names:\n","        raise ValueError(\"DataFrame must have a MultiIndex with 'Date' as one of the levels.\")\n","\n","    train_end_dt = pd.to_datetime(train_end_date)\n","    val_end_dt = pd.to_datetime(val_end_date)\n","\n","    if train_end_dt >= val_end_dt:\n","        raise ValueError(\"train_end_date must be strictly before val_end_date.\")\n","\n","    # Split the DataFrame chronologically\n","    dates_in_df = df.index.get_level_values('Date')\n","\n","    df_train = df.loc[dates_in_df <= train_end_dt]\n","    df_val = df.loc[(dates_in_df > train_end_dt) & (dates_in_df <= val_end_dt)]\n","    df_test = df.loc[dates_in_df > val_end_dt]\n","\n","    print(f\"Data split chronologically:\")\n","    print(f\"Training set: up to {df_train.index.get_level_values('Date').max()} ({df_train.shape[0]} rows)\")\n","    print(f\"Validation set: from {df_val.index.get_level_values('Date').min()} to {df_val.index.get_level_values('Date').max()} ({df_val.shape[0]} rows)\")\n","    print(f\"Test set: from {df_test.index.get_level_values('Date').min()} ({df_test.shape[0]} rows)\")\n","\n","    if df_train.empty or df_val.empty or df_test.empty:\n","        print(\"WARNING: One or more split sets are empty. Check your dates and data range.\")\n","\n","    return df_train, df_val, df_test\n"],"metadata":{"id":"oM34PP30GeGR","executionInfo":{"status":"ok","timestamp":1751932301627,"user_tz":-60,"elapsed":59,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Calling the function to split the data into training, validation and testing for FTSE 100 stocks\n","\n","df100_train_split, df100_val_split, df100_test_split = split_time_series_data(\n","        final_processed_FTSE100_df,\n","        train_end_date='2019-12-31',\n","        val_end_date='2021-12-31'\n","    )\n","\n","print(\"\\nTraining set head:\\n\", df100_train_split.head())\n","print(\"\\nValidation set head:\\n\", df100_val_split.head())\n","print(\"\\nTest set head:\\n\", df100_test_split.head())\n","\n","print(\"\\nLast date in training set:\", df100_train_split.index.get_level_values('Date').max())\n","print(\"First date in validation set:\", df100_val_split.index.get_level_values('Date').min())\n","print(\"Last date in validation set:\", df100_val_split.index.get_level_values('Date').max())\n","print(\"First date in test set:\", df100_test_split.index.get_level_values('Date').min())\n","\n","\n","# Calling the function to split the data into training, validation and testing for FTSE 250 stocks\n","df250_train_split, df250_val_split, df250_test_split = split_time_series_data(\n","        final_processed_FTSE250_df,\n","        train_end_date='2019-12-31',\n","        val_end_date='2021-12-31'\n","    )\n","\n","print(\"\\nTraining set head:\\n\", df250_train_split.head())\n","print(\"\\nValidation set head:\\n\", df250_val_split.head())\n","print(\"\\nTest set head:\\n\", df250_test_split.head())\n","\n","print(\"\\nLast date in training set:\", df250_train_split.index.get_level_values('Date').max())\n","print(\"First date in validation set:\", df250_val_split.index.get_level_values('Date').min())\n","print(\"Last date in validation set:\", df250_val_split.index.get_level_values('Date').max())\n","print(\"First date in test set:\", df250_test_split.index.get_level_values('Date').min())"],"metadata":{"id":"4n-YDY4hJBRx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to train the data using the Isolation Forest Model\n","\n","def train_and_score_isolation_forest(\n","    X_train_normal: pd.DataFrame,\n","    X_eval: pd.DataFrame,\n","    feature_cols: list,\n","    contamination: float = 0.01,\n","    n_estimators: int = 100,\n","    random_state: int = 42,\n","    max_features: float = 1.0,\n","    max_samples: str = 'auto'\n",") -> Tuple[pd.DataFrame, pd.DataFrame, IsolationForest]:\n","    \"\"\"\n","    Trains an Isolation Forest model on X_train_normal and applies it to\n","    both X_train_normal and X_eval to generate anomaly scores and labels.\n","\n","    Args:\n","        X_train_normal (pd.DataFrame): DataFrame containing the features for training\n","                                       the Isolation Forest model. Assumed to be\n","                                       already scaled and without NaNs for the specified feature_cols.\n","        X_eval (pd.DataFrame): DataFrame containing the features for evaluation.\n","                               Assumed to be already scaled and without NaNs for the specified feature_cols.\n","        feature_cols (list): List of column names to be used as features for the model.\n","        contamination (float): The proportion of outliers in the training data set.\n","                               Used to define the threshold for anomaly prediction.\n","                               Defaults to 0.01 (1%).\n","        n_estimators (int): The number of base estimators (trees) in the ensemble.\n","        random_state (int): Seed for reproducibility of the Isolation Forest model.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - pd.DataFrame: X_train_normal with 'iso_forest_anomaly_score' and 'iso_forest_is_anomaly' columns added.\n","            - pd.DataFrame: X_eval with 'iso_forest_anomaly_score' and 'iso_forest_is_anomaly' columns added.\n","            - IsolationForest: The trained Isolation Forest model.\n","        Returns empty DataFrames and None for the model if input X_train_normal is empty.\n","    \"\"\"\n","    if X_train_normal.empty:\n","        print(\"X_train_normal is empty. Cannot train Isolation Forest.\")\n","        return pd.DataFrame(), pd.DataFrame(), None\n","\n","    #print(\"\\n--- Training and Scoring Isolation Forest Model ---\")\n","\n","    # Select only the specified feature columns for X\n","    # Assumed to be already scaled and without NaNs from prior preprocessing steps.\n","    X_train_features = X_train_normal[feature_cols]\n","    X_eval_features = X_eval[feature_cols] # Ensure X_eval also uses only feature_cols\n","\n","    # Initialize and train the Isolation Forest model\n","    print(f\"Training Isolation Forest with contamination={contamination}, n_estimators={n_estimators}, and random_state={random_state}...\")\n","    iso_forest_model = IsolationForest(n_estimators=n_estimators, contamination=contamination, random_state=random_state)\n","    iso_forest_model.fit(X_train_features) # Fit on the selected features\n","   # print(\"Isolation Forest model trained successfully.\")\n","\n","    # --- Apply the model to generate scores and labels ---\n","\n","    # Score and label X_train_normal\n","   # print(\"Generating anomaly scores and labels for X_train_normal...\")\n","    X_train_normal_scored = X_train_normal.copy()\n","    X_train_normal_scored['iso_forest_anomaly_score'] = iso_forest_model.decision_function(X_train_features)\n","    # predict returns -1 for outliers and 1 for inliers\n","    X_train_normal_scored['iso_forest_anomaly_label'] = iso_forest_model.predict(X_train_features)\n","    # Map to 0 (normal) and 1 (anomaly) for easier interpretation\n","    X_train_normal_scored['iso_forest_is_anomaly'] = X_train_normal_scored['iso_forest_anomaly_label'].map({1: 0, -1: 1})\n","   # print(\"Anomaly scores and labels generated for X_train_normal.\")\n","\n","    # Score and label X_eval\n","    X_eval_scored = X_eval.copy()\n","    if not X_eval_features.empty: # Check if the features slice of X_eval is empty\n","        #print(\"Generating anomaly scores and labels for X_eval...\")\n","        X_eval_scored['iso_forest_anomaly_score'] = iso_forest_model.decision_function(X_eval_features)\n","        X_eval_scored['iso_forest_anomaly_label'] = iso_forest_model.predict(X_eval_features)\n","        X_eval_scored['iso_forest_is_anomaly'] = X_eval_scored['iso_forest_anomaly_label'].map({1: 0, -1: 1})\n","       # print(\"Anomaly scores and labels generated for X_eval.\")\n","    else:\n","        print(\"X_eval is empty, skipping anomaly scoring for evaluation set.\")\n","\n","    print(\"--- Isolation Forest Training and Scoring Complete ---\")\n","\n","    return X_train_normal_scored, X_eval_scored, iso_forest_model"],"metadata":{"id":"chuPEFGKonvf","executionInfo":{"status":"ok","timestamp":1751932323941,"user_tz":-60,"elapsed":24,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#Training and Scoring the Isolation Forest Model on FTSE 100 stocks\n","    # We train on the 'normal' period (df_train_split) and evaluate on the 'COVID' period (df_val_split)\n","\n","selected_features = final_processed_FTSE100_df.columns.tolist()\n","\n","df100_trained_score, df100_val_score, iso_forest_model_trained = train_and_score_isolation_forest(\n","        X_train_normal=df100_train_split,\n","        X_eval=df100_val_split, # Use validation set for initial evaluation\n","        feature_cols=selected_features,\n","        contamination=0.01, # Adjust based on expected anomaly percentage in training data\n","        n_estimators=100\n","    )\n","\n","    # 4. Score the Test Set using the SAME TRAINED MODEL\n","    # This is crucial for an unbiased evaluation on truly unseen data.\n","if iso_forest_model_trained is not None:\n","        print(\"\\n--- Scoring Test Set with Trained Isolation Forest Model ---\")\n","        df100_test_score = df100_test_split.copy()\n","        df100_test_score['iso_forest_anomaly_score'] = iso_forest_model_trained.decision_function(df100_test_split[selected_features])\n","        df100_test_score['iso_forest_anomaly_label'] = iso_forest_model_trained.predict(df100_test_split[selected_features])\n","        df100_test_score['iso_forest_is_anomaly'] = df100_test_score['iso_forest_anomaly_label'].map({1: 0, -1: 1})\n","        print(\"Test set scored successfully.\")\n","else:\n","        print(\"Model was not trained, skipping test set scoring.\")\n","        df100_test_score = df100_test_split.copy() # Ensure df_test_scored is defined\n"],"metadata":{"id":"a0-NtAAWVtai"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Training and Scoring the Isolation Forest Model on FTSE 250 stocks\n","    # We train on the 'normal' period (df_train_split) and evaluate on the 'COVID' period (df_val_split)\n","\n","selected_features = final_processed_FTSE250_df.columns.tolist()\n","\n","df250_trained_score, df250_val_score, iso_forest_model_trained = train_and_score_isolation_forest(\n","        X_train_normal=df250_train_split,\n","        X_eval=df250_val_split, # Use validation set for initial evaluation\n","        feature_cols=selected_features,\n","        contamination=0.01, # Adjust based on expected anomaly percentage in training data\n","        n_estimators=100\n","    )\n","\n","    # 4. Score the Test Set using the SAME TRAINED MODEL\n","    # This is crucial for an unbiased evaluation on truly unseen data.\n","if iso_forest_model_trained is not None:\n","        print(\"\\n--- Scoring Test Set with Trained Isolation Forest Model ---\")\n","        df250_test_score = df250_test_split.copy()\n","        df250_test_score['iso_forest_anomaly_score'] = iso_forest_model_trained.decision_function(df250_test_split[selected_features])\n","        df250_test_score['iso_forest_anomaly_label'] = iso_forest_model_trained.predict(df250_test_split[selected_features])\n","        df250_test_score['iso_forest_is_anomaly'] = df250_test_score['iso_forest_anomaly_label'].map({1: 0, -1: 1})\n","        print(\"Test set scored successfully.\")\n","else:\n","        print(\"Model was not trained, skipping test set scoring.\")\n","        df250_test_score = df250_test_split.copy() # Ensure df_test_scored is defined\n"],"metadata":{"id":"jhlTay3GweNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analyze and Visualize Results for FTSE 100 stocks\n","print(\"\\n--- Isolation Forest Results (Validation Set Head) ---\")\n","print(df100_val_score.head())\n","print(\"\\n--- Isolation Forest Results (Test Set Head) ---\")\n","print(df100_test_score.head())\n","\n","# Example: Count anomalies in each set\n","print(f\"\\nAnomalies in Training Set: {df100_trained_score['iso_forest_is_anomaly'].sum()} / {len(df100_trained_score)}\")\n","print(f\"Anomalies in Validation Set: {df100_val_score['iso_forest_is_anomaly'].sum()} / {len(df100_val_score)}\")\n","print(f\"Anomalies in Test Set: {df100_test_score['iso_forest_is_anomaly'].sum()} / {len(df100_test_score)}\")\n","\n","# Combine all results for overall visualization\n","df100_all_results = pd.concat([df100_trained_score, df100_val_score, df100_test_score]).sort_index()"],"metadata":{"id":"T2_wpz9ixZl_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analyze and Visualize Results for FTSE 100 stocks\n","print(\"\\n--- Isolation Forest Results (Validation Set Head) ---\")\n","print(df250_val_score.head())\n","print(\"\\n--- Isolation Forest Results (Test Set Head) ---\")\n","print(df250_test_score.head())\n","\n","# Example: Count anomalies in each set\n","print(f\"\\nAnomalies in Training Set: {df250_trained_score['iso_forest_is_anomaly'].sum()} / {len(df250_trained_score)}\")\n","print(f\"Anomalies in Validation Set: {df250_val_score['iso_forest_is_anomaly'].sum()} / {len(df250_val_score)}\")\n","print(f\"Anomalies in Test Set: {df250_test_score['iso_forest_is_anomaly'].sum()} / {len(df250_test_score)}\")\n","\n","# Combine all results for overall visualization\n","df250_all_results = pd.concat([df250_trained_score, df250_val_score, df250_test_score]).sort_index()"],"metadata":{"id":"JBmhEniJYLGf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the distribution of anomaly scores for FTSE 100 stocks\n","contamination_rate = 0.01\n","\n","plt.figure(figsize=(12, 6))\n","sns.histplot(df100_all_results['iso_forest_anomaly_score'], bins=50, kde=True, color='skyblue')\n","if iso_forest_model_trained is not None:\n","    # Calculate threshold from the training scores based on contamination\n","    # The threshold is the score below which points are considered anomalies.\n","    # np.percentile takes a percentile value between 0 and 100.\n","    # The contamination is the *proportion* of outliers, so 100 * contamination is the percentile.\n","    threshold_val = np.percentile(df100_trained_score['iso_forest_anomaly_score'], (contamination_rate * 100))\n","    plt.axvline(threshold_val, color='red', linestyle='--', label=f'Anomaly Threshold ({threshold_val:.4f})')\n","plt.title('Distribution of Isolation Forest Anomaly Scores (All Data)')\n","plt.xlabel('Anomaly Score (Lower is more anomalous)')\n","plt.ylabel('Frequency')\n","plt.legend()\n","plt.grid(True, linestyle='--', alpha=0.7)\n","plt.show()\n","\n","# Visualize anomalies over time for a key feature (e.g., log_return)\n","anomalies_for_plot = df100_all_results[df100_all_results['iso_forest_is_anomaly'] == 1].reset_index()\n","\n","plt.figure(figsize=(15, 7))\n","# Plot normal points (e.g., Adjusted Close or a key return feature)\n","if 'log_return' in df100_all_results.columns:\n","    sns.lineplot(data=df100_all_results[df100_all_results['iso_forest_is_anomaly'] == 0].reset_index(),\n","                  x='Date', y='log_return', hue='Ticker', alpha=0.5, legend=False)\n","    sns.scatterplot(data=anomalies_for_plot,\n","                    x='Date', y='log_return',\n","                    hue='Ticker', palette='deep', s=100, marker='X', legend='full',\n","                    edgecolor='black', linewidth=1) # Removed 'label' argument here\n","    plt.title('Log Returns with Isolation Forest Anomalies Highlighted (All Data)')\n","    plt.xlabel('Date')\n","    plt.ylabel('Log Return')\n","else:\n","    print(\"Cannot plot 'log_return' as it's not in the DataFrame. Please choose another feature.\")\n","    # Fallback to plotting something else if log_return is not available\n","    if selected_features:\n","        sns.lineplot(data=df100_all_results[df100_all_results['iso_forest_is_anomaly'] == 0].reset_index(),\n","                      x='Date', y=selected_features[0], hue='Ticker', alpha=0.5, legend=False)\n","        sns.scatterplot(data=anomalies_for_plot,\n","                        x='Date', y=selected_features[0],\n","                        hue='Ticker', palette='deep', s=100, marker='X', legend='full') # Removed 'label' argument here\n","        plt.title(f'{selected_features[0]} with Isolation Forest Anomalies Highlighted (All Data)')\n","        plt.xlabel('Date')\n","        plt.ylabel(selected_features[0])\n","    else:\n","        print(\"No features selected for plotting.\")\n","\n","plt.grid(True, linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"JDEdydg3WOM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the distribution of anomaly scores for FTSE 250 stocks\n","#contamination_rate = 0.01\n","\n","plt.figure(figsize=(12, 6))\n","sns.histplot(df250_all_results['iso_forest_anomaly_score'], bins=50, kde=True, color='skyblue')\n","if iso_forest_model_trained is not None:\n","    # Calculate threshold from the training scores based on contamination\n","    # The threshold is the score below which points are considered anomalies.\n","    # np.percentile takes a percentile value between 0 and 100.\n","    # The contamination is the *proportion* of outliers, so 100 * contamination is the percentile.\n","    threshold_val = np.percentile(df250_trained_score['iso_forest_anomaly_score'], (contamination_rate * 100))\n","    plt.axvline(threshold_val, color='red', linestyle='--', label=f'Anomaly Threshold ({threshold_val:.4f})')\n","plt.title('Distribution of Isolation Forest Anomaly Scores (All Data)')\n","plt.xlabel('Anomaly Score (Lower is more anomalous)')\n","plt.ylabel('Frequency')\n","plt.legend()\n","plt.grid(True, linestyle='--', alpha=0.7)\n","plt.show()\n","\n","# Visualize anomalies over time for a key feature (e.g., log_return)\n","anomalies_for_plot = df250_all_results[df250_all_results['iso_forest_is_anomaly'] == 1].reset_index()\n","\n","plt.figure(figsize=(15, 7))\n","# Plot normal points (e.g., Adjusted Close or a key return feature)\n","if 'log_return' in df250_all_results.columns:\n","    sns.lineplot(data=df250_all_results[df250_all_results['iso_forest_is_anomaly'] == 0].reset_index(),\n","                  x='Date', y='log_return', hue='Ticker', alpha=0.5, legend=False)\n","    sns.scatterplot(data=anomalies_for_plot,\n","                    x='Date', y='log_return',\n","                    hue='Ticker', palette='deep', s=100, marker='X', legend='full',\n","                    edgecolor='black', linewidth=1) # Removed 'label' argument here\n","    plt.title('Log Returns with Isolation Forest Anomalies Highlighted (All Data)')\n","    plt.xlabel('Date')\n","    plt.ylabel('Log Return')\n","else:\n","    print(\"Cannot plot 'log_return' as it's not in the DataFrame. Please choose another feature.\")\n","    # Fallback to plotting something else if log_return is not available\n","    if selected_features:\n","        sns.lineplot(data=df250_all_results[df250_all_results['iso_forest_is_anomaly'] == 0].reset_index(),\n","                      x='Date', y=selected_features[0], hue='Ticker', alpha=0.5, legend=False)\n","        sns.scatterplot(data=anomalies_for_plot,\n","                        x='Date', y=selected_features[0],\n","                        hue='Ticker', palette='deep', s=100, marker='X', legend='full') # Removed 'label' argument here\n","        plt.title(f'{selected_features[0]} with Isolation Forest Anomalies Highlighted (All Data)')\n","        plt.xlabel('Date')\n","        plt.ylabel(selected_features[0])\n","    else:\n","        print(\"No features selected for plotting.\")\n","\n","plt.grid(True, linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"_6Jnzckvynf6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine all scored dataframes (train, val, test)\n","df100_all_scored = pd.concat([df100_trained_score, df100_val_score, df100_test_score]).sort_index()\n","\n","# Prepare original_data_for_merge: filter by date range and ensure unique MultiIndex\n","# This assumes ready_ftse100_data is your original raw data DataFrame.\n","# It should contain 'close', 'open', 'high', 'low', 'adj close', 'volume'.\n","\n","# Get the full date range covered by your scored data\n","min100_date_scored = df100_all_scored.index.get_level_values('Date').min()\n","max100_date_scored = df100_all_scored.index.get_level_values('Date').max()\n","\n","original100_data_for_merge = ready_ftse100_data.loc[\n","(ready_ftse100_data.index.get_level_values('Date') >= min100_date_scored) &\n","(ready_ftse100_data.index.get_level_values('Date') <= max100_date_scored)\n","].copy()\n","\n","# Crucial: Drop duplicates from the MultiIndex if any exist, before any merge operations.\n","if not original100_data_for_merge.index.is_unique:\n","  print(\"Warning: Duplicates found in original_data_for_merge index. Dropping them.\")\n","  original100_data_for_merge = original100_data_for_merge[~original100_data_for_merge.index.duplicated(keep='first')].copy()\n","\n","\n","# Ensure df_all_scored also has a unique MultiIndex\n","if not df100_all_scored.index.is_unique:\n","  print(\"Warning: Duplicates found in df_all_scored index. Dropping them.\")\n","  df100_all_scored_unique = df100_all_scored[~df100_all_scored.index.duplicated(keep='first')].copy()\n","else:\n","  df100_all_scored_unique = df100_all_scored.copy()\n","\n","\n","# Ensure that both DataFrames have the same index for a clean merge (intersection)\n","common100_indices = original100_data_for_merge.index.intersection(df100_all_scored_unique.index)\n","\n","original100_data_aligned = original100_data_for_merge.loc[common100_indices]\n","scores100_features_aligned = df100_all_scored_unique.loc[common100_indices]\n","\n","\n","# Perform the merge using `left_index=True` and `right_index=True`\n","#    This merges directly on the MultiIndex, which is the most stable approach.\n","#    We are merging the features and anomaly data from scores_features_aligned\n","#    onto the original raw data from original_data_aligned.\n","\n","# Columns to take from scores_features_aligned (all columns except base data that might overlap)\n","# We want the anomaly score and label columns, and potentially other engineered features\n","# that are not in the raw data but are in df_features.\n","cols100_to_merge_from_scores = [col for col in scores100_features_aligned.columns\n","                          if col not in original100_data_aligned.columns or\n","                          col.startswith('iso_forest_anomaly_') or\n","                          col == 'iso_forest_is_anomaly']\n","\n","\n","full100_data_with_anomalies_context = original100_data_aligned.merge(\n","scores100_features_aligned[cols100_to_merge_from_scores],\n","left_index=True,  # Merge on the index of the left DataFrame\n","right_index=True, # Merge on the index of the right DataFrame\n","how='left'        # Keep all rows from the left DataFrame (original data)\n",")\n","\n","print(f\"\\nFull data with anomalies context shape: {full100_data_with_anomalies_context.shape}\")\n","print(f\"Is index truly unique in full_data_with_anomalies_context? {full100_data_with_anomalies_context.index.is_unique}\") # Should be True\n","\n","\n","# Filter and Inspect Anomalies from the Merged DataFrame ---\n","anomalies100_df_context = full100_data_with_anomalies_context[full100_data_with_anomalies_context['iso_forest_is_anomaly'] == 1].copy()\n","\n","print(f\"\\n--- Detected Anomalies with Original Context (Total: {anomalies100_df_context.shape[0]}) ---\")\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', 1000)\n","print(anomalies100_df_context.sort_values(by='iso_forest_anomaly_score').head(10))\n","\n","\n","# Visualize Individual Anomalies in Context ---\n","print(\"\\n--- Visualizing Individual Anomalies with Original Close Price Context ---\")\n","\n","window_days = 30 # Number of days before and after the anomaly date to plot\n","\n","if not anomalies100_df_context.empty:\n","# Get top 3 most anomalous points for visualization\n","  sample100_anomaly_indices = anomalies100_df_context.sort_values(by='iso_forest_anomaly_score').head(3).index.tolist()\n","else:\n","  sample100_anomaly_indices = []\n","  print(\"No anomalies detected to visualize.\")\n","\n","for anomaly_idx in sample100_anomaly_indices:\n","  anomaly_date = anomaly_idx[0]\n","  anomaly_ticker = anomaly_idx[1]\n","\n","  print(f\"\\nVisualizing anomaly for {anomaly_ticker} on {anomaly_date.date()}\")\n","\n","  # Select data for this specific ticker across a time window from the full_data_with_anomalies_context\n","  ticker100_data_for_plot = full100_data_with_anomalies_context.loc[(slice(None), anomaly_ticker), :].droplevel('Ticker')\n","\n","  start_window = anomaly_date - pd.Timedelta(days=window_days)\n","  end_window = anomaly_date + pd.Timedelta(days=window_days)\n","\n","  plot_data = ticker100_data_for_plot.loc[start_window:end_window]\n","\n","  if plot_data.empty:\n","      print(f\"No data available for {anomaly_ticker} around {anomaly_date.date()} for plotting window.\")\n","      continue\n","\n","  plt.figure(figsize=(15, 8))\n","\n","  ax1 = plt.subplot(3, 1, 1)\n","  ax1.plot(plot_data.index, plot_data['close'], label=f'{anomaly_ticker} Close Price', color='blue')\n","\n","  anomalies_in_window = plot_data[plot_data['iso_forest_is_anomaly'] == 1]\n","  if not anomalies_in_window.empty:\n","      ax1.scatter(anomalies_in_window.index, anomalies_in_window['close'],\n","                  color='red', s=100, zorder=5, label='Detected Anomaly')\n","\n","  ax1.set_title(f'Close Price for {anomaly_ticker} around {anomaly_date.date()}')\n","  ax1.set_ylabel('Close Price')\n","  ax1.legend()\n","  ax1.grid(True)\n","\n","  ax2 = plt.subplot(3, 1, 2, sharex=ax1)\n","  if 'log_return' in plot_data.columns:\n","      ax2.plot(plot_data.index, plot_data['log_return'], label=f'{anomaly_ticker} Log Return', color='green')\n","      if not anomalies_in_window.empty:\n","          ax2.scatter(anomalies_in_window.index, anomalies_in_window['log_return'], color='red', s=100, zorder=5)\n","      ax2.set_ylabel('Log Return')\n","      ax2.set_title(f'Log Return for {anomaly_ticker} around {anomaly_date.date()}')\n","      ax2.legend()\n","      ax2.grid(True)\n","  elif 'relative_volume' in plot_data.columns:\n","      ax2.plot(plot_data.index, plot_data['relative_volume'], label=f'{anomaly_ticker} Relative Volume', color='purple')\n","      if not anomalies_in_window.empty:\n","          ax2.scatter(anomalies_in_window.index, anomalies_in_window['relative_volume'], color='red', s=100, zorder=5)\n","      ax2.set_ylabel('Relative Volume')\n","      ax2.set_title(f'Relative Volume for {anomaly_ticker} around {anomaly_date.date()}')\n","      ax2.legend()\n","      ax2.grid(True)\n","  else:\n","      ax2.text(0.5, 0.5, \"No suitable feature for second plot.\", horizontalalignment='center', verticalalignment='center', transform=ax2.transAxes)\n","\n","\n","  ax3 = plt.subplot(3, 1, 3, sharex=ax1)\n","  if 'relative_volume' in plot_data.columns:\n","      ax3.plot(plot_data.index, plot_data['relative_volume'], label=f'{anomaly_ticker} Relative Volume', color='purple')\n","      if not anomalies_in_window.empty:\n","          ax3.scatter(anomalies_in_window.index, anomalies_in_window['relative_volume'], color='red', s=100, zorder=5)\n","      ax3.set_ylabel('Relative Volume')\n","      ax3.set_title(f'Relative Volume for {anomaly_ticker} around {anomaly_date.date()}')\n","  elif 'deviation_from_daily_median_return' in plot_data.columns:\n","        ax3.plot(plot_data.index, plot_data['deviation_from_daily_median_return'], label=f'{anomaly_ticker} Deviation from Median Daily Return', color='orange')\n","        if not anomalies_in_window.empty:\n","          ax3.scatter(anomalies_in_window.index, anomalies_in_window['deviation_from_daily_median_return'], color='red', s=100, zorder=5)\n","        ax3.set_ylabel('Deviation from Median Daily Return')\n","        ax3.set_title(f'Deviation from Median Daily Return for {anomaly_ticker} around {anomaly_date.date()}')\n","  else:\n","      ax3.text(0.5, 0.5, \"No suitable feature for third plot.\", horizontalalignment='center', verticalalignment='center', transform=ax3.transAxes)\n","\n","  ax3.set_xlabel('Date')\n","  ax3.legend()\n","  ax3.grid(True)\n","\n","  plt.tight_layout()\n","  plt.show()"],"metadata":{"id":"t9qhdFHSoqJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine all scored dataframes (train, val, test)\n","df250_all_scored = pd.concat([df250_trained_score, df250_val_score, df250_test_score]).sort_index()\n","\n","# 2. Prepare original_data_for_merge: filter by date range and ensure unique MultiIndex\n","# This assumes ready_ftse100_data is your original raw data DataFrame.\n","# It should contain 'close', 'open', 'high', 'low', 'adj close', 'volume'.\n","\n","# Get the full date range covered by your scored data\n","min250_date_scored = df250_all_scored.index.get_level_values('Date').min()\n","max250_date_scored = df250_all_scored.index.get_level_values('Date').max()\n","\n","original250_data_for_merge = ready_ftse250_data.loc[\n","(ready_ftse250_data.index.get_level_values('Date') >= min250_date_scored) &\n","(ready_ftse250_data.index.get_level_values('Date') <= max250_date_scored)\n","].copy()\n","\n","# Crucial: Drop duplicates from the MultiIndex if any exist, before any merge operations.\n","if not original250_data_for_merge.index.is_unique:\n","  print(\"Warning: Duplicates found in original_data_for_merge index. Dropping them.\")\n","  original_data_for_merge = original250_data_for_merge[~original250_data_for_merge.index.duplicated(keep='first')].copy()\n","\n","\n","# 3. Ensure df_all_scored also has a unique MultiIndex\n","if not df250_all_scored.index.is_unique:\n","  print(\"Warning: Duplicates found in df_all_scored index. Dropping them.\")\n","  df250_all_scored_unique = df250_all_scored[~df250_all_scored.index.duplicated(keep='first')].copy()\n","else:\n","  df250_all_scored_unique = df250_all_scored.copy()\n","\n","\n","# 4. Ensure that both DataFrames have the same index for a clean merge (intersection)\n","common250_indices = original250_data_for_merge.index.intersection(df250_all_scored_unique.index)\n","\n","original250_data_aligned = original250_data_for_merge.loc[common250_indices]\n","scores250_features_aligned = df250_all_scored_unique.loc[common250_indices]\n","\n","\n","# 5. Perform the merge using `left_index=True` and `right_index=True`\n","#    This merges directly on the MultiIndex, which is the most stable approach.\n","#    We are merging the features and anomaly data from scores_features_aligned\n","#    onto the original raw data from original_data_aligned.\n","\n","# Columns to take from scores_features_aligned (all columns except base data that might overlap)\n","# We want the anomaly score and label columns, and potentially other engineered features\n","# that are not in the raw data but are in df_features.\n","cols250_to_merge_from_scores = [col for col in scores250_features_aligned.columns\n","                          if col not in original250_data_aligned.columns or\n","                          col.startswith('iso_forest_anomaly_') or\n","                          col == 'iso_forest_is_anomaly']\n","\n","\n","full250_data_with_anomalies_context = original250_data_aligned.merge(\n","scores250_features_aligned[cols250_to_merge_from_scores],\n","left_index=True,  # Merge on the index of the left DataFrame\n","right_index=True, # Merge on the index of the right DataFrame\n","how='left'        # Keep all rows from the left DataFrame (original data)\n",")\n","\n","print(f\"\\nFull data with anomalies context shape: {full250_data_with_anomalies_context.shape}\")\n","print(f\"Is index truly unique in full_data_with_anomalies_context? {full250_data_with_anomalies_context.index.is_unique}\") # Should be True\n","\n","\n","# --- Step 7: Filter and Inspect Anomalies from the Merged DataFrame ---\n","anomalies250_df_context = full250_data_with_anomalies_context[full250_data_with_anomalies_context['iso_forest_is_anomaly'] == 1].copy()\n","\n","print(f\"\\n--- Detected Anomalies with Original Context (Total: {anomalies250_df_context.shape[0]}) ---\")\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', 1000)\n","print(anomalies250_df_context.sort_values(by='iso_forest_anomaly_score').head(10))\n","\n","\n","# --- Step 8: Visualize Individual Anomalies in Context ---\n","print(\"\\n--- Visualizing Individual Anomalies with Original Close Price Context ---\")\n","\n","window_days = 30 # Number of days before and after the anomaly date to plot\n","\n","if not anomalies250_df_context.empty:\n","# Get top 3 most anomalous points for visualization\n","  sample250_anomaly_indices = anomalies250_df_context.sort_values(by='iso_forest_anomaly_score').head(3).index.tolist()\n","else:\n","  sample250_anomaly_indices = []\n","  print(\"No anomalies detected to visualize.\")\n","\n","for anomaly_idx in sample250_anomaly_indices:\n","  anomaly_date = anomaly_idx[0]\n","  anomaly_ticker = anomaly_idx[1]\n","\n","  print(f\"\\nVisualizing anomaly for {anomaly_ticker} on {anomaly_date.date()}\")\n","\n","  # Select data for this specific ticker across a time window from the full_data_with_anomalies_context\n","  ticker250_data_for_plot = full250_data_with_anomalies_context.loc[(slice(None), anomaly_ticker), :].droplevel('Ticker')\n","\n","  start_window = anomaly_date - pd.Timedelta(days=window_days)\n","  end_window = anomaly_date + pd.Timedelta(days=window_days)\n","\n","  plot_data = ticker250_data_for_plot.loc[start_window:end_window]\n","\n","  if plot_data.empty:\n","      print(f\"No data available for {anomaly_ticker} around {anomaly_date.date()} for plotting window.\")\n","      continue\n","\n","  plt.figure(figsize=(15, 8))\n","\n","  ax1 = plt.subplot(3, 1, 1)\n","  ax1.plot(plot_data.index, plot_data['close'], label=f'{anomaly_ticker} Close Price', color='blue')\n","\n","  anomalies_in_window = plot_data[plot_data['iso_forest_is_anomaly'] == 1]\n","  if not anomalies_in_window.empty:\n","      ax1.scatter(anomalies_in_window.index, anomalies_in_window['close'],\n","                  color='red', s=100, zorder=5, label='Detected Anomaly')\n","\n","  ax1.set_title(f'Close Price for {anomaly_ticker} around {anomaly_date.date()}')\n","  ax1.set_ylabel('Close Price')\n","  ax1.legend()\n","  ax1.grid(True)\n","\n","  ax2 = plt.subplot(3, 1, 2, sharex=ax1)\n","  if 'log_return' in plot_data.columns:\n","      ax2.plot(plot_data.index, plot_data['log_return'], label=f'{anomaly_ticker} Log Return', color='green')\n","      if not anomalies_in_window.empty:\n","          ax2.scatter(anomalies_in_window.index, anomalies_in_window['log_return'], color='red', s=100, zorder=5)\n","      ax2.set_ylabel('Log Return')\n","      ax2.set_title(f'Log Return for {anomaly_ticker} around {anomaly_date.date()}')\n","      ax2.legend()\n","      ax2.grid(True)\n","  elif 'relative_volume' in plot_data.columns:\n","      ax2.plot(plot_data.index, plot_data['relative_volume'], label=f'{anomaly_ticker} Relative Volume', color='purple')\n","      if not anomalies_in_window.empty:\n","          ax2.scatter(anomalies_in_window.index, anomalies_in_window['relative_volume'], color='red', s=100, zorder=5)\n","      ax2.set_ylabel('Relative Volume')\n","      ax2.set_title(f'Relative Volume for {anomaly_ticker} around {anomaly_date.date()}')\n","      ax2.legend()\n","      ax2.grid(True)\n","  else:\n","      ax2.text(0.5, 0.5, \"No suitable feature for second plot.\", horizontalalignment='center', verticalalignment='center', transform=ax2.transAxes)\n","\n","\n","  ax3 = plt.subplot(3, 1, 3, sharex=ax1)\n","  if 'relative_volume' in plot_data.columns:\n","      ax3.plot(plot_data.index, plot_data['relative_volume'], label=f'{anomaly_ticker} Relative Volume', color='purple')\n","      if not anomalies_in_window.empty:\n","          ax3.scatter(anomalies_in_window.index, anomalies_in_window['relative_volume'], color='red', s=100, zorder=5)\n","      ax3.set_ylabel('Relative Volume')\n","      ax3.set_title(f'Relative Volume for {anomaly_ticker} around {anomaly_date.date()}')\n","  elif 'deviation_from_daily_median_return' in plot_data.columns:\n","        ax3.plot(plot_data.index, plot_data['deviation_from_daily_median_return'], label=f'{anomaly_ticker} Deviation from Median Daily Return', color='orange')\n","        if not anomalies_in_window.empty:\n","          ax3.scatter(anomalies_in_window.index, anomalies_in_window['deviation_from_daily_median_return'], color='red', s=100, zorder=5)\n","        ax3.set_ylabel('Deviation from Median Daily Return')\n","        ax3.set_title(f'Deviation from Median Daily Return for {anomaly_ticker} around {anomaly_date.date()}')\n","  else:\n","      ax3.text(0.5, 0.5, \"No suitable feature for third plot.\", horizontalalignment='center', verticalalignment='center', transform=ax3.transAxes)\n","\n","  ax3.set_xlabel('Date')\n","  ax3.legend()\n","  ax3.grid(True)\n","\n","  plt.tight_layout()\n","  plt.show()"],"metadata":{"id":"9bVbET3b0dMW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Function to analyze anomalies\n","\n","def analyze_and_visualize_anomalies(\n","    anomalies_df_multiindex: pd.DataFrame, # Now explicitly named as MultiIndex input\n","    full_eval_data_with_anomalies_multiindex: pd.DataFrame, # Now explicitly named as MultiIndex input\n","    num_top_anomalies_to_plot: int = 5,\n","    window_days_for_plot: int = 40 # Days before and after anomaly for time-series plot\n","):\n","    \"\"\"\n","    Performs detailed analysis and visualization of detected anomalies.\n","\n","    Args:\n","        anomalies_df_multiindex (pd.DataFrame): DataFrame containing only the data points\n","                                               identified as anomalies (from Isolation Forest).\n","                                               Must have MultiIndex (Date, Ticker).\n","        full_eval_data_with_anomalies_multiindex (pd.DataFrame): The complete evaluation DataFrame\n","                                                                 with all features, original data,\n","                                                                 and anomaly scores/labels (both normal and anomalous points).\n","                                                                 Must have MultiIndex (Date, Ticker).\n","        num_top_anomalies_to_plot (int): Number of top (most anomalous) events to plot in detail.\n","        window_days_for_plot (int): Number of calendar days before and after an anomaly\n","                                    to show in time-series plots for context.\n","    \"\"\"\n","    if anomalies_df_multiindex.empty:\n","        print(\"No anomalies detected. Skipping analysis and visualization.\")\n","        return\n","\n","    print(\"\\n--- Starting Anomaly Analysis and Visualization ---\")\n","\n","    # Set display options for full column view in print statements\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    sns.set_style(\"whitegrid\")\n","\n","    # --- CRITICAL FIX: Flatten DataFrames to have Date/Ticker as regular columns ---\n","    # This makes subsequent operations (feature selection, plotting) much more consistent.\n","    full_eval_data_flat = full_eval_data_with_anomalies_multiindex.reset_index().copy()\n","    anomalies_df_flat = anomalies_df_multiindex.reset_index().copy()\n","\n","    # --- 1. Overall Anomaly Summary ---\n","    total_anomalies = anomalies_df_flat.shape[0] # Use the flattened version here\n","    print(f\"\\n1. Overall Anomaly Summary:\")\n","    print(f\"Total anomalies detected: {total_anomalies}\")\n","\n","    print(\"\\nTop 10 Tickers with Most Anomalies:\")\n","    print(anomalies_df_flat['Ticker'].value_counts().head(10)) # Use Ticker column from flat DF\n","\n","    print(\"\\nMost Anomalous Events (Top 10 by Score - Lower is More Anomalous):\")\n","    # Sort on the flat DF, but we'll print using the MultiIndexed df for consistency with previous user output.\n","    # So, use anomalies_df_multiindex here.\n","    print(anomalies_df_multiindex.sort_values(by='iso_forest_anomaly_score').head(10))\n","\n","    # --- 2. Distribution of Anomaly Scores (for Anomalies Only) ---\n","    plt.figure(figsize=(10, 6))\n","    sns.histplot(anomalies_df_flat['iso_forest_anomaly_score'], kde=True, bins=30, color='red')\n","    plt.title('Distribution of Anomaly Scores for Detected Anomalies')\n","    plt.xlabel('Anomaly Score (Lower = More Anomalous)')\n","    plt.ylabel('Count')\n","    plt.show()\n","\n","    # --- 3. Feature Comparison: Anomalies vs. Normal Data (Box Plots) ---\n","    print(\"\\n3. Feature Comparison: Anomalies vs. Normal Data (Box Plots):\")\n","\n","    # Corrected way to get feature columns when DataFrame has a flat index and Date/Ticker are columns\n","    # Get all numerical columns that are NOT Date, Ticker, or anomaly scores/labels, or original base columns.\n","    feature_cols = [col for col in full_eval_data_flat.columns\n","                    if pd.api.types.is_numeric_dtype(full_eval_data_flat[col]) # Ensure numerical\n","                    and col not in ['Date', 'Ticker', 'iso_forest_anomaly_score', 'iso_forest_anomaly_label', 'iso_forest_is_anomaly']\n","                    and col not in ['open', 'high', 'low', 'close', 'adj close', 'volume']] # Exclude raw original data if desired\n","\n","    # Select a diverse sample of actual features for comparison\n","    comparison_features = ['log_return', 'rolling_std_20d_log_return', 'relative_volume', 'deviation_from_daily_median_return']\n","    existing_comparison_features = [f for f in comparison_features if f in feature_cols]\n","\n","    if not existing_comparison_features:\n","        # Fallback: if specific features aren't found, pick first few numerical features\n","        existing_comparison_features = feature_cols[:min(5, len(feature_cols))]\n","        print(f\"⚠️ Using a fallback list of {len(existing_comparison_features)} features for comparison plots.\")\n","\n","    if not existing_comparison_features:\n","        print(\"⚠️ No suitable features found for comparison plots after all filters. Skipping.\")\n","    else:\n","        # Use the flattened DataFrame directly for plotting box plots\n","        fig, axes = plt.subplots(len(existing_comparison_features), 1, figsize=(12, 5 * len(existing_comparison_features)))\n","        if len(existing_comparison_features) == 1: axes = [axes] # Ensure axes is iterable for single plot\n","\n","        for i, feature in enumerate(existing_comparison_features):\n","            sns.boxplot(x='iso_forest_is_anomaly', y=feature, data=full_eval_data_flat, ax=axes[i])\n","            axes[i].set_title(f'Distribution of {feature} for Normal (0) vs. Anomaly (1)')\n","            axes[i].set_xlabel('Is Anomaly (0=Normal, 1=Anomaly)')\n","            axes[i].set_ylabel(feature)\n","        plt.tight_layout()\n","        plt.show()\n","\n","    # --- 4. Anomaly Frequency Over Time ---\n","    print(\"\\n4. Anomaly Frequency Over Time:\")\n","    # Use the flat DataFrame for value_counts on the 'Date' column\n","    anomalies_per_day = anomalies_df_flat['Date'].value_counts().sort_index()\n","    if not anomalies_per_day.empty:\n","        plt.figure(figsize=(15, 6))\n","        anomalies_per_day.plot(kind='line', marker='o', linestyle='-', color='red')\n","        plt.title('Number of Anomalies Detected Per Day')\n","        plt.xlabel('Date')\n","        plt.ylabel('Number of Anomalies')\n","        plt.grid(True)\n","        plt.tight_layout()\n","        plt.show()\n","    else:\n","        print(\"No daily anomaly counts to plot.\")\n","\n","    # --- 5. Detailed Time-Series Plots for Top N Anomalies ---\n","    print(f\"\\n5. Detailed Time-Series Plots for Top {num_top_anomalies_to_plot} Anomalies:\")\n","\n","    # Get the indices (Date, Ticker) of the top N most anomalous points from the original MultiIndex anomalies_df\n","    top_anomaly_indices = anomalies_df_multiindex.sort_values(by='iso_forest_anomaly_score').head(num_top_anomalies_to_plot).index.tolist()\n","\n","    if not top_anomaly_indices:\n","        print(\"No top anomalies to plot in detail.\")\n","        return\n","\n","    # Use the flattened full_eval_data_flat for plotting, as it has Date/Ticker as columns\n","    plot_source_df_flat = full_eval_data_flat.copy()\n","\n","    # Ensure essential plot columns exist\n","    required_plot_cols = ['close', 'log_return', 'relative_volume']\n","    if not all(col in plot_source_df_flat.columns for col in required_plot_cols):\n","        print(f\"Error: Missing one or more required plotting columns ({', '.join(required_plot_cols)}) in the main DataFrame. Skipping detailed plots.\")\n","        return\n","\n","    for anomaly_idx in tqdm(top_anomaly_indices, desc=\"Plotting Top Anomalies\"):\n","        anomaly_date = anomaly_idx[0]\n","        anomaly_ticker = anomaly_idx[1]\n","\n","        print(f\"\\nVisualizing anomaly for {anomaly_ticker} on {anomaly_date.date()}\")\n","\n","        # Filter the flat DataFrame for plotting by Date and Ticker columns\n","        ticker_data_for_plot_flat = plot_source_df_flat[\n","            (plot_source_df_flat['Ticker'] == anomaly_ticker) &\n","            (plot_source_df_flat['Date'] >= (anomaly_date - pd.Timedelta(days=window_days_for_plot))) &\n","            (plot_source_df_flat['Date'] <= (anomaly_date + pd.Timedelta(days=window_days_for_plot)))\n","        ].copy()\n","\n","        # Temporarily set index to Date for plotting, as plot functions usually prefer this\n","        plot_data = ticker_data_for_plot_flat.set_index('Date').sort_index()\n","\n","        if plot_data.empty:\n","            print(f\"No data available for {anomaly_ticker} around {anomaly_date.date()} for plotting window. Skipping.\")\n","            continue\n","\n","        plt.figure(figsize=(15, 10))\n","\n","        # Subplot 1: Close Price with Anomaly Highlight\n","        ax1 = plt.subplot(3, 1, 1)\n","        ax1.plot(plot_data.index, plot_data['close'], label=f'{anomaly_ticker} Close Price', color='blue')\n","\n","        anomalies_in_window = plot_data[plot_data['iso_forest_is_anomaly'] == 1]\n","        if not anomalies_in_window.empty:\n","            ax1.scatter(anomalies_in_window.index, anomalies_in_window['close'],\n","                        color='red', s=100, zorder=5, label='Detected Anomaly')\n","\n","        ax1.set_title(f'Close Price for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax1.set_ylabel('Close Price')\n","        ax1.legend()\n","        ax1.grid(True)\n","\n","        # Subplot 2: Log Return with Anomaly Highlight\n","        ax2 = plt.subplot(3, 1, 2, sharex=ax1) # Share x-axis to align dates\n","        ax2.plot(plot_data.index, plot_data['log_return'], label=f'{anomaly_ticker} Log Return', color='green')\n","        if not anomalies_in_window.empty:\n","            ax2.scatter(anomalies_in_window.index, anomalies_in_window['log_return'], color='red', s=100, zorder=5)\n","        ax2.set_ylabel('Log Return')\n","        ax2.set_title(f'Log Return for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax2.legend()\n","        ax2.grid(True)\n","\n","        # Subplot 3: Relative Volume with Anomaly Highlight\n","        ax3 = plt.subplot(3, 1, 3, sharex=ax1) # Share x-axis to align dates\n","        ax3.plot(plot_data.index, plot_data['relative_volume'], label=f'{anomaly_ticker} Relative Volume', color='purple')\n","        if not anomalies_in_window.empty:\n","            ax3.scatter(anomalies_in_window.index, anomalies_in_window['relative_volume'], color='red', s=100, zorder=5)\n","        ax3.set_ylabel('Relative Volume')\n","        ax3.set_title(f'Relative Volume for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax3.set_xlabel('Date')\n","        ax3.legend()\n","        ax3.grid(True)\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    print(\"\\n--- Anomaly Analysis and Visualization Complete ---\")\n","\n"],"metadata":{"id":"Yryc-MJERQff","executionInfo":{"status":"ok","timestamp":1751926773008,"user_tz":-60,"elapsed":22,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["\n","    # Filter anomalies for visualization for FTSE 100 stocks\n","    anomalies100_df_context = full100_data_with_anomalies_context[full100_data_with_anomalies_context['iso_forest_is_anomaly'] == 1].copy()\n","\n","    # --- Call the analyze_and_visualize_anomalies function ---\n","    analyze_and_visualize_anomalies(\n","        anomalies_df_multiindex=anomalies100_df_context,\n","        full_eval_data_with_anomalies_multiindex=full100_data_with_anomalies_context,\n","        num_top_anomalies_to_plot=5,  # You can adjust this number\n","        window_days_for_plot=40       # You can adjust this number\n","    )"],"metadata":{"id":"-x6DQb0cuijq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","    # Filter anomalies for visualization for FTSE 250 stocks\n","    anomalies250_df_context = full250_data_with_anomalies_context[full250_data_with_anomalies_context['iso_forest_is_anomaly'] == 1].copy()\n","\n","    # --- Call the analyze_and_visualize_anomalies function ---\n","    analyze_and_visualize_anomalies(\n","        anomalies_df_multiindex=anomalies250_df_context,\n","        full_eval_data_with_anomalies_multiindex=full250_data_with_anomalies_context,\n","        num_top_anomalies_to_plot=5,  # You can adjust this number\n","        window_days_for_plot=40       # You can adjust this number\n","    )"],"metadata":{"id":"b1iC9lXb7Xwb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def custom_unsupervised_scorer(model: IsolationForest, X_train_scored: pd.DataFrame, X_val_scored: pd.DataFrame, contamination_rate: float) -> float:\n","    \"\"\"\n","    Custom scoring function for unsupervised anomaly detection.\n","    Aims to reward models that flag more anomalies in the validation set\n","    while maintaining the expected contamination rate in the training set.\n","\n","    Args:\n","        model (IsolationForest): The trained Isolation Forest model.\n","        X_train_scored (pd.DataFrame): Training data with 'iso_forest_anomaly_score' and 'iso_forest_is_anomaly'.\n","        X_val_scored (pd.DataFrame): Validation data with 'iso_forest_anomaly_score' and 'iso_forest_is_anomaly'.\n","        contamination_rate (float): The expected contamination rate.\n","\n","    Returns:\n","        float: A score indicating model performance. Higher is better.\n","    \"\"\"\n","    train_anomalies_count = X_train_scored['iso_forest_is_anomaly'].sum()\n","    val_anomalies_count = X_val_scored['iso_forest_is_anomaly'].sum()\n","\n","    expected_train_anomalies = len(X_train_scored) * contamination_rate\n","\n","    # Ensure expected_train_anomalies is not zero if contamination_rate is very small and len(X_train_scored) is small\n","    if expected_train_anomalies < 1:\n","        expected_train_anomalies = 1\n","\n","    score = (val_anomalies_count / len(X_val_scored)) - (abs(train_anomalies_count - expected_train_anomalies) / expected_train_anomalies)\n","\n","   # print(f\"  Train Anomalies: {train_anomalies_count} (Expected: {expected_train_anomalies:.2f})\")\n","   # print(f\"  Validation Anomalies: {val_anomalies_count}\")\n","   # print(f\"  Custom Score: {score:.4f}\")\n","\n","    return score\n","\n","\n","def optimize_isolation_forest(\n","    df_train: pd.DataFrame,\n","    df_val: pd.DataFrame,\n","    df_test: pd.DataFrame,\n","    feature_cols: list,\n","    param_grid: Dict[str, Any],\n","    random_state: int = 42\n",") -> Tuple[Dict[str, Any], IsolationForest, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n","    \"\"\"\n","    Optimizes Isolation Forest hyperparameters using a custom scoring function\n","    on the validation set.\n","\n","    Args:\n","        df_train (pd.DataFrame): Training DataFrame with features (scaled, no NaNs).\n","        df_val (pd.DataFrame): Validation DataFrame with features (scaled, no NaNs).\n","        df_test (pd.DataFrame): Test DataFrame with features (scaled, no NaNs).\n","        feature_cols (list): List of column names to be used as features.\n","        param_grid (Dict[str, Any]): Dictionary with parameters names (str) as keys\n","                                     and lists of parameter settings to try as values.\n","                                     Example: {'contamination': [0.005, 0.01, 0.02], 'n_estimators': [100, 200]}\n","        random_state (int): Seed for reproducibility.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - Dict[str, Any]: Best hyperparameters found.\n","            - IsolationForest: The best trained Isolation Forest model.\n","            - pd.DataFrame: Training data scored with the best model.\n","            - pd.DataFrame: Validation data scored with the best model.\n","            - pd.DataFrame: Test data scored with the best model.\n","    \"\"\"\n","    best_score = -np.inf\n","    best_params = {}\n","    best_model = None\n","    best_df_train_scored = None\n","    best_df_val_scored = None\n","    best_df_test_scored = None\n","\n","    grid_combinations = list(ParameterGrid(param_grid))\n","    print(f\"\\n--- Starting Isolation Forest Optimization ({len(grid_combinations)} combinations) ---\")\n","\n","    for i, params in enumerate(grid_combinations):\n","        #print(f\"\\nEvaluating combination {i+1}/{len(grid_combinations)}: {params}\")\n","\n","        current_contamination = params.get('contamination', 0.01)\n","\n","        df_train_scored, df_val_scored, current_model = train_and_score_isolation_forest(\n","            X_train_normal=df_train,\n","            X_eval=df_val,\n","            feature_cols=feature_cols,\n","            contamination=current_contamination,\n","            n_estimators=params.get('n_estimators', 100),\n","            max_features=params.get('max_features', 1.0), # Pass max_features\n","            max_samples=params.get('max_samples', 'auto'), # Pass max_samples\n","            random_state=random_state\n","        )\n","\n","        if current_model is not None:\n","            score = custom_unsupervised_scorer(current_model, df_train_scored, df_val_scored, current_contamination)\n","\n","            if score > best_score:\n","                best_score = score\n","                best_params = params\n","                best_model = current_model\n","                best_df_train_scored = df_train_scored\n","                best_df_val_scored = df_val_scored\n","\n","                df_test_scored_current = df_test.copy()\n","                df_test_scored_current['iso_forest_anomaly_score'] = best_model.decision_function(df_test[feature_cols])\n","                df_test_scored_current['iso_forest_is_anomaly'] = best_model.predict(df_test[feature_cols]) == -1\n","                best_df_test_scored = df_test_scored_current\n","\n","        else:\n","            print(f\"Skipping combination {params} due to empty training data.\")\n","\n","    print(\"\\n--- Isolation Forest Optimization Complete ---\")\n","    print(f\"Best Score: {best_score:.4f}\")\n","    print(f\"Best Parameters: {best_params}\")\n","\n","    return best_params, best_model, best_df_train_scored, best_df_val_scored, best_df_test_scored\n"],"metadata":{"id":"KhvKejGuuFgL","executionInfo":{"status":"ok","timestamp":1751926813028,"user_tz":-60,"elapsed":19,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["\n","\n","    # --- STEP 4: Define the parameter grid for optimization ---\n","\n","    param_grid = {\n","        'contamination': [0.07, 0.10, 0.12, 0.15], # Common contamination rates to try\n","        'n_estimators': [200, 300, 400],          # Number of trees\n","        'max_features': [0.3, 0.4, 0.5, 0.6],               # Fraction of features to consider for each tree\n","        'max_samples': ['auto', 0.5],             # Number of samples to draw for each tree\n","    }\n","\n","\n","    # STEP 5: Call optimize_isolation_forest\n","    # Call your pre-defined optimize_isolation_forest function\n","    best100_params, best100_model, best100_df_train_scored, best100_df_val_scored, best100_df_test_scored = optimize_isolation_forest(\n","        df_train=df100_train_split,\n","        df_val=df100_val_split,\n","        df_test=df100_test_split,\n","        feature_cols=selected_features,\n","        param_grid=param_grid,\n","        random_state=42\n","    )\n","\n","    print(\"\\n--- Optimization Results ---\")\n","    print(f\"Best Parameters Found: {best100_params}\")\n","    print(f\"Best Model: {best100_model}\")\n"],"metadata":{"id":"Hrf6H1Llu7Kq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    # --- STEP 4: Define the parameter grid for optimization for 250 Stocks\n","\n","    param_grid = {\n","        'contamination': [0.12, 0.15, 0.17, 0.20], # Common contamination rates to try\n","        'n_estimators': [200, 300, 400],          # Number of trees\n","        'max_features': [0.2, 0.3, 0.4, 0.5],               # Fraction of features to consider for each tree\n","        'max_samples': ['auto', 0.5],             # Number of samples to draw for each tree\n","    }\n","\n","\n","    # STEP 5: Call optimize_isolation_forest\n","    # Call your pre-defined optimize_isolation_forest function\n","    best250_params, best250_model, best250_df_train_scored, best250_df_val_scored, best250_df_test_scored = optimize_isolation_forest(\n","        df_train=df250_train_split,\n","        df_val=df250_val_split,\n","        df_test=df250_test_split,\n","        feature_cols=selected_features,\n","        param_grid=param_grid,\n","        random_state=42\n","    )\n","\n","    print(\"\\n--- Optimization Results ---\")\n","    print(f\"Best Parameters Found: {best250_params}\")\n","    print(f\"Best Model: {best250_model}\")"],"metadata":{"id":"pVJocKqx4RtO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Isolation Forest Optimization Complete 100 ---\n","#Best Score: 0.3903\n","#Best Parameters: {'contamination': 0.15, 'max_features': 0.3, 'max_samples': 'auto', 'n_estimators': 400}\n","\n","#Isolation Forest Optimization Complete ---\n","#Best Score: 0.3883\n","#Best Parameters: {'contamination': 0.2, 'max_features': 0.2, 'max_samples': 'auto', 'n_estimators': 300}\n"],"metadata":{"id":"mgTdWXrG4Ed0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def summarize_anomalies_in_table(anomalies_df_context: pd.DataFrame, num_top_features: int = 3) -> pd.DataFrame:\n","    \"\"\"\n","    Generates a summary table of detected anomalies, including stock, date,\n","    anomaly score, original close, and separate columns for the top most extreme feature values.\n","\n","    Args:\n","        anomalies_df_context (pd.DataFrame): A DataFrame containing only the detected anomalies.\n","                                             This DataFrame should include original stock data\n","                                             ('close', 'log_return', 'volume', etc.)\n","                                             as well as 'iso_forest_anomaly_score' and 'iso_forest_is_anomaly'.\n","                                             It must have a MultiIndex (Date, Ticker).\n","        num_top_features (int): The number of features to display that are most extreme\n","                                for each anomaly, based on their absolute scaled value.\n","\n","    Returns:\n","        pd.DataFrame: A DataFrame representing the summary table of anomalies.\n","                      Returns an empty DataFrame if no anomalies are detected.\n","    \"\"\"\n","    if anomalies_df_context.empty:\n","        print(\"No anomalies detected. Returning an empty summary table.\")\n","        return pd.DataFrame()\n","\n","    print(\"\\n--- Generating Anomaly Summary Table ---\")\n","\n","    # Ensure display options for Pandas are set to show full content\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    pd.set_option('display.max_rows', 100) # Adjust as needed\n","\n","    # Sort anomalies by score (lower score = more anomalous)\n","    sorted_anomalies = anomalies_df_context.sort_values(by='iso_forest_anomaly_score', ascending=True)\n","\n","    # Prepare list to hold rows for the summary table\n","    summary_rows = []\n","\n","    # Get the list of features that were used by the Isolation Forest model\n","    # Exclude original price columns, anomaly scores/labels\n","    feature_cols = [col for col in anomalies_df_context.columns\n","                    if col not in ['open', 'high', 'low', 'close', 'adj close', 'volume',\n","                                  'iso_forest_anomaly_score', 'iso_forest_anomaly_label', 'iso_forest_is_anomaly']\n","                    and pd.api.types.is_numeric_dtype(anomalies_df_context[col])]\n","\n","    # Prepare column names for the extreme features\n","    extreme_feature_column_names = [f'Extreme_Feature_{i+1}' for i in range(num_top_features)]\n","\n","    for idx, row in sorted_anomalies.iterrows():\n","        anomaly_date = idx[0].date() # Get just the date part\n","        anomaly_ticker = idx[1]\n","        anomaly_score = row['iso_forest_anomaly_score']\n","\n","        # Find the most extreme features for this anomaly\n","        feature_values = row[feature_cols].copy()\n","        feature_values.dropna(inplace=True)\n","\n","        # Calculate absolute values for ranking extremity\n","        extreme_features_series = feature_values.abs().sort_values(ascending=False).head(num_top_features)\n","\n","        # Initialize a dictionary for the current row's summary\n","        current_row_summary = {\n","            'Date': anomaly_date,\n","            'Ticker': anomaly_ticker,\n","            'Anomaly Score': f\"{anomaly_score:.4f}\",\n","            'Original Close': f\"{row['close']:.2f}\",\n","        }\n","\n","        # Dynamically add columns for each extreme feature\n","        for i, (feat_name, abs_feat_val) in enumerate(extreme_features_series.items()):\n","            original_feat_val = feature_values.loc[feat_name] # Get the original value (not absolute)\n","            current_row_summary[extreme_feature_column_names[i]] = f\"{feat_name}: {original_feat_val:.4f}\"\n","\n","        # Fill any remaining extreme feature columns with N/A if less than num_top_features were found\n","        for i in range(len(extreme_features_series), num_top_features):\n","            current_row_summary[extreme_feature_column_names[i]] = \"N/A\"\n","\n","        summary_rows.append(current_row_summary)\n","\n","    summary_table_df = pd.DataFrame(summary_rows)\n","    return summary_table_df"],"metadata":{"id":"cz1BGbMmu6tY","executionInfo":{"status":"ok","timestamp":1751928661036,"user_tz":-60,"elapsed":53,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# --- Call the summarize_anomalies_in_table function for 100 ---\n","anomaly_summary_table = summarize_anomalies_in_table(anomalies100_df_context, num_top_features=3)\n","\n","print(\"\\n--- Anomaly Summary Table ---\")\n","print(anomaly_summary_table)"],"metadata":{"id":"1qCw2BUoI2hY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Call the summarize_anomalies_in_table function for 250---\n","anomaly_summary_table = summarize_anomalies_in_table(anomalies250_df_context, num_top_features=3)\n","\n","print(\"\\n--- Anomaly Summary Table ---\")\n","print(anomaly_summary_table)"],"metadata":{"id":"0CbGllIjcXIy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Define the plot_k_distance_graph function ---\n","def plot_k_distance_graph(df_features: pd.DataFrame, min_samples_val: int):\n","    \"\"\"\n","    Plots the k-distance graph to help determine the optimal 'eps' parameter for DBSCAN.\n","\n","    Args:\n","        df_features (pd.DataFrame): The DataFrame of features (e.g., X_eval from your split),\n","                                    assumed to be scaled.\n","        min_samples_val (int): The chosen 'min_samples' value for DBSCAN, which determines 'k'\n","                                for the k-distance calculation (k = min_samples_val - 1).\n","    \"\"\"\n","    if df_features.empty:\n","        print(\"Input DataFrame is empty. Cannot plot k-distance graph.\")\n","        return\n","\n","    print(f\"\\n--- Plotting K-Distance Graph for eps Tuning (k = {min_samples_val-1}) ---\")\n","\n","    # Use NearestNeighbors to find the distance to the (min_samples_val - 1)-th nearest neighbor\n","    # (k in k-distance is min_samples - 1 because min_samples includes the point itself)\n","    neighbors = NearestNeighbors(n_neighbors=min_samples_val)\n","    neighbors_fit = neighbors.fit(df_features)\n","\n","    # Distances to the n_neighbors-th nearest point for each data point\n","    distances, indices = neighbors_fit.kneighbors(df_features)\n","\n","    # Sort distances by the distance to the (k-1)th nearest neighbor (which is min_samples_val - 1)\n","    # The last column of 'distances' array contains the distances to the n_neighbors-th point.\n","    distances = np.sort(distances[:, min_samples_val-1], axis=0)\n","\n","    plt.figure(figsize=(15, 7))\n","    plt.plot(distances)\n","    plt.title(f'K-Distance Graph for eps Tuning (k = {min_samples_val-1})')\n","    plt.xlabel('Data Points Sorted by Distance')\n","    plt.ylabel(f'Distance to {min_samples_val-1}-th Nearest Neighbor')\n","    plt.grid(True)\n","    plt.axhline(y=0.1, color='r', linestyle='--', label='Example Threshold (0.1)') # Example threshold\n","    plt.legend()\n","    plt.show()\n"],"metadata":{"id":"hevt0F6phbPJ","executionInfo":{"status":"ok","timestamp":1751928684172,"user_tz":-60,"elapsed":6,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["num_features = final_processed_FTSE100_df.shape[1]\n","min_samples_for_dbscan = 2 * num_features # Example: 2 * 5 features = 10\n","\n","print(f\"\\nChosen min_samples for DBSCAN: {min_samples_for_dbscan}\")\n","print(f\"This means k for the k-distance graph is: {min_samples_for_dbscan - 1}\")\n","\n","# --- STEP 3: Call the plot_k_distance_graph function ---\n","plot_k_distance_graph(final_processed_FTSE100_df, min_samples_val=min_samples_for_dbscan)\n","\n","print(\"\\nK-Distance graph plotted. Look for the 'elbow' point to choose your 'eps' value.\")\n","print(\"The 'elbow' is where the curve sharply changes slope, indicating a good 'eps' value.\")\n"],"metadata":{"id":"YBe_jFkuheNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_and_score_dbscan(\n","    df_features: pd.DataFrame,\n","    eps: float,\n","    min_samples: int\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Trains a DBSCAN model on the input features and assigns cluster labels,\n","    identifying anomalies as noise points.\n","\n","    Args:\n","        df_features (pd.DataFrame): The DataFrame of features, assumed to be scaled and NaN-free.\n","                                    It should have a MultiIndex (Date, Ticker).\n","        eps (float): The maximum distance between two samples for one to be considered\n","                     as in the neighborhood of the other. Determined from k-distance graph.\n","        min_samples (int): The number of samples (or total weight) in a neighborhood\n","                           for a point to be considered as a core point.\n","\n","    Returns:\n","        pd.DataFrame: A copy of the input df_features with 'dbscan_cluster_label'\n","                      and 'dbscan_is_anomaly' columns added.\n","                      'dbscan_cluster_label' will be -1 for anomalies, and 0, 1, 2... for clusters.\n","                      'dbscan_is_anomaly' will be True for anomalies, False otherwise.\n","    \"\"\"\n","    if df_features.empty:\n","        print(\"Input DataFrame is empty. Cannot train DBSCAN model.\")\n","        return pd.DataFrame()\n","\n","    print(f\"\\n--- Training DBSCAN Model (eps={eps}, min_samples={min_samples}) ---\")\n","\n","    # Initialize DBSCAN model\n","    dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n","\n","    # Fit the model and predict cluster labels\n","    # DBSCAN operates on a NumPy array, so we pass df_features.values\n","    # The labels will be -1 for noise (anomalies), and non-negative integers for clusters.\n","    dbscan_labels = dbscan_model.fit_predict(df_features.values)\n","\n","    # Create a copy of the input DataFrame to add results\n","    df_results = df_features.copy()\n","\n","    # Add the cluster labels to the DataFrame\n","    df_results['dbscan_cluster_label'] = dbscan_labels\n","\n","    # Identify anomalies: points with a cluster label of -1 are considered anomalies\n","    df_results['dbscan_is_anomaly'] = (dbscan_labels == -1)\n","\n","    num_anomalies = df_results['dbscan_is_anomaly'].sum()\n","    num_clusters = len(np.unique(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n","\n","    print(f\"DBSCAN training complete. Detected {num_anomalies} anomalies.\")\n","    print(f\"Found {num_clusters} clusters (excluding noise).\")\n","    print(\"--- DBSCAN Training and Scoring Complete ---\")\n","\n","    return df_results"],"metadata":{"id":"jwjxPpQpzX8i","executionInfo":{"status":"ok","timestamp":1751928715547,"user_tz":-60,"elapsed":5,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":[" # --- Call the train_and_score_dbscan function ---\n","\n","my_eps_value = 2.5 # As determined from your k-distance graph\n","\n","df100_dbscan_results = train_and_score_dbscan(\n","    df_features=final_processed_FTSE100_df,\n","    eps=my_eps_value,\n","    min_samples=min_samples_for_dbscan\n",")\n","\n","print(\"\\n--- DBSCAN Results (head) ---\")\n","print(df100_dbscan_results.head())\n","print(\"\\n--- DBSCAN Results (Anomalies) ---\")\n","print(df100_dbscan_results[df100_dbscan_results['dbscan_is_anomaly'] == True])\n","print(f\"\\nTotal anomalies detected by DBSCAN: {df100_dbscan_results['dbscan_is_anomaly'].sum()}\")\n"],"metadata":{"id":"IoAwgmxMzpyc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # --- Call the train_and_score_dbscan function ---\n","\n","my_eps_value = 2.5 # As determined from your k-distance graph\n","\n","df250_dbscan_results = train_and_score_dbscan(\n","    df_features=final_processed_FTSE250_df,\n","    eps=my_eps_value,\n","    min_samples=min_samples_for_dbscan\n",")\n","\n","print(\"\\n--- DBSCAN Results (head) ---\")\n","print(df250_dbscan_results.head())\n","print(\"\\n--- DBSCAN Results (Anomalies) ---\")\n","print(df250_dbscan_results[df250_dbscan_results['dbscan_is_anomaly'] == True])\n","print(f\"\\nTotal anomalies detected by DBSCAN: {df250_dbscan_results['dbscan_is_anomaly'].sum()}\")"],"metadata":{"id":"-rQRS7mM7twm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analyze_and_visualize_dbscan_anomalies(\n","    X_eval_dbscan_scored: pd.DataFrame,\n","    my_clean_base_df: pd.DataFrame, # Original clean data with raw prices/volumes\n","    num_top_anomalies_to_plot: int = 5,\n","    window_days_for_plot: int = 40 # Days before and after anomaly for time-series plot\n","):\n","    \"\"\"\n","    Performs detailed analysis and visualization of anomalies detected by DBSCAN.\n","\n","    Args:\n","        X_eval_dbscan_scored (pd.DataFrame): DataFrame with features and DBSCAN labels\n","                                            (assumed to have MultiIndex 'Date', 'Ticker').\n","        my_clean_base_df (pd.DataFrame): The original, cleaned DataFrame with raw prices/volumes\n","                                         (MultiIndex 'Date', 'Ticker'). Used for context.\n","        num_top_anomalies_to_plot (int): Number of top anomalous events to plot in detail.\n","        window_days_for_plot (int): Number of calendar days before and after anomaly for time-series plot.\n","    \"\"\"\n","    if X_eval_dbscan_scored.empty:\n","        print(\"X_eval_dbscan_scored DataFrame is empty. No analysis to perform.\")\n","        return\n","\n","    print(\"\\n--- Starting DBSCAN Anomaly Analysis and Visualization ---\")\n","\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    sns.set_style(\"whitegrid\")\n","\n","    # --- 1. Prepare Merged Data for Analysis ---\n","    # This part merges DBSCAN results with original price data for context\n","    eval_start_date = X_eval_dbscan_scored.index.get_level_values('Date').min()\n","    eval_end_date = X_eval_dbscan_scored.index.get_level_values('Date').max()\n","\n","    original_eval_data_base = my_clean_base_df.loc[\n","        (my_clean_base_df.index.get_level_values('Date') >= eval_start_date) &\n","        (my_clean_base_df.index.get_level_values('Date') <= eval_end_date)\n","    ].copy()\n","\n","    original_eval_data_base = original_eval_data_base[~original_eval_data_base.index.duplicated(keep='first')].copy()\n","    original_eval_data_base['unique_id'] = original_eval_data_base.index.get_level_values('Date').astype(str) + '_' + \\\n","                                          original_eval_data_base.index.get_level_values('Ticker').astype(str)\n","    original_eval_data_base.set_index('unique_id', inplace=True)\n","\n","    eval_scores_features_base = X_eval_dbscan_scored.copy()\n","    eval_scores_features_base = eval_scores_features_base[~eval_scores_features_base.index.duplicated(keep='first')].copy()\n","\n","    eval_scores_features_base['unique_id'] = eval_scores_features_base.index.get_level_values('Date').astype(str) + '_' + \\\n","                                           eval_scores_features_base.index.get_level_values('Ticker').astype(str)\n","    eval_scores_features_base.set_index('unique_id', inplace=True)\n","\n","    cols_to_merge_from_dbscan = [col for col in eval_scores_features_base.columns\n","                                 if col not in original_eval_data_base.columns or col in ['dbscan_cluster_label', 'dbscan_is_anomaly']]\n","\n","    full_eval_data_merged_for_analysis = original_eval_data_base.merge(\n","        eval_scores_features_base[cols_to_merge_from_dbscan],\n","        left_index=True,\n","        right_index=True,\n","        how='left'\n","    )\n","\n","    full_eval_data_merged_for_analysis = full_eval_data_merged_for_analysis.reset_index().copy()\n","    full_eval_data_merged_for_analysis[['Date', 'Ticker']] = full_eval_data_merged_for_analysis['unique_id'].str.split('_', expand=True)\n","    full_eval_data_merged_for_analysis['Date'] = pd.to_datetime(full_eval_data_merged_for_analysis['Date'])\n","    full_eval_data_merged_for_analysis.drop(columns=['unique_id'], inplace=True)\n","\n","    anomalies_df_flat = full_eval_data_merged_for_analysis[full_eval_data_merged_for_analysis['dbscan_is_anomaly'] == 1].copy()\n","\n","    if not anomalies_df_flat.empty:\n","        anomalies_df = anomalies_df_flat.set_index(['Date', 'Ticker']).sort_index()\n","    else:\n","        anomalies_df = pd.DataFrame()\n","\n","\n","    # --- 2. Overall Anomaly Summary ---\n","    total_anomalies = anomalies_df.shape[0]\n","    print(f\"\\n2. Overall Anomaly Summary (DBSCAN):\")\n","    print(f\"Total anomalies detected: {total_anomalies}\")\n","\n","    if total_anomalies > 0:\n","        print(\"\\nTop 10 Tickers with Most Anomalies (DBSCAN):\")\n","        print(anomalies_df.index.get_level_values('Ticker').value_counts().head(10))\n","\n","        print(\"\\nFirst 10 Detected Anomalies (DBSCAN):\")\n","        print(anomalies_df.head(10))\n","    else:\n","        print(\"No anomalies detected by DBSCAN.\")\n","        return\n","\n","\n","    # --- 3. Cluster Label Distribution ---\n","    print(\"\\n3. DBSCAN Cluster Label Distribution:\")\n","    print(full_eval_data_merged_for_analysis['dbscan_cluster_label'].value_counts().sort_index())\n","\n","    # --- 4. Feature Comparison: Anomalies vs. Normal Data (Box Plots) ---\n","    print(\"\\n4. Feature Comparison: Anomalies vs. Normal Data (Box Plots):\")\n","\n","    feature_cols = [col for col in full_eval_data_merged_for_analysis.columns\n","                    if pd.api.types.is_numeric_dtype(full_eval_data_merged_for_analysis[col])\n","                    and col not in ['Date', 'Ticker', 'dbscan_cluster_label', 'dbscan_is_anomaly']\n","                    and col not in ['open', 'high', 'low', 'close', 'adj close', 'volume']]\n","\n","    comparison_features = ['log_return', 'rolling_std_20d_log_return', 'relative_volume', 'deviation_from_daily_median_return']\n","    existing_comparison_features = [f for f in comparison_features if f in feature_cols]\n","\n","    if not existing_comparison_features:\n","        existing_comparison_features = feature_cols[:min(5, len(feature_cols))]\n","        print(f\"⚠️ Using a fallback list of {len(existing_comparison_features)} features for comparison plots.\")\n","\n","    if not existing_comparison_features:\n","        print(\"⚠️ No suitable features found for comparison plots after all filters. Skipping.\")\n","    else:\n","        fig, axes = plt.subplots(len(existing_comparison_features), 1, figsize=(12, 5 * len(existing_comparison_features)))\n","        if len(existing_comparison_features) == 1: axes = [axes]\n","\n","        for i, feature in enumerate(existing_comparison_features):\n","            sns.boxplot(x='dbscan_is_anomaly', y=feature, data=full_eval_data_merged_for_analysis, ax=axes[i])\n","            axes[i].set_title(f'Distribution of {feature} for Normal (0) vs. Anomaly (1) (DBSCAN)')\n","            axes[i].set_xlabel('Is Anomaly (0=Normal, 1=Anomaly)')\n","            axes[i].set_ylabel(feature)\n","        plt.tight_layout()\n","        plt.show()\n","\n","    # --- 5. Anomaly Frequency Over Time ---\n","    print(\"\\n5. Anomaly Frequency Over Time (DBSCAN):\")\n","    anomalies_per_day = anomalies_df_flat['Date'].value_counts().sort_index()\n","    if not anomalies_per_day.empty:\n","        plt.figure(figsize=(15, 6))\n","        anomalies_per_day.plot(kind='line', marker='o', linestyle='-', color='blue')\n","        plt.title('Number of DBSCAN Anomalies Detected Per Day')\n","        plt.xlabel('Date')\n","        plt.ylabel('Number of Anomalies')\n","        plt.grid(True)\n","        plt.tight_layout()\n","        plt.show()\n","    else:\n","        print(\"No daily anomaly counts to plot for DBSCAN.\")\n","\n","    # --- NEW: 6. DBSCAN Cluster Visualization (using PCA) ---\n","    print(\"\\n6. DBSCAN Cluster Visualization (2D PCA Projection):\")\n","\n","    # Get the features used for DBSCAN (excluding labels, Date, Ticker, original prices)\n","    # This assumes these features were already scaled.\n","    features_for_pca = [col for col in X_eval_dbscan_scored.columns # Use the original scored DF for feature list\n","                        if pd.api.types.is_numeric_dtype(X_eval_dbscan_scored[col]) # Ensure numerical\n","                        and col not in ['dbscan_cluster_label', 'dbscan_is_anomaly']\n","                        and col not in ['open', 'high', 'low', 'close', 'adj close', 'volume']] # Exclude original prices/volumes\n","\n","    # Ensure there are enough features for PCA\n","    if len(features_for_pca) < 2:\n","        print(\"⚠️ Not enough numerical features (less than 2) for PCA visualization. Skipping cluster plot.\")\n","    else:\n","        # Create a DataFrame with just the features and ensure no NaNs\n","        X_for_pca = X_eval_dbscan_scored[features_for_pca].dropna()\n","\n","        if X_for_pca.empty:\n","            print(\"⚠️ No data for PCA after dropping NaNs. Skipping cluster plot.\")\n","        else:\n","            # OPTIONAL: Sample data for faster plotting if dataset is very large\n","            max_plot_points = 50000 # Limit to avoid extremely slow plots\n","            if X_for_pca.shape[0] > max_plot_points:\n","                print(f\"Sampling {max_plot_points} points for PCA plot due to large dataset size...\")\n","                X_for_pca_sampled = X_for_pca.sample(max_plot_points, random_state=42)\n","            else:\n","                X_for_pca_sampled = X_for_pca\n","\n","            # Perform PCA\n","            pca = PCA(n_components=2)\n","            principal_components = pca.fit_transform(X_for_pca_sampled)\n","\n","            # Create a DataFrame for plotting PCA results, retaining index for labels\n","            pca_df = pd.DataFrame(data = principal_components, columns = ['principal_component_1', 'principal_component_2'], index=X_for_pca_sampled.index)\n","\n","            # Merge back DBSCAN labels for plotting\n","            # Use X_eval_dbscan_scored (which has the MultiIndex intact)\n","            pca_df['cluster_label'] = X_eval_dbscan_scored.loc[pca_df.index, 'dbscan_cluster_label']\n","            pca_df['is_anomaly'] = X_eval_dbscan_scored.loc[pca_df.index, 'dbscan_is_anomaly']\n","\n","            plt.figure(figsize=(12, 10))\n","\n","            # Plot clusters\n","            sns.scatterplot(\n","                x='principal_component_1',\n","                y='principal_component_2',\n","                hue='cluster_label', # Color by cluster label\n","                palette='tab10',     # Use a distinct color palette\n","                data=pca_df[pca_df['is_anomaly'] == 0], # Plot normal points\n","                s=20, # Size of points\n","                alpha=0.7, # Transparency\n","                legend='full'\n","            )\n","\n","            # Plot anomalies separately (label -1)\n","            anomalies_pca = pca_df[pca_df['is_anomaly'] == 1]\n","            if not anomalies_pca.empty:\n","                sns.scatterplot(\n","                    x='principal_component_1',\n","                    y='principal_component_2',\n","                    data=anomalies_pca,\n","                    color='red', # Anomalies in red\n","                    marker='X', # Use 'X' marker for anomalies\n","                    s=100, # Larger size for anomalies\n","                    label='Anomalies (Noise)',\n","                    zorder=5 # Ensure anomalies are on top\n","                )\n","\n","            plt.title('DBSCAN Clusters and Anomalies (2D PCA Projection)')\n","            plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)')\n","            plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)')\n","            plt.legend(title='Cluster Label')\n","            plt.grid(True)\n","            plt.show()\n","\n","    # --- 7. Detailed Time-Series Plots for Sample Anomalies (Same as before) ---\n","    print(f\"\\n7. Detailed Time-Series Plots for Sample DBSCAN Anomalies:\")\n","\n","    sample_anomaly_indices = anomalies_df.sample(min(anomalies_df.shape[0], num_top_anomalies_to_plot), random_state=42).index.tolist()\n","\n","    if not sample_anomaly_indices:\n","        print(\"No sample anomalies to plot in detail.\")\n","        return\n","\n","    plot_source_df_flat = full_eval_data_merged_for_analysis.copy()\n","\n","    required_plot_cols = ['close', 'log_return', 'relative_volume']\n","    if not all(col in plot_source_df_flat.columns for col in required_plot_cols):\n","        print(f\"Error: Missing one or more required plotting columns ({', '.join(required_plot_cols)}) in the main DataFrame. Skipping detailed plots.\")\n","        return\n","\n","    for anomaly_idx in tqdm(sample_anomaly_indices, desc=\"Plotting Sample DBSCAN Anomalies\"):\n","        anomaly_date = anomaly_idx[0]\n","        anomaly_ticker = anomaly_idx[1]\n","\n","        print(f\"\\nVisualizing anomaly for {anomaly_ticker} on {anomaly_date.date()}\")\n","\n","        ticker_data_for_plot_flat = plot_source_df_flat[\n","            (plot_source_df_flat['Ticker'] == anomaly_ticker) &\n","            (plot_source_df_flat['Date'] >= (anomaly_date - pd.Timedelta(days=window_days_for_plot))) &\n","            (plot_source_df_flat['Date'] <= (anomaly_date + pd.Timedelta(days=window_days_for_plot)))\n","        ].copy()\n","\n","        plot_data = ticker_data_for_plot_flat.set_index('Date').sort_index()\n","\n","        if plot_data.empty:\n","            print(f\"No data available for {anomaly_ticker} around {anomaly_date.date()} for plotting window. Skipping.\")\n","            continue\n","\n","        plt.figure(figsize=(15, 10))\n","\n","        ax1 = plt.subplot(3, 1, 1)\n","        ax1.plot(plot_data.index, plot_data['close'], label=f'{anomaly_ticker} Close Price', color='blue')\n","\n","        anomalies_in_window = plot_data[plot_data['dbscan_is_anomaly'] == 1] # Use dbscan_is_anomaly\n","        if not anomalies_in_window.empty:\n","            ax1.scatter(anomalies_in_window.index, anomalies_in_window['close'],\n","                        color='red', s=100, zorder=5, label='Detected Anomaly')\n","\n","        ax1.set_title(f'Close Price for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax1.set_ylabel('Close Price')\n","        ax1.legend()\n","        ax1.grid(True)\n","\n","        ax2 = plt.subplot(3, 1, 2, sharex=ax1)\n","        ax2.plot(plot_data.index, plot_data['log_return'], label=f'{anomaly_ticker} Log Return', color='green')\n","        if not anomalies_in_window.empty:\n","            ax2.scatter(anomalies_in_window.index, anomalies_in_window['log_return'], color='red', s=100, zorder=5)\n","        ax2.set_ylabel('Log Return')\n","        ax2.set_title(f'Log Return for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax2.legend()\n","        ax2.grid(True)\n","\n","        ax3 = plt.subplot(3, 1, 3, sharex=ax1)\n","        ax3.plot(plot_data.index, plot_data['relative_volume'], label=f'{anomaly_ticker} Relative Volume', color='purple')\n","        if not anomalies_in_window.empty:\n","            ax3.scatter(anomalies_in_window.index, anomalies_in_window['relative_volume'], color='red', s=100, zorder=5)\n","        ax3.set_ylabel('Relative Volume')\n","        ax3.set_title(f'Relative Volume for {anomaly_ticker} around {anomaly_date.date()}')\n","        ax3.set_xlabel('Date')\n","        ax3.legend()\n","        ax3.grid(True)\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    print(\"\\n--- DBSCAN Anomaly Analysis and Visualization Complete ---\")"],"metadata":{"id":"pEJChjAfuUQO","executionInfo":{"status":"ok","timestamp":1751928839918,"user_tz":-60,"elapsed":26,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["\n","# Call the DBSCAN anomaly analysis function\n","analyze_and_visualize_dbscan_anomalies(\n","    df100_dbscan_results.copy(), # Pass a copy if you don't want the original altered\n","    ready_ftse100_data.copy(),     # Pass a copy for the original data\n","    num_top_anomalies_to_plot=5\n",")\n","\n","# Call the DBSCAN anomaly analysis function\n","analyze_and_visualize_dbscan_anomalies(\n","    df250_dbscan_results.copy(), # Pass a copy if you don't want the original altered\n","    ready_ftse250_data.copy(),     # Pass a copy for the original data\n","    num_top_anomalies_to_plot=5\n",")"],"metadata":{"id":"nTwmqfhosha4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def summarize_dbscan_anomalies_in_table(df_dbscan_results: pd.DataFrame, num_top_features: int = 3) -> pd.DataFrame:\n","    \"\"\"\n","    Generates a summary table of detected DBSCAN anomalies, including stock, date,\n","    cluster label (-1 for anomalies), and the top most extreme feature values.\n","\n","    Args:\n","        df_dbscan_results (pd.DataFrame): The DataFrame returned by train_and_score_dbscan,\n","                                          containing original features, 'dbscan_cluster_label',\n","                                          and 'dbscan_is_anomaly'. Must have MultiIndex (Date, Ticker).\n","        num_top_features (int): The number of features to display that are most extreme\n","                                for each anomaly, based on their absolute scaled value.\n","\n","    Returns:\n","        pd.DataFrame: A DataFrame representing the summary table of DBSCAN anomalies.\n","                      Returns an empty DataFrame if no anomalies are detected.\n","    \"\"\"\n","    if df_dbscan_results.empty:\n","        print(\"Input DataFrame is empty. Returning an empty summary table.\")\n","        return pd.DataFrame()\n","\n","    # Filter for anomalies (where dbscan_cluster_label is -1)\n","    anomalies_df = df_dbscan_results[df_dbscan_results['dbscan_is_anomaly'] == True].copy()\n","\n","    if anomalies_df.empty:\n","        print(\"No DBSCAN anomalies detected. Returning an empty summary table.\")\n","        return pd.DataFrame()\n","\n","    print(\"\\n--- Generating DBSCAN Anomaly Summary Table ---\")\n","\n","    # Set display options for Pandas to show full content\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    pd.set_option('display.max_rows', 100) # Adjust as needed\n","\n","    # Prepare list to hold rows for the summary table\n","    summary_rows = []\n","\n","    # Get the list of features that were used by the DBSCAN model\n","    # Exclude original price columns if they are present and not part of the features used for clustering,\n","    # and exclude the DBSCAN specific columns.\n","    feature_cols = [col for col in df_dbscan_results.columns\n","                    if col not in ['open', 'high', 'low', 'close', 'adj close', 'volume', # Original raw columns\n","                                  'dbscan_cluster_label', 'dbscan_is_anomaly'] # DBSCAN specific columns\n","                    and pd.api.types.is_numeric_dtype(df_dbscan_results[col])]\n","\n","    for idx, row in anomalies_df.iterrows():\n","        anomaly_date = idx[0].date() # Get just the date part\n","        anomaly_ticker = idx[1]\n","        cluster_label = int(row['dbscan_cluster_label']) # Should be -1 for anomalies\n","\n","        # Find the most extreme features for this anomaly\n","        # Assuming features are scaled, so absolute value indicates extremity\n","        feature_values = row[feature_cols].copy()\n","        feature_values.dropna(inplace=True)\n","\n","        # Calculate absolute values for ranking extremity\n","        extreme_features_series = feature_values.abs().sort_values(ascending=False).head(num_top_features)\n","\n","        # Initialize a dictionary for the current row's summary\n","        current_row_summary = {\n","            'Date': anomaly_date,\n","            'Ticker': anomaly_ticker,\n","            'DBSCAN Cluster Label': cluster_label, # Will be -1 for anomalies\n","        }\n","        # Add original close price if available in the DataFrame for context\n","        if 'close' in row.index:\n","            current_row_summary['Adj Close'] = f\"{row['close']:.2f}\"\n","        else:\n","            current_row_summary['Adj Close'] = \"N/A\"\n","\n","        # Dynamically add columns for each extreme feature\n","        for i, (feat_name, abs_feat_val) in enumerate(extreme_features_series.items()):\n","            original_feat_val = feature_values.loc[feat_name] # Get the original value (not absolute)\n","            current_row_summary[f'Extreme_Feature_{i+1}'] = f\"{feat_name}: {original_feat_val:.4f}\"\n","\n","        # Fill any remaining extreme feature columns with N/A if less than num_top_features were found\n","        for i in range(len(extreme_features_series), num_top_features):\n","            current_row_summary[f'Extreme_Feature_{i+1}'] = \"N/A\"\n","\n","        summary_rows.append(current_row_summary)\n","    summary_table_df = pd.DataFrame(summary_rows)\n","    return summary_table_df"],"metadata":{"id":"GPDboenMCELM","executionInfo":{"status":"ok","timestamp":1751928900671,"user_tz":-60,"elapsed":39,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["dbscan100_anomaly_summary_table = summarize_dbscan_anomalies_in_table(df100_dbscan_results, num_top_features=3)\n","\n","print(\"\\n--- DBSCAN Anomaly Summary Table for FTSE 100 stocks ---\")\n","print(dbscan100_anomaly_summary_table )\n","\n","dbscan250_anomaly_summary_table = summarize_dbscan_anomalies_in_table(df250_dbscan_results, num_top_features=3)\n","\n","print(\"\\n--- DBSCAN Anomaly Summary Table for FTSE 250 stocks ---\")\n","print(dbscan250_anomaly_summary_table)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G_GbLN1HCIhF","executionInfo":{"status":"ok","timestamp":1751928918900,"user_tz":-60,"elapsed":5359,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}},"outputId":"03874f1c-f158-4224-81e7-f2ba234ff34d"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Generating DBSCAN Anomaly Summary Table ---\n","\n","--- DBSCAN Anomaly Summary Table for FTSE 100 stocks ---\n","            Date Ticker  DBSCAN Cluster Label Adj Close                             Extreme_Feature_1                            Extreme_Feature_2                          Extreme_Feature_3\n","0     2014-01-30    DGE                    -1       N/A                       relative_volume: 4.1499  deviation_from_daily_median_return: -3.2908                   daily_range_norm: 3.0741\n","1     2014-01-30   HSBA                    -1       N/A                      daily_range_norm: 3.5002                     garman_klass_vol: 1.6299            deviation_from_sma_20d: -1.1698\n","2     2014-02-11   BARC                    -1       N/A                       relative_volume: 3.9847  deviation_from_daily_median_return: -3.6043  deviation_from_daily_mean_return: -3.3660\n","3     2014-02-13     RR                    -1       N/A  deviation_from_daily_median_return: -10.0302    deviation_from_daily_mean_return: -9.6032              log_adj_close_return: -8.3199\n","4     2014-02-14     RR                    -1       N/A             rolling_std_5d_log_return: 5.1116                           return_5d: -3.3362            deviation_from_sma_20d: -3.1460\n","...          ...    ...                   ...       ...                                           ...                                          ...                                        ...\n","2215  2024-11-12    AZN                    -1       N/A                               sma_20d: 4.6730                        typical_price: 4.1127                             sma_5d: 4.0640\n","2216  2024-11-15    AZN                    -1       N/A                               sma_20d: 4.5335                               sma_5d: 4.1502                      typical_price: 4.1198\n","2217  2024-11-18    AZN                    -1       N/A                               sma_20d: 4.4830                               sma_5d: 4.1432                      typical_price: 4.0592\n","2218  2024-11-22    AZN                    -1       N/A                               sma_20d: 4.3213                        typical_price: 4.3016                             sma_5d: 4.1557\n","2219  2024-12-06   BT-A                    -1       N/A                       relative_volume: 9.6094                           return_20d: 1.6250             deviation_from_sma_20d: 1.0236\n","\n","[2220 rows x 7 columns]\n","\n","--- Generating DBSCAN Anomaly Summary Table ---\n","\n","--- DBSCAN Anomaly Summary Table for FTSE 250 stocks ---\n","            Date Ticker  DBSCAN Cluster Label Adj Close                            Extreme_Feature_1                          Extreme_Feature_2                   Extreme_Feature_3\n","0     2014-01-30    SRE                    -1       N/A                       volume_change: 13.7641                    relative_volume: 2.2020  rolling_std_5d_log_return: -1.2892\n","1     2014-02-13    LRE                    -1       N/A                      relative_volume: 2.7111                   daily_range_norm: 2.4237            garman_klass_vol: 2.3146\n","2     2014-02-20    RAT                    -1       N/A                      relative_volume: 7.3467                          return_5d: 1.1563      deviation_from_sma_20d: 1.1324\n","3     2014-02-27    EMG                    -1       N/A   deviation_from_daily_median_return: 7.0188   deviation_from_daily_mean_return: 6.7458               simple_return: 6.7135\n","4     2014-02-28    EMG                    -1       N/A                            return_5d: 5.2013                     avg_volume_20d: 4.7070      deviation_from_sma_20d: 4.3182\n","...          ...    ...                   ...       ...                                          ...                                        ...                                 ...\n","2744  2024-12-18    THG                    -1       N/A  deviation_from_daily_median_return: -6.6636  deviation_from_daily_mean_return: -6.3025       log_adj_close_return: -5.5492\n","2745  2024-12-19    THG                    -1       N/A  deviation_from_daily_median_return: -4.6165  deviation_from_daily_mean_return: -4.2653       log_adj_close_return: -4.0167\n","2746  2024-12-20    THG                    -1       N/A            rolling_std_5d_log_return: 4.1183         rolling_std_20d_log_return: 2.8020                  return_5d: -2.7909\n","2747  2024-12-23    THG                    -1       N/A                           return_5d: -3.5021          rolling_std_5d_log_return: 3.4896  rolling_std_20d_log_return: 2.6583\n","2748  2024-12-24    THG                    -1       N/A            rolling_std_5d_log_return: 3.5428                         return_5d: -3.4129  rolling_std_20d_log_return: 2.6252\n","\n","[2749 rows x 7 columns]\n"]}]},{"cell_type":"code","source":["#A function to build an LSTM Autoencoder\n","\n","def build_lstm_autoencoder(n_timesteps: int, n_features: int, lstm_units: int = 64, dropout_rate: float = 0.2) -> Sequential:\n","    \"\"\"\n","    Builds an LSTM Autoencoder model for unsupervised anomaly detection.\n","\n","    The autoencoder consists of an encoder LSTM, a RepeatVector layer,\n","    a decoder LSTM, and a TimeDistributed Dense layer to reconstruct the input sequence.\n","\n","    Args:\n","        n_timesteps (int): The length of the input sequences (e.g., 10 days).\n","        n_features (int): The number of features per timestep in the input sequences.\n","        lstm_units (int): The number of units (neurons) in the LSTM layers.\n","                          Defaults to 64.\n","        dropout_rate (float): The dropout rate to apply for regularization.\n","                              Defaults to 0.2.\n","\n","    Returns:\n","        Sequential: The compiled Keras Sequential model (LSTM Autoencoder).\n","    \"\"\"\n","    model = Sequential()\n","\n","    # Encoder\n","    # Input layer defines the shape of one input sequence (timesteps, features)\n","    model.add(Input(shape=(n_timesteps, n_features)))\n","    # First LSTM layer acts as the encoder.\n","    # return_sequences=False means it outputs a single vector (the encoded representation)\n","    # for each input sequence.\n","    model.add(LSTM(units=lstm_units, activation='relu', return_sequences=False))\n","    model.add(Dropout(dropout_rate))\n","\n","    # RepeatVector layer\n","    # This layer repeats the encoded vector 'n_timesteps' times.\n","    # This is necessary to feed a sequence back into the decoder LSTM.\n","    model.add(RepeatVector(n_timesteps))\n","\n","    # Decoder\n","    # Second LSTM layer acts as the decoder.\n","    # return_sequences=True means it outputs a sequence (one vector per timestep)\n","    # which is then passed to the TimeDistributed Dense layer.\n","    model.add(LSTM(units=lstm_units, activation='relu', return_sequences=True))\n","    model.add(Dropout(dropout_rate))\n","\n","    # Output layer\n","    # TimeDistributed applies the Dense layer independently to each timestep of the sequence.\n","    # The number of units in the Dense layer must match the number of features (n_features)\n","    # to reconstruct the original input sequence.\n","    model.add(TimeDistributed(Dense(n_features)))\n","\n","    # Compile the model\n","    # For autoencoders, Mean Squared Error (MSE) is a common loss function\n","    # as the goal is to minimize the difference between input and output.\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","    print(\"\\n--- LSTM Autoencoder Model Summary ---\")\n","    model.summary()\n","\n","    return model"],"metadata":{"id":"SXew80m-GkHR","executionInfo":{"status":"ok","timestamp":1751932397492,"user_tz":-60,"elapsed":26,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["#Calling the function to build the LSTM Auto Encoder Model for FTSE 100 stocks\n","\n","# --- Determine n_timesteps and n_features ---\n","    # n_timesteps: This is your sequence length (e.g., 10 days)\n","n_timesteps_chosen = 10\n","    # n_features: This is the number of columns in your final_processed_data\n","n_features_chosen = final_processed_FTSE100_df.shape[1]\n","\n","print(f\"\\nChosen n_timesteps: {n_timesteps_chosen}\")\n","print(f\"Chosen n_features: {n_features_chosen}\")\n","\n","# --- Call the build_lstm_autoencoder function ---\n","lstm100_autoencoder_model = build_lstm_autoencoder(\n","    n_timesteps=n_timesteps_chosen,\n","    n_features=n_features_chosen,\n","    lstm_units=128, # You can experiment with different unit sizes\n","    dropout_rate=0.2\n",")\n","\n","print(\"\\nLSTM Autoencoder model built successfully.\")\n","# The model is now ready to be trained using X_train, y_train (where y_train is X_train)\n","# as prepared by your create_sequences_autoencoder function.\n"],"metadata":{"id":"HZazAmapHVAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calling the function to build the LSTM Auto Encoder Model for FTSE 100 stocks\n","\n","# --- Determine n_timesteps and n_features ---\n","    # n_timesteps: This is your sequence length (e.g., 10 days)\n","n_timesteps_chosen = 10\n","    # n_features: This is the number of columns in your final_processed_data\n","n_features_chosen = final_processed_FTSE250_df.shape[1]\n","\n","print(f\"\\nChosen n_timesteps: {n_timesteps_chosen}\")\n","print(f\"Chosen n_features: {n_features_chosen}\")\n","\n","# --- Call the build_lstm_autoencoder function ---\n","lstm250_autoencoder_model = build_lstm_autoencoder(\n","    n_timesteps=n_timesteps_chosen,\n","    n_features=n_features_chosen,\n","    lstm_units=128, # You can experiment with different unit sizes\n","    dropout_rate=0.2\n",")\n","\n","print(\"\\nLSTM Autoencoder model built successfully.\")\n","# The model is now ready to be trained using X_train, y_train (where y_train is X_train)\n","# as prepared by your create_sequences_autoencoder function."],"metadata":{"id":"78icbqCmJQKQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_sequences_autoencoder(data_frame, timesteps, features_cols):\n","    \"\"\"\n","    Creates sequences from a multi-index DataFrame for LSTM Autoencoder input.\n","    For autoencoders, X and y are the same sequence.\n","\n","    Args:\n","        data_frame (pd.DataFrame): The input DataFrame with features (already scaled and sorted).\n","        timesteps (int): The number of past time steps to consider for each sequence.\n","        features_cols (list): List of column names that are your features.\n","\n","    Returns:\n","        np.array: A 3D numpy array X in the format (n_samples, n_timesteps, n_features).\n","        np.array: A 3D numpy array y (target) in the same format as X.\n","        list: List of (Date, Ticker) for the *end* of each sequence, for mapping back.\n","    \"\"\"\n","    X, sample_indices = [], []\n","    tickers = data_frame.index.get_level_values('Ticker').unique()\n","\n","    for ticker in tickers:\n","        # Get data for the current ticker, ensuring it's sorted by date\n","        ticker_data = data_frame.loc[(slice(None), ticker), features_cols].droplevel('Ticker').sort_index()\n","\n","        # Check if there's enough data for at least one full sequence\n","        if len(ticker_data) < timesteps: # Note: no +1 here, as y is not a future point\n","            continue\n","\n","        for i in range(len(ticker_data) - timesteps + 1): # +1 to include the last possible sequence\n","            # Extract the sequence of features\n","            feature_sequence = ticker_data.iloc[i:(i + timesteps)].values\n","            X.append(feature_sequence)\n","\n","            # For an autoencoder, the target 'y' is the input 'X' itself.\n","            # However, for convenience in passing to model.fit, we often set y = X_train\n","            # and X = X_train.\n","            # The sample_indices correspond to the last point in the sequence.\n","            sample_date = ticker_data.index[i + timesteps - 1]\n","            sample_indices.append((sample_date, ticker))\n","\n","    # X and y will be the same for an autoencoder trained to reconstruct its input\n","    X = np.array(X)\n","    y = np.array(X) # y is the same as X for reconstruction\n","\n","    return X, y, sample_indices"],"metadata":{"id":"DYI43xfQJE8h","executionInfo":{"status":"ok","timestamp":1751932445158,"user_tz":-60,"elapsed":32,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Assuming final_processed_FTSE100_df is your processed, scaled, NaN-free DataFrame\n","# Assuming n_timesteps_chosen and n_features_chosen are already determined\n","\n","# You need to have the create_sequences_autoencoder function defined in your environment.\n","# (This function was provided in the 'LSTM Data Preparation for MultiIndex DataFrame (Autoencoder Style)' Canvas)\n","\n","X100_lstm, y100_lstm, sample100_indices_lstm = create_sequences_autoencoder(\n","    final_processed_FTSE100_df, # data_frame argument\n","    n_timesteps_chosen,         # timesteps argument (positional, or as timesteps=n_timesteps_chosen)\n","    final_processed_FTSE100_df.columns.tolist() # features_cols argument\n",")\n","\n","print(f\"\\nShape of X for LSTM Autoencoder: {X100_lstm.shape}\")\n","print(f\"Shape of y for LSTM Autoencoder: {y100_lstm.shape}\")\n","print(f\"Number of generated samples: {len(sample100_indices_lstm)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F1GB7xnfKZSk","executionInfo":{"status":"ok","timestamp":1751935450234,"user_tz":-60,"elapsed":1880,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}},"outputId":"80b3a1d0-7477-4bee-df96-33ef01597c22"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Shape of X for LSTM Autoencoder: (41230, 10, 21)\n","Shape of y for LSTM Autoencoder: (41230, 10, 21)\n","Number of generated samples: 41230\n"]}]},{"cell_type":"code","source":["# splitting data\n","\n","# Assuming X_lstm, y_lstm, and sample_indices_lstm are from the previous step\n","\n","# You need to have the split_time_series_data function defined in your environment.\n","# (This function was provided in the 'Chronological Data Split Function' Canvas)\n","\n","# Note: split_time_series_data expects a DataFrame with MultiIndex.\n","# We need to temporarily re-create a DataFrame from X_lstm/y_lstm with the original index\n","# to use split_time_series_data, or manually split the NumPy arrays.\n","# A simpler way for NumPy arrays is direct slicing:\n","\n","# Calculate split points\n","train100_size = int(len(X100_lstm) * 0.7) # 70% for training\n","val100_size = int(len(X100_lstm) * 0.15) # 15% for validation\n","test100_size = len(X100_lstm) - train100_size - val100_size # Remaining 15% for testing\n","\n","# Split the data chronologically\n","X100_train, y100_train = X100_lstm[0:train100_size], y100_lstm[0:train100_size]\n","X100_val, y100_val = X100_lstm[train100_size:train100_size+val100_size], y100_lstm[train100_size:train100_size+val100_size]\n","X100_test, y100_test = X100_lstm[train100_size+val100_size:len(X100_lstm)], y100_lstm[train100_size+val100_size:len(X100_lstm)]\n","\n","# Also split sample_indices_lstm to keep track of original (Date, Ticker) for each sample\n","train100_indices = sample100_indices_lstm[0:train100_size]\n","val100_indices = sample100_indices_lstm[train100_size:train100_size+val100_size]\n","test100_indices = sample100_indices_lstm[train100_size+val100_size:len(X100_lstm)]\n","\n","\n","print(f\"\\nTrain set shape: X_train {X100_train.shape}, y_train {y100_train.shape}\")\n","print(f\"Validation set shape: X_val {X100_val.shape}, y_val {y100_val.shape}\")\n","print(f\"Test set shape: X_test {X100_test.shape}, y_test {y100_test.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rbvb0nXUNNtW","executionInfo":{"status":"ok","timestamp":1751935475447,"user_tz":-60,"elapsed":16,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}},"outputId":"0b705503-5325-4da8-e8fe-9d93a457e50c"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Train set shape: X_train (28860, 10, 21), y_train (28860, 10, 21)\n","Validation set shape: X_val (6184, 10, 21), y_val (6184, 10, 21)\n","Test set shape: X_test (6186, 10, 21), y_test (6186, 10, 21)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Assuming lstm_autoencoder_model is built from build_lstm_autoencoder\n","# Assuming X_train, y_train, X_val, y_val are from the data splitting step\n","\n","# Define training hyperparameters\n","epochs = 50 # Start with a reasonable number, EarlyStopping will manage it\n","batch_size = 32 # Common batch size, adjust based on memory/performance\n","\n","# Define Early Stopping callback\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',  # Monitor the validation loss\n","    patience=10,         # Number of epochs with no improvement after which training will be stopped\n","    restore_best_weights=True, # Restores model weights from the epoch with the best value of the monitored quantity.\n","    verbose=1            # Prints messages when stopping\n",")\n","\n","print(\"\\n--- Training the LSTM Autoencoder Model ---\")\n","history = lstm100_autoencoder_model.fit(\n","    X100_train, y100_train, # For autoencoder, y_train is X_train\n","    epochs=epochs,\n","    batch_size=batch_size,\n","    validation_data=(X100_val, y100_val), # For autoencoder, y_val is X_val\n","    callbacks=[early_stopping],\n","    verbose=1\n",")\n","\n","print(\"\\nLSTM Autoencoder model training complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CC3Kd3uzPQ8E","executionInfo":{"status":"ok","timestamp":1751934864354,"user_tz":-60,"elapsed":119361,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}},"outputId":"64c93b20-b373-40e0-d3a3-ed0eba9a8050"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Training the LSTM Autoencoder Model ---\n","Epoch 1/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.3209 - val_loss: 0.3489\n","Epoch 2/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.3238 - val_loss: 0.3318\n","Epoch 3/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.3656 - val_loss: 0.3375\n","Epoch 4/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.2886 - val_loss: 0.3224\n","Epoch 5/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.3155 - val_loss: 0.3200\n","Epoch 6/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.3006 - val_loss: 0.3628\n","Epoch 7/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.3270 - val_loss: 0.3478\n","Epoch 8/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.3532 - val_loss: 0.3283\n","Epoch 9/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.2832 - val_loss: 0.3490\n","Epoch 10/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.3148 - val_loss: 0.3402\n","Epoch 11/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.3556 - val_loss: 0.3561\n","Epoch 12/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.3181 - val_loss: 0.3524\n","Epoch 13/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.3473 - val_loss: 0.3406\n","Epoch 14/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.2988 - val_loss: 0.3687\n","Epoch 15/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.3411 - val_loss: 0.3096\n","Epoch 16/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.2735 - val_loss: 0.3368\n","Epoch 17/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.3255 - val_loss: 0.3494\n","Epoch 18/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.3988 - val_loss: 0.5466\n","Epoch 19/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.4073 - val_loss: 0.3845\n","Epoch 20/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.3256 - val_loss: 0.4063\n","Epoch 21/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.3185 - val_loss: 0.4112\n","Epoch 22/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.3815 - val_loss: 0.3959\n","Epoch 23/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.3388 - val_loss: 0.3773\n","Epoch 24/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.3789 - val_loss: 0.4599\n","Epoch 25/50\n","\u001b[1m902/902\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.3846 - val_loss: 0.4600\n","Epoch 25: early stopping\n","Restoring model weights from the end of the best epoch: 15.\n","\n","LSTM Autoencoder model training complete.\n"]}]},{"cell_type":"code","source":["# A function to evaluate the performance of your trained LSTM Autoencoder model and detect anomalies using the reconstruction error.\n","\n","def evaluate_and_detect_lstm_anomalies(\n","    model: Sequential,\n","    X_train: np.ndarray,\n","    y_train: np.ndarray, # For autoencoder, y_train is X_train\n","    X_val: np.ndarray,\n","    y_val: np.ndarray,   # For autoencoder, y_val is X_val\n","    X_test: np.ndarray,\n","    y_test: np.ndarray,  # For autoencoder, y_test is X_test\n","    sample_indices_all: list, # List of (Date, Ticker) for all X_lstm samples\n","    original_raw_data: pd.DataFrame # Your ready_ftse100_data or ready_ftse250_data\n",") -> Tuple[pd.DataFrame, pd.DataFrame, float]:\n","    \"\"\"\n","    Evaluates the LSTM Autoencoder model, calculates reconstruction errors,\n","    detects anomalies based on a threshold, and prepares data for visualization.\n","\n","    Args:\n","        model (Sequential): The trained LSTM Autoencoder model.\n","        X_train (np.ndarray): Training input sequences.\n","        y_train (np.ndarray): Training target sequences (same as X_train for autoencoder).\n","        X_val (np.ndarray): Validation input sequences.\n","        y_val (np.ndarray): Validation target sequences (same as X_val for autoencoder).\n","        X_test (np.ndarray): Test input sequences.\n","        y_test (np.ndarray): Test target sequences (same as X_test for autoencoder).\n","        sample_indices_all (list): List of (Date, Ticker) tuples corresponding to each\n","                                   sample in the concatenated X_lstm (train+val+test).\n","        original_raw_data (pd.DataFrame): Your original raw stock data DataFrame\n","                                          (e.g., ready_ftse100_data) with 'close' and other\n","                                          base columns, and MultiIndex (Date, Ticker).\n","\n","    Returns:\n","        tuple[pd.DataFrame, pd.DataFrame, float]:\n","            - full_data_with_anomalies_context (pd.DataFrame): Merged DataFrame with original\n","              data, reconstruction errors, and anomaly labels for all samples.\n","            - anomalies_df_context (pd.DataFrame): DataFrame containing only the detected anomalies\n","              with their original context.\n","            - threshold (float): The calculated anomaly threshold.\n","    \"\"\"\n","    print(\"\\n--- Starting LSTM Anomaly Evaluation and Detection ---\")\n","\n","    # 1. Evaluate Model Performance (Loss on test set)\n","    test_loss = model.evaluate(X_test, y_test, verbose=0)\n","    print(f\"Test Reconstruction Loss (MSE): {test_loss:.6f}\")\n","\n","    # 2. Calculate Reconstruction Errors for all data\n","    # Concatenate all X and y data for unified processing\n","    X_all = np.concatenate((X_train, X_val, X_test), axis=0)\n","    y_all = np.concatenate((y_train, y_val, y_test), axis=0) # Should be same as X_all\n","\n","    print(f\"Predicting reconstructions for all {X_all.shape[0]} samples...\")\n","    X_pred = model.predict(X_all)\n","\n","    # Calculate Mean Squared Error (MSE) for each sequence\n","    # The error is calculated element-wise for the entire sequence, then averaged across timesteps and features.\n","    reconstruction_errors = np.mean(np.square(X_all - X_pred), axis=(1, 2))\n","\n","    # 3. Create DataFrame of Reconstruction Errors\n","    df_reconstruction_error = pd.DataFrame(\n","        {'reconstruction_error': reconstruction_errors},\n","        index=pd.MultiIndex.from_tuples(sample_indices_all, names=['Date', 'Ticker'])\n","    )\n","\n","    print(\"\\nReconstruction Error Statistics:\")\n","    print(df_reconstruction_error.describe())\n","\n","    # 4. Determine Anomaly Threshold\n","    # Get reconstruction errors only for the training set to set the threshold\n","    # The indices for train, val, test splits are based on the order in X_all\n","    train_size = len(X_train)\n","    val_size = len(X_val)\n","\n","    train_errors = reconstruction_errors[0:train_size]\n","\n","    # Define a threshold (e.g., 95th or 99th percentile of training errors)\n","    # This assumes your training data is primarily \"normal\".\n","    # You might tune this percentile based on desired sensitivity.\n","    threshold = np.percentile(train_errors, 95) # 95th percentile is a common starting point\n","    print(f\"\\nAnomaly Threshold (95th percentile of training errors): {threshold:.6f}\")\n","\n","    # 5. Identify Anomalies\n","    df_reconstruction_error['is_anomaly'] = (df_reconstruction_error['reconstruction_error'] > threshold)\n","\n","    # 6. Merge Anomaly Results with Original Raw Data for Context\n","    print(\"\\nMerging anomaly results with original raw data for context...\")\n","\n","    # Get the full date range covered by your scored data\n","    min_date_scored = df_reconstruction_error.index.get_level_values('Date').min()\n","    max_date_scored = df_reconstruction_error.index.get_level_values('Date').max()\n","\n","    original_data_for_merge = original_raw_data.loc[\n","        (original_raw_data.index.get_level_values('Date') >= min_date_scored) &\n","        (original_raw_data.index.get_level_values('Date') <= max_date_scored)\n","    ].copy()\n","\n","    # Crucial: Drop duplicates from the MultiIndex if any exist, before any merge operations.\n","    if not original_data_for_merge.index.is_unique:\n","        print(\"Warning: Duplicates found in original_data_for_merge index. Dropping them.\")\n","        original_data_for_merge = original_data_for_merge[~original_data_for_merge.index.duplicated(keep='first')].copy()\n","\n","    df_reconstruction_error_unique = df_reconstruction_error.copy()\n","    if not df_reconstruction_error.index.is_unique:\n","        print(\"Warning: Duplicates found in df_reconstruction_error index. Dropping them.\")\n","        df_reconstruction_error_unique = df_reconstruction_error[~df_reconstruction_error.index.duplicated(keep='first')].copy()\n","\n","    common_indices = original_data_for_merge.index.intersection(df_reconstruction_error_unique.index)\n","    original_data_aligned = original_data_for_merge.loc[common_indices]\n","    reconstruction_data_aligned = df_reconstruction_error_unique.loc[common_indices]\n","\n","    # Perform the merge using `left_index=True` and `right_index=True`\n","    full_data_with_anomalies_context = original_data_aligned.merge(\n","        reconstruction_data_aligned[['reconstruction_error', 'is_anomaly']], # Only merge these columns\n","        left_index=True,\n","        right_index=True,\n","        how='left'\n","    )\n","    # Ensure 'is_anomaly' is boolean after merge for consistency\n","    if 'is_anomaly' in full_data_with_anomalies_context.columns:\n","        full_data_with_anomalies_context['is_anomaly'] = full_data_with_anomalies_context['is_anomaly'].astype(bool)\n","\n","    print(f\"Full data with anomalies context shape: {full_data_with_anomalies_context.shape}\")\n","    print(f\"Is index truly unique in full_data_with_anomalies_context? {full_data_with_anomalies_context.index.is_unique}\")\n","\n","    # Filter to get only the anomalies with context\n","    anomalies_df_context = full_data_with_anomalies_context[full_data_with_anomalies_context['is_anomaly'] == True].copy()\n","\n","    num_anomalies_detected = anomalies_df_context.shape[0]\n","    print(f\"\\nTotal LSTM Anomalies Detected: {num_anomalies_detected}\")\n","    print(\"\\nDetected LSTM Anomalies (head):\\n\", anomalies_df_context.head())\n","\n","    # 7. Visualization (Overall Error Distribution)\n","    plt.figure(figsize=(12, 6))\n","    sns.histplot(reconstruction_errors, bins=50, kde=True, color='skyblue')\n","    plt.axvline(threshold, color='red', linestyle='--', label=f'Anomaly Threshold ({threshold:.4f})')\n","    plt.title('Distribution of LSTM Reconstruction Errors')\n","    plt.xlabel('Reconstruction Error (MSE)')\n","    plt.ylabel('Frequency')\n","    plt.legend()\n","    plt.grid(True, linestyle='--', alpha=0.7)\n","    plt.show()\n","\n","    return full_data_with_anomalies_context, anomalies_df_context, threshold"],"metadata":{"id":"mM6QBqLbXHve","executionInfo":{"status":"ok","timestamp":1751933457750,"user_tz":-60,"elapsed":60,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# --- Call the evaluate_and_detect_lstm_anomalies function ---\n","full_lstm_results, anomalies_lstm_df, threshold_lstm = evaluate_and_detect_lstm_anomalies(\n","    model=lstm100_autoencoder_model,\n","    X_train=X100_train,\n","    y_train=y100_train,\n","    X_val=X100_val,\n","    y_val=y100_val,\n","    X_test=X100_test,\n","    y_test=y100_test,\n","    sample_indices_all=sample100_indices_lstm,\n","    original_raw_data=ready_ftse100_data # Your actual ready_ftse100_data or ready_ftse250_data\n",")\n","\n","print(\"\\n--- LSTM Anomaly Detection Complete ---\")\n","print(\"\\nHead of full_lstm_results (merged with original data):\\n\", full_lstm_results.head())\n","print(\"\\nHead of anomalies_lstm_df (only detected anomalies):\\n\", anomalies_lstm_df.head())\n","print(f\"\\nCalculated LSTM Anomaly Threshold: {threshold_lstm:.6f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ySk5tzyqXyo0","executionInfo":{"status":"ok","timestamp":1751935639217,"user_tz":-60,"elapsed":5026,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}},"outputId":"831088c0-3ac9-4f7d-e2a0-e07498edc591"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Starting LSTM Anomaly Evaluation and Detection ---\n","Test Reconstruction Loss (MSE): 0.198646\n","Predicting reconstructions for all 41230 samples...\n","\u001b[1m1289/1289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n","\n","Reconstruction Error Statistics:\n","       reconstruction_error\n","count          41230.000000\n","mean               0.229098\n","std                2.114853\n","min                0.019814\n","25%                0.085760\n","50%                0.122401\n","75%                0.187989\n","max              212.692967\n","\n","Anomaly Threshold (95th percentile of training errors): 0.395990\n","\n","Merging anomaly results with original raw data for context...\n","Full data with anomalies context shape: (41230, 8)\n","Is index truly unique in full_data_with_anomalies_context? True\n","\n","Total LSTM Anomalies Detected: 2475\n","\n","Detected LSTM Anomalies (head):\n","                      adj close   close         high     low    open     volume  reconstruction_error  is_anomaly\n","Date       Ticker                                                                                               \n","2014-02-17 DGE     1860.278442  1865.5  1866.500000   950.0  1851.0  3641135.0              1.942113        True\n","2014-02-18 DGE     1865.763062  1871.0  1879.800049  1857.0  1878.0  3947419.0              0.435264        True\n","2014-02-19 DGE     1897.174683  1902.5  1903.050049  1870.5  1874.0  5223273.0              1.164619        True\n","2014-02-20 DGE     1898.171997  1903.5  1903.500000  1878.5  1893.0  3403806.0              0.625939        True\n","2014-02-21 DGE     1889.196899  1894.5  1920.000000  1890.0  1906.0  3765554.0              0.604611        True\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","--- LSTM Anomaly Detection Complete ---\n","\n","Head of full_lstm_results (merged with original data):\n","                      adj close        close         high          low         open      volume  reconstruction_error  is_anomaly\n","Date       Ticker                                                                                                               \n","2014-02-12 AZN     3987.271729  4002.500000  4008.500000  3956.500000  3989.500000   2830986.0              0.129761       False\n","           BARC     259.617554   260.450012   271.515991   259.750000   264.700012  58225865.0              0.146167       False\n","           BATS    2974.115723  2995.000000  3012.500000  2985.000000  3000.000000   3635252.0              0.096368       False\n","           BHP     1852.980347  1862.500000  1881.000000  1852.035034  1864.000000   7398187.0              0.070718       False\n","           BP       483.940948   487.049988   493.200012   486.000000   487.350006  25415556.0              0.063192       False\n","\n","Head of anomalies_lstm_df (only detected anomalies):\n","                      adj close   close         high     low    open     volume  reconstruction_error  is_anomaly\n","Date       Ticker                                                                                               \n","2014-02-17 DGE     1860.278442  1865.5  1866.500000   950.0  1851.0  3641135.0              1.942113        True\n","2014-02-18 DGE     1865.763062  1871.0  1879.800049  1857.0  1878.0  3947419.0              0.435264        True\n","2014-02-19 DGE     1897.174683  1902.5  1903.050049  1870.5  1874.0  5223273.0              1.164619        True\n","2014-02-20 DGE     1898.171997  1903.5  1903.500000  1878.5  1893.0  3403806.0              0.625939        True\n","2014-02-21 DGE     1889.196899  1894.5  1920.000000  1890.0  1906.0  3765554.0              0.604611        True\n","\n","Calculated LSTM Anomaly Threshold: 0.395990\n"]}]},{"cell_type":"code","source":["!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zsI7cXzA8JWY","executionInfo":{"status":"ok","timestamp":1751422982684,"user_tz":-60,"elapsed":515,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}},"outputId":"5676b973-10ab-410e-9586-9e9b3ece9235"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   Unsupervised_Anomaly_Detection.ipynb\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}]}]}