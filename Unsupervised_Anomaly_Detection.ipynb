{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m7jN0d3qJZZK","outputId":"7f016303-9b1c-48cf-a118-c0b0f7b3ec36","executionInfo":{"status":"ok","timestamp":1750115572817,"user_tz":-60,"elapsed":23941,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Importing drive model from the google.colab package\n","from google.colab import drive\n","\n","#Mounting the google drive to a specific path\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_8kLZ9NLAgy","outputId":"2b9895f4-38d1-400d-eae6-b71b1a5602dd","executionInfo":{"status":"ok","timestamp":1750071478727,"user_tz":-60,"elapsed":316,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection\n"]}]},{"cell_type":"code","source":["# Configuring Git user details\n","!git config --global user.email \"dorothy.sarpongk@gmail.com\"\n","!git config --global user.name \"01DorSarpong\""],"metadata":{"id":"sdbbjCzE0tWT","executionInfo":{"status":"ok","timestamp":1750115579431,"user_tz":-60,"elapsed":206,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Importing libraries for code\n","\n","import pandas as pd\n","import numpy  as np\n","import yfinance as yf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from tqdm import tqdm\n","from typing import Tuple, Union\n"],"metadata":{"id":"jQ_wBKNkmV25","executionInfo":{"status":"ok","timestamp":1750117220972,"user_tz":-60,"elapsed":5,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def download_and_save_FTSE_stocks(tickers: list, start_date: str, end_date: str, directory: str):\n","  \"\"\"\n","  This function downloads historical stock data for a list of tickers and saves it to CSV files.\n","\n","  Args:\n","    tickers_list (list): A list of stock ticker symbols (e.g., ['TSCO.L', 'BARC.L']).\n","    start_date (str): The start date for data download in 'YYYY-MM-DD' format.\n","    end_date (str): The end date for data download in 'YYYY-MM-DD' format.\n","    directory (str): The path to the directory where CSV files will be saved.\n","  \"\"\"\n","\n","  # Ensure the save directory exists\n","  if not os.path.exists(directory):\n","    os.makedirs(directory)\n","    print(f\"Created directory: {directory}\")\n","\n","  print(f\"Starting download for {len(tickers)} tickers from {start_date} to {end_date}...\")\n","\n","  for ticker in tqdm(tickers, desc=\"Downloading Stocks\"):\n","    # Format the filename: remove '.L' and add date range for clarity\n","    cleaned_ticker = ticker.replace('.L', '')\n","    file_name = f\"{cleaned_ticker}_{start_date.replace('-', '')}_{end_date.replace('-', '')}.csv\"\n","    full_file_path = os.path.join(directory, file_name)\n","\n","    try:\n","      df = yf.download(ticker, start=start_date, end=end_date, auto_adjust=False)\n","\n","      if not df.empty:\n","        df.to_csv(full_file_path)\n","        # print(f\"✅ Saved data for {ticker} to {full_file_path}\") # Optional: uncomment for more verbose output\n","      else:\n","        print(f\"⚠️ No data available for {ticker} for the specified period.\")\n","    except Exception as e:\n","      print(f\"❌ Error downloading or saving data for {ticker}: {e}\")\n","\n","  print(\"Download process completed.\")\n"],"metadata":{"id":"6Iby76j7PtZR","executionInfo":{"status":"ok","timestamp":1750115644198,"user_tz":-60,"elapsed":3,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#Creating a list of FTSE 100 and FTSE 250 tickers\n","\n","FTSE_100_tickers = [\"AZN.L\", \"HSBA.L\", \"ULVR.L\", \"REL.L\", \"BATS.L\", \"BP.L\", \"GSK.L\", \"DGE.L\",\n","                   \"RR.L\", \"NG.L\", \"BARC.L\", \"TSCO.L\", \"PRU.L\", \"BHP.L\", \"BT-A.L\",]\n","\n","FTSE_250_tickers = [\"BWY.L\", \"EMG.L\", \"JUST.L\", \"SXS.L\", \"CKN.L\", \"LRE.L\", \"RAT.L\", \"THG.L\",\n","                    \"JDW.L\", \"SCT.L\", \"DOM.L\", \"SRE.L\", \"HIK.L\", \"ICGT.L\", \"HSX.L\"]"],"metadata":{"id":"2L4EdTEsuCu8","executionInfo":{"status":"ok","timestamp":1750115647519,"user_tz":-60,"elapsed":15,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#Defining the period for stocks range\n","start_date = \"2014-01-01\"\n","end_date = \"2024-12-31\""],"metadata":{"id":"yESfV6rzUuQ1","executionInfo":{"status":"ok","timestamp":1750115650259,"user_tz":-60,"elapsed":3,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Defining the path to save the CSVs\n","\n","ftse_100_path = '/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection/FTSE_100'\n","ftse_250_path = '/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection/FTSE_250'\n"],"metadata":{"id":"w9mFm0wLVH6v","executionInfo":{"status":"ok","timestamp":1750115652453,"user_tz":-60,"elapsed":5,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Calling the function for FTSE 100 and FTSE 250 tickers\n","\n","download_and_save_FTSE_stocks(\n","    tickers=FTSE_100_tickers,\n","    start_date=start_date,\n","    end_date=end_date,\n","    directory=ftse_100_path\n",")\n","\n","download_and_save_FTSE_stocks(\n","    tickers=FTSE_250_tickers,\n","    start_date=start_date,\n","    end_date=end_date,\n","    directory=ftse_250_path\n",")"],"metadata":{"id":"dnZIuB7oXMSA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a function to load stocks and pre-process into a dataframe\n","\n","def load_and_structure_stock_data(folder_path: str) -> pd.DataFrame:\n","    \"\"\"\n","    Loads historical stock data from CSV files in a specified folder,\n","    cleans, processes, filters to weekdays, fills NaNs/gaps, and\n","    consolidates them into a single structured DataFrame with a MultiIndex.\n","\n","    Args:\n","        folder_path (str): The path to the directory containing stock data CSVs.\n","\n","    Returns:\n","        pd.DataFrame: A single DataFrame containing data for all tickers,\n","                      indexed by 'Date' and 'Ticker', sorted by Date and then Ticker.\n","                      Returns an empty DataFrame if no data is loaded or processed.\n","    \"\"\"\n","    all_dfs = []\n","\n","    if not os.path.exists(folder_path):\n","        print(f\"❌ Error: Folder not found at {folder_path}\")\n","        return pd.DataFrame() # Return empty DataFrame if folder doesn't exist\n","\n","    # Get list of CSV files to process\n","    file_list = [f.name for f in os.scandir(folder_path) if f.name.endswith(\".csv\")]\n","\n","    if not file_list:\n","        print(f\"⚠️ No CSV files found in {folder_path}\")\n","        return pd.DataFrame()\n","\n","    print(f\"Loading and processing data from {len(file_list)} CSV files in {folder_path}...\")\n","\n","    # Define columns that typically contain numerical stock data to be filled\n","    numerical_cols_to_fill = ['open', 'high', 'low', 'close', 'adj close', 'volume']\n","\n","    for filename in tqdm(file_list, desc=\"Processing Stock Files\"):\n","        file_path = os.path.join(folder_path, filename)\n","\n","        try:\n","            # Read CSV: use first row as header, skip second row (often contains ticker name repeated)\n","            df = pd.read_csv(file_path, header=0, skiprows=[1], encoding='utf-8-sig')\n","\n","            # --- Initial Cleaning and Date Conversion ---\n","            # Rename the first column to 'Date' if it's not already\n","            if df.columns[0].strip().lower() != 'date':\n","                df.rename(columns={df.columns[0]: 'Date'}, inplace=True)\n","\n","            # Normalize all column names to lowercase and remove leading/trailing spaces\n","            df.columns = [col.strip().lower() for col in df.columns]\n","\n","            # Clean and prepare the 'date' column\n","            df['date'] = df['date'].astype(str).str.strip()\n","            df = df[df['date'].str.lower() != 'date'] # Drop any rows where the date column contains the string 'Date'\n","\n","            # Convert to datetime, coercing errors will turn unparseable dates into NaT\n","            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n","            df.dropna(subset=['date'], inplace=True) # Drop rows where date conversion failed (NaT)\n","\n","            # --- Add Ticker Information ---\n","            # Extract ticker from filename (e.g., 'TSCO_20140101_20241231.csv' -> 'TSCO')\n","            ticker_symbol = filename.split('_')[0]\n","            df['ticker'] = ticker_symbol\n","\n","            # --- Set Date as Index (temporarily for filtering/filling) ---\n","            df.set_index('date', inplace=True)\n","\n","            # --- Filter for Weekdays Only ---\n","            # .dayofweek returns Monday=0, ..., Sunday=6. Keep only 0 to 4.\n","            df = df[df.index.dayofweek < 5]\n","\n","            # --- Fill NaNs/Gaps for Numerical Columns ---\n","            # Identify columns to fill that actually exist in the current DataFrame\n","            existing_numerical_cols = [col for col in numerical_cols_to_fill if col in df.columns]\n","\n","            if not existing_numerical_cols:\n","                # print(f\"⚠️ No numerical columns found for {filename}. Skipping NaN filling for this file.\")\n","                pass # This is fine, just means the file might only have non-numerical data or very specific columns\n","\n","            # Apply forward-fill then backward-fill for numerical columns within this ticker's data\n","            # This handles gaps for individual stock series\n","            df[existing_numerical_cols] = df[existing_numerical_cols].ffill().bfill()\n","\n","            # --- Final Check for essential data after filling ---\n","            # If 'open' column is vital and still has NaNs (e.g., entire series was NaN), drop those rows\n","            if 'open' in df.columns:\n","                df.dropna(subset=['open'], inplace=True)\n","            else:\n","                print(f\"⚠️ 'open' column not found in {filename}. Skipping this file as essential data is missing.\")\n","                continue # Skip this file if 'open' is genuinely missing\n","\n","            if df.empty:\n","                print(f\"⚠️ No valid weekday data remaining for {filename} after filtering. Skipping.\")\n","                continue\n","\n","            all_dfs.append(df)\n","\n","        except pd.errors.EmptyDataError:\n","            print(f\"⚠️ {filename} is empty. Skipping.\")\n","        except pd.errors.ParserError as e:\n","            print(f\"❌ Error parsing {filename}: {type(e).__name__} - {e}. Skipping.\")\n","        except Exception as e:\n","            print(f\"❌ Error processing {filename}: {type(e).__name__} - {e}. Skipping.\")\n","\n","    if not all_dfs:\n","        print(\"No valid stock data loaded after processing. Returning empty DataFrame.\")\n","        return pd.DataFrame()\n","\n","    # --- Consolidate all DataFrames into one structured DataFrame ---\n","    combined_df = pd.concat(all_dfs)\n","\n","    # Set a MultiIndex: primary index is 'Date', secondary is 'ticker'\n","    # 'ticker' was added as a regular column inside the loop, now it becomes part of the index\n","    combined_df.set_index('ticker', append=True, inplace=True)\n","    combined_df.index.names = ['Date', 'Ticker']\n","\n","    # Sort the MultiIndex for better performance and consistency\n","    combined_df.sort_index(inplace=True)\n","\n","    print(\"All stock data loaded, structured, filtered, and filled successfully.\")\n","    return combined_df\n","\n"],"metadata":{"id":"oQt_Z87EH2L9","executionInfo":{"status":"ok","timestamp":1750115679522,"user_tz":-60,"elapsed":4,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["#Calling the function to load and structure df for FTSE 100 stocks\n","\n","ready_ftse100_data = load_and_structure_stock_data(ftse_100_path)\n","\n","if not ready_ftse100_data.empty:\n","  print(\"\\n--- Final Structured and Cleaned DataFrame ---\")\n","  print(ready_ftse100_data.head(15)) # Show more rows to see multiple dates/tickers\n","  print(\"\\nDataFrame Info:\")\n","  ready_ftse100_data.info()\n","  print(\"\\nSample of weekdays (should only be Mon-Fri):\")\n","  print(ready_ftse100_data.index.get_level_values('Date').day_name().value_counts())\n","  print(\"\\nNaNs after processing (should be very few or none in numerical columns):\")\n","  print(ready_ftse100_data.isnull().sum())\n","else:\n"," print(\"No data was successfully loaded and processed.\")"],"metadata":{"id":"gK9gOL3vIK_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calling the function to load and structure df for FTSE 250 stocks\n","\n","ready_ftse250_data = load_and_structure_stock_data(ftse_250_path)\n","\n","if not ready_ftse250_data.empty:\n","  print(\"\\n--- Final Structured and Cleaned DataFrame ---\")\n","  print(ready_ftse250_data.head(15)) # Show more rows to see multiple dates/tickers\n","  print(\"\\nDataFrame Info:\")\n","  ready_ftse250_data.info()\n","  print(\"\\nSample of weekdays (should only be Mon-Fri):\")\n","  print(ready_ftse250_data.index.get_level_values('Date').day_name().value_counts())\n","  print(\"\\nNaNs after processing (should be very few or none in numerical columns):\")\n","  print(ready_ftse250_data.isnull().sum())\n","else:\n"," print(\"No data was successfully loaded and processed.\")"],"metadata":{"id":"XktAAHheAscj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to genereate the stock features needed to build ML model\n","\n","def generate_all_stock_features(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Generates a comprehensive set of time-series and cross-sectional numerical features\n","    for stock data, suitable for unsupervised anomaly detection models.\n","\n","    This function assumes the input DataFrame has a MultiIndex (Date, Ticker)\n","    and contains cleaned base columns like 'open', 'high', 'low', 'close',\n","    'adj close', and 'volume', with no critical NaNs.\n","\n","    Args:\n","        df (pd.DataFrame): Your clean DataFrame with MultiIndex (Date, Ticker)\n","                           and base stock price/volume data.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with all original and newly engineered numerical\n","                      features. NaNs introduced by calculations (e.g., at the start\n","                      of rolling windows) will be present.\n","    \"\"\"\n","    if df.empty:\n","        print(\"Input DataFrame is empty for feature generation. Returning empty DataFrame.\")\n","        return pd.DataFrame()\n","\n","    processed_df = df.copy() # Always work on a copy to keep the original untouched\n","\n","    # --- Ensure critical columns are numerical for calculations ---\n","    for col in ['open', 'high', 'low', 'close', 'adj close', 'volume']:\n","        if col in processed_df.columns:\n","            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')\n","    processed_df.dropna(subset=['open','close','volume'], inplace=True)\n","    if processed_df.empty:\n","        print(\"No data remaining after ensuring essential columns are numerical and not NaN.\")\n","        return pd.DataFrame()\n","\n","\n","    grouped_by_ticker = processed_df.groupby(level='Ticker')\n","    grouped_by_date = processed_df.groupby(level='Date')\n","    epsilon = 1e-9\n","\n","    print(\"Generating Time-Series Based Features (per stock ticker)...\")\n","\n","    # 1. Return-Based Features:\n","    processed_df['log_return'] = grouped_by_ticker['close'].transform(\n","        lambda x: np.log(x / x.shift(1).replace(0, epsilon)).replace([-np.inf, np.inf], np.nan)\n","    )\n","    processed_df['simple_return'] = grouped_by_ticker['close'].transform(\n","        lambda x: (x / x.shift(1).replace(0, epsilon)) - 1\n","    )\n","    if 'adj close' in processed_df.columns:\n","        processed_df['log_adj_close_return'] = grouped_by_ticker['adj close'].transform(\n","            lambda x: np.log(x / x.shift(1).replace(0, epsilon)).replace([-np.inf, np.inf], np.nan)\n","        )\n","    processed_df['return_5d'] = grouped_by_ticker['close'].transform(lambda x: x.pct_change(periods=5))\n","    processed_df['return_20d'] = grouped_by_ticker['close'].transform(lambda x: x.pct_change(periods=20))\n","\n","    print(\"Generating Volatility Measures...\")\n","    # 2. Volatility Measures:\n","    processed_df['rolling_std_5d_log_return'] = grouped_by_ticker['log_return'].transform(lambda x: x.rolling(window=5).std())\n","    processed_df['rolling_std_20d_log_return'] = grouped_by_ticker['log_return'].transform(lambda x: x.rolling(window=20).std())\n","\n","    if 'high' in processed_df.columns and 'low' in processed_df.columns:\n","        processed_df['daily_range_norm'] = (processed_df['high'] - processed_df['low']) / (processed_df['close'] + epsilon)\n","\n","        log_high_div_low = np.log((processed_df['high'] / processed_df['low'].replace(0, epsilon)).clip(lower=epsilon))\n","        log_close_div_open = np.log((processed_df['close'] / processed_df['open'].replace(0, epsilon)).clip(lower=epsilon))\n","\n","        gk_term = 0.5 * (log_high_div_low)**2 - (2 * np.log(2) - 1) * (log_close_div_open)**2\n","        gk_term[gk_term < 0] = np.nan # Set negative values before sqrt to NaN\n","        processed_df['garman_klass_vol'] = np.sqrt(gk_term)\n","\n","        # Explicitly replace infs/NaNs that might result from sqrt or division by zero, and fill\n","        processed_df['garman_klass_vol'] = processed_df['garman_klass_vol'].replace([-np.inf, np.inf], np.nan)\n","        processed_df['garman_klass_vol'].fillna(0, inplace=True) # Fill with 0 for Garman-Klass specific NaNs\n","    else:\n","        print(\"⚠️ Missing 'high' or 'low' columns for some volatility features. Skipping.\")\n","\n","    print(\"Generating Volume-Based Features...\")\n","    # 3. Volume-Based Features:\n","    processed_df['volume_change'] = grouped_by_ticker['volume'].transform(lambda x: x.pct_change(periods=1))\n","    processed_df['avg_volume_20d'] = grouped_by_ticker['volume'].transform(lambda x: x.rolling(window=20).mean())\n","    processed_df['relative_volume'] = processed_df['volume'] / (processed_df['avg_volume_20d'] + epsilon)\n","\n","    print(\"Generating Momentum/Trend Indicators...\")\n","    # 4. Momentum/Trend Indicators:\n","    processed_df['sma_5d'] = grouped_by_ticker['close'].transform(lambda x: x.rolling(window=5).mean())\n","    processed_df['sma_20d'] = grouped_by_ticker['close'].transform(lambda x: x.rolling(window=20).mean())\n","    processed_df['deviation_from_sma_20d'] = (processed_df['close'] - processed_df['sma_20d']) / (processed_df['sma_20d'] + epsilon)\n","\n","    print(\"Generating Price-Volume Interaction Features...\")\n","    # 5. Price-Volume Interaction Features:\n","    if 'high' in processed_df.columns and 'low' in processed_df.columns:\n","        processed_df['typical_price'] = (processed_df['high'] + processed_df['low'] + processed_df['close']) / 3\n","    else:\n","        print(\"⚠️ Missing 'high' or 'low' columns. Skipping 'typical_price'.\")\n","\n","    print(\"Generating Cross-Sectional Features (comparing stocks on the same day)...\")\n","    # 6. Cross-Sectional Features:\n","    if 'log_return' in processed_df.columns:\n","        processed_df['daily_market_mean_log_return'] = grouped_by_date['log_return'].transform('mean')\n","        processed_df['daily_market_median_log_return'] = grouped_by_date['log_return'].transform('median')\n","\n","        processed_df['deviation_from_daily_mean_return'] = processed_df['log_return'] - processed_df['daily_market_mean_log_return']\n","        processed_df['deviation_from_daily_median_return'] = processed_df['log_return'] - processed_df['daily_market_median_log_return']\n","        processed_df['daily_return_rank_pct'] = grouped_by_date['log_return'].rank(pct=True, method='average')\n","    else:\n","        print(\"⚠️ 'log_return' column not available for cross-sectional feature generation. Skipping these features.\")\n","\n","    # --- FINAL CLEANUP: Replace any remaining inf/-inf with NaN across all numerical columns ---\n","    print(\"Finalizing features: cleaning up any remaining inf/-inf values...\")\n","    numerical_cols_after_gen = processed_df.select_dtypes(include=np.number).columns\n","    for col in numerical_cols_after_gen:\n","        processed_df[col] = processed_df[col].replace([np.inf, -np.inf], np.nan)\n","\n","    print(\"Feature generation complete. NaNs from calculations are present and will need further handling.\")\n","    return processed_df.sort_index()"],"metadata":{"id":"CX47oDUYDIPX","executionInfo":{"status":"ok","timestamp":1750115874470,"user_tz":-60,"elapsed":25,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["\n","# Call the feature generation function on the FTSE 100 stock\n","\n","df_with_all_100_features = generate_all_stock_features(ready_ftse100_data)\n","\n","if not df_with_all_100_features.empty:\n","    print(\"\\n--- NEWLY GENERATED DataFrame with ALL Engineered Features (Head) ---\")\n","    # To ensure you see all columns, even if there are many:\n","    pd.set_option('display.max_columns', None) # Display all columns\n","    pd.set_option('display.width', 1000)      # Ensure wide display in terminal/Colab output\n","\n","    # Now, print the head of the *NEW* DataFrame\n","    print(df_with_all_100_features.head(15))\n","\n","    print(\"\\n--- NEWLY GENERATED DataFrame Info (should show many more columns) ---\")\n","    # Now, print the info of the *NEW* DataFrame\n","    df_with_all_100_features.info()\n","\n","    print(\"\\n--- ALL Column Names in the NEW DataFrame ---\")\n","    # This will explicitly list ALL column names, proving they are there\n","    print(df_with_all_100_features.columns.tolist())\n","\n","    print(\"\\n--- Count of NaNs per column (expect some NaNs from rolling/shifting, esp. at start of series) ---\")\n","    print(df_with_all_100_features.isnull().sum().sort_values(ascending=False).head(20))\n","else:\n","    print(\"The feature generation function returned an empty DataFrame.\")"],"metadata":{"id":"GNB55xrl1RAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Call the feature generation function on the FTSE 100 stock\n","\n","df_with_all_250_features = generate_all_stock_features(ready_ftse250_data)\n","\n","if not df_with_all_250_features.empty:\n","    print(\"\\n--- NEWLY GENERATED DataFrame with ALL Engineered Features (Head) ---\")\n","    # To ensure you see all columns, even if there are many:\n","    pd.set_option('display.max_columns', None) # Display all columns\n","    pd.set_option('display.width', 1000)      # Ensure wide display in terminal/Colab output\n","\n","    # Now, print the head of the *NEW* DataFrame\n","    print(df_with_all_250_features.head(15))\n","\n","    print(\"\\n--- NEWLY GENERATED DataFrame Info (should show many more columns) ---\")\n","    # Now, print the info of the *NEW* DataFrame\n","    df_with_all_250_features.info()\n","\n","    print(\"\\n--- ALL Column Names in the NEW DataFrame ---\")\n","    # This will explicitly list ALL column names, proving they are there\n","    print(df_with_all_250_features.columns.tolist())\n","\n","    print(\"\\n--- Count of NaNs per column (expect some NaNs from rolling/shifting, esp. at start of series) ---\")\n","    print(df_with_all_250_features.isnull().sum().sort_values(ascending=False).head(20))\n","else:\n","    print(\"The feature generation function returned an empty DataFrame.\")"],"metadata":{"id":"pkgU-aLTsJr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to remove all NaNs from features and scales values using standard scaler\n","#from sklearn.preprocessing import StandardScaler\n","\n","def preprocess_features_for_model(\n","    df_with_all_features: pd.DataFrame,\n","    columns_to_exclude_from_features: list = None,\n","    apply_scaling: bool = True,\n","    scaler_obj: StandardScaler = None # Optional: provide a pre-fitted scaler for consistent scaling\n",") -> Tuple[pd.DataFrame, Union[StandardScaler, None]]:\n","    \"\"\"\n","    Handles NaN removal, feature selection, and feature scaling for a DataFrame\n","    containing engineered stock features. This prepares the data for anomaly\n","    detection models.\n","\n","    Args:\n","        df_with_engineered_features (pd.DataFrame): The DataFrame containing\n","                                                    all engineered features, with a MultiIndex.\n","                                                    (Output of `generate_all_stock_features`).\n","        columns_to_exclude_from_features (list, optional): A list of column names\n","                                                        that should NOT be treated as features\n","                                                        for the model (e.g., raw 'open', 'close',\n","                                                        or non-numeric helper columns like 'weekday').\n","                                                        If None, a default list is used.\n","        apply_scaling (bool): Whether to apply StandardScaler to numerical features.\n","                              Defaults to True. Highly recommended for most ML models.\n","        scaler_obj (StandardScaler, optional): An pre-fitted StandardScaler object.\n","                                               If `apply_scaling` is True and `scaler_obj` is None,\n","                                               a new scaler will be fitted. Useful for consistent\n","                                               scaling between train/test datasets.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - pd.DataFrame: The DataFrame with selected, cleaned, and optionally scaled\n","                            numerical features, ready for an anomaly detection model.\n","                            Retains the MultiIndex.\n","            - StandardScaler or None: The fitted or used StandardScaler object if\n","                                      scaling was applied, else None.\n","    \"\"\"\n","    if df_with_all_features.empty:\n","        print(\"Input DataFrame is empty for preprocessing. Returning empty DataFrame.\")\n","        return pd.DataFrame(), None\n","\n","    processed_df = df_with_all_features.copy()\n","    print(\"\\n--- Starting Feature Preprocessing for Model ---\")\n","\n","    # --- 1. Feature Selection (Identify Numerical Features to Use) ---\n","    # Default list of columns that are typically not features, but base data or helpers\n","    if columns_to_exclude_from_features is None:\n","        columns_to_exclude_from_features = ['open', 'high', 'low', 'close', 'adj close', 'volume', 'weekday']\n","\n","    # Get all numerical columns from the DataFrame\n","    all_numerical_cols = processed_df.select_dtypes(include=np.number).columns.tolist()\n","\n","    # Filter out the columns that should be excluded\n","    features_for_model_names = [\n","        col for col in all_numerical_cols\n","        if col not in columns_to_exclude_from_features\n","    ]\n","\n","    if not features_for_model_names:\n","        print(\"⚠️ No valid numerical features identified after exclusion. Using all numerical original columns.\")\n","        features_for_model_names = all_numerical_cols # Fallback to all if custom exclusion leads to empty list\n","\n","    print(f\"Selected {len(features_for_model_names)} features for the model.\")\n","    df_features_only = processed_df[features_for_model_names].copy()\n","\n","\n","    # --- 2. NaN Removal (Final Handling for Model Input) ---\n","    # Drop rows where any of the *selected features* have NaNs.\n","    # This is critical as most ML models cannot handle NaNs.\n","    original_rows_count = df_features_only.shape[0]\n","    df_features_only.dropna(inplace=True)\n","    rows_after_nan_drop = df_features_only.shape[0]\n","\n","    if original_rows_count > rows_after_nan_drop:\n","        print(f\"Dropped {original_rows_count - rows_after_nan_drop} rows due to NaNs in selected features.\")\n","    if df_features_only.empty:\n","        print(\"DataFrame is empty after NaN removal. Cannot proceed with preprocessing.\")\n","        return pd.DataFrame(), None\n","    print(f\"Data shape after NaN removal: {df_features_only.shape}\")\n","\n","\n","    # --- 3. Feature Scaling ---\n","    scaler = None\n","    if apply_scaling:\n","        print(\"Applying StandardScaler to features...\")\n","        scaler = scaler_obj if scaler_obj is not None else StandardScaler()\n","\n","        # Fit and/or transform the features\n","        X_scaled = scaler.fit_transform(df_features_only) if scaler_obj is None else scaler.transform(df_features_only)\n","\n","        # Convert scaled array back to DataFrame, retaining index and column names\n","        df_scaled_features = pd.DataFrame(X_scaled, index=df_features_only.index, columns=df_features_only.columns)\n","        print(f\"Features scaled. Scaler: {'New' if scaler_obj is None else 'Existing'}.\")\n","    else:\n","        df_scaled_features = df_features_only.copy()\n","        print(\"Skipping feature scaling.\")\n","\n","    print(\"--- Feature Preprocessing for Model Complete ---\")\n","    return df_scaled_features, scaler"],"metadata":{"id":"IULteCpM6AMz","executionInfo":{"status":"ok","timestamp":1750117281729,"user_tz":-60,"elapsed":6,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#Calling the preprocess_features_for_model function on the FTSE 100 and FTSE 250 stocks\n","\n","final_processed_FTSE100_df, fitted_scaler = preprocess_features_for_model(\n","    df_with_all_100_features.copy(), # Pass a copy to avoid modifying the original df_with_all_features\n","    columns_to_exclude_from_features=None, # Use default exclusion (removes original price/volume/weekday)\n","    apply_scaling=True,                     # Apply StandardScaler (best for all 3 models)\n","    scaler_obj=None                         # Fit a new scaler\n",")\n","if not final_processed_FTSE100_df.empty:\n","    print(\"\\n--- Processed DataFrame (Head) after fixing infs ---\")\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    print(final_processed_FTSE100_df.head(15))\n","\n","    print(\"\\n--- Processed DataFrame (Info) after fixing infs ---\")\n","    final_processed_FTSE100_df.info()\n","\n","    print(\"\\n--- NaNs in Processed DataFrame (should be 0) ---\")\n","    print(final_processed_FTSE100_df.isnull().sum().sum()) # Should be 0 NaNs if successful\n","else:\n","    print(\"Preprocessing returned an empty DataFrame.\")\n","\n","\n","\n","final_processed_FTSE250_df, fitted_scaler = preprocess_features_for_model(\n","    df_with_all_250_features.copy(), # Pass a copy to avoid modifying the original df_with_all_features\n","    columns_to_exclude_from_features=None, # Use default exclusion (removes original price/volume/weekday)\n","    apply_scaling=True,                     # Apply StandardScaler (best for all 3 models)\n","    scaler_obj=None                         # Fit a new scaler\n",")\n","if not final_processed_FTSE250_df.empty:\n","    print(\"\\n--- Processed DataFrame (Head) after fixing infs ---\")\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    print(final_processed_FTSE100_df.head(15))\n","\n","    print(\"\\n--- Processed DataFrame (Info) after fixing infs ---\")\n","    final_processed_FTSE250_df.info()\n","\n","    print(\"\\n--- NaNs in Processed DataFrame (should be 0) ---\")\n","    print(final_processed_FTSE250_df.isnull().sum().sum()) # Should be 0 NaNs if successful\n","else:\n","    print(\"Preprocessing returned an empty DataFrame.\")"],"metadata":{"id":"W9dD2nPsraNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add"],"metadata":{"id":"4n-YDY4hJBRx","executionInfo":{"status":"ok","timestamp":1750117663857,"user_tz":-60,"elapsed":110,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}},"outputId":"74b670af-cfaf-446d-958d-0c931adbdddc","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: not a git repository (or any of the parent directories): .git\n"]}]}]}