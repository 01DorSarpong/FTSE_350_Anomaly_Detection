{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m7jN0d3qJZZK","outputId":"625d4ba7-2742-45f4-83be-2b1717bbfab7","executionInfo":{"status":"ok","timestamp":1749861117669,"user_tz":-60,"elapsed":17815,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Importing drive model from the google.colab package\n","from google.colab import drive\n","\n","#Mounting the google drive to a specific path\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_8kLZ9NLAgy","outputId":"a9c319a3-5437-4f23-bd22-03537986cfd7","executionInfo":{"status":"ok","timestamp":1749861121171,"user_tz":-60,"elapsed":294,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection\n"]}]},{"cell_type":"code","source":["# Configuring Git user details\n","!git config --global user.email \"dorothy.sarpongk@gmail.com\"\n","!git config --global user.name \"01DorSarpong\""],"metadata":{"id":"sdbbjCzE0tWT","executionInfo":{"status":"ok","timestamp":1749861142526,"user_tz":-60,"elapsed":716,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Importing libraries for code\n","\n","import pandas as pd\n","import numpy  as np\n","import yfinance as yf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from tqdm import tqdm\n"],"metadata":{"id":"jQ_wBKNkmV25","executionInfo":{"status":"ok","timestamp":1749863620149,"user_tz":-60,"elapsed":41,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["def download_and_save_FTSE_stocks(tickers: list, start_date: str, end_date: str, directory: str):\n","  \"\"\"\n","  This function downloads historical stock data for a list of tickers and saves it to CSV files.\n","\n","  Args:\n","    tickers_list (list): A list of stock ticker symbols (e.g., ['TSCO.L', 'BARC.L']).\n","    start_date (str): The start date for data download in 'YYYY-MM-DD' format.\n","    end_date (str): The end date for data download in 'YYYY-MM-DD' format.\n","    directory (str): The path to the directory where CSV files will be saved.\n","  \"\"\"\n","\n","  # Ensure the save directory exists\n","  if not os.path.exists(directory):\n","    os.makedirs(directory)\n","    print(f\"Created directory: {directory}\")\n","\n","  print(f\"Starting download for {len(tickers)} tickers from {start_date} to {end_date}...\")\n","\n","  for ticker in tqdm(tickers, desc=\"Downloading Stocks\"):\n","    # Format the filename: remove '.L' and add date range for clarity\n","    cleaned_ticker = ticker.replace('.L', '')\n","    file_name = f\"{cleaned_ticker}_{start_date.replace('-', '')}_{end_date.replace('-', '')}.csv\"\n","    full_file_path = os.path.join(directory, file_name)\n","\n","    try:\n","      df = yf.download(ticker, start=start_date, end=end_date, auto_adjust=False)\n","\n","      if not df.empty:\n","        df.to_csv(full_file_path)\n","        # print(f\"✅ Saved data for {ticker} to {full_file_path}\") # Optional: uncomment for more verbose output\n","      else:\n","        print(f\"⚠️ No data available for {ticker} for the specified period.\")\n","    except Exception as e:\n","      print(f\"❌ Error downloading or saving data for {ticker}: {e}\")\n","\n","  print(\"Download process completed.\")\n"],"metadata":{"id":"6Iby76j7PtZR","executionInfo":{"status":"ok","timestamp":1749863073941,"user_tz":-60,"elapsed":11,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["#Creating a list of FTSE 100 and FTSE 250 tickers\n","\n","FTSE_100_tickers = [\"AZN.L\", \"HSBA.L\", \"ULVR.L\", \"REL.L\", \"BATS.L\", \"BP.L\", \"GSK.L\", \"DGE.L\",\n","                   \"RR.L\", \"NG.L\", \"BARC.L\", \"TSCO.L\", \"PRU.L\", \"BHP.L\", \"BT-A.L\",]\n","\n","FTSE_250_tickers = [\"BWY.L\", \"EMG.L\", \"JUST.L\", \"SXS.L\", \"CKN.L\", \"LRE.L\", \"RAT.L\", \"THG.L\",\n","                    \"JDW.L\", \"SCT.L\", \"DOM.L\", \"SRE.L\", \"HIK.L\", \"ICGT.L\", \"HSX.L\"]"],"metadata":{"id":"2L4EdTEsuCu8","executionInfo":{"status":"ok","timestamp":1749863080944,"user_tz":-60,"elapsed":39,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["#Defining the period for stocks range\n","start_date = \"2014-01-01\"\n","end_date = \"2024-12-31\""],"metadata":{"id":"yESfV6rzUuQ1","executionInfo":{"status":"ok","timestamp":1749863083859,"user_tz":-60,"elapsed":4,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# Defining the path to save the CSVs\n","\n","ftse_100_path = '/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection/FTSE_100'\n","ftse_250_path = '/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection/FTSE_250'\n"],"metadata":{"id":"w9mFm0wLVH6v","executionInfo":{"status":"ok","timestamp":1749863088893,"user_tz":-60,"elapsed":9,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# Calling the function for FTSE 100 and FTSE 250 tickers\n","\n","download_and_save_FTSE_stocks(\n","    tickers=FTSE_100_tickers,\n","    start_date=start_date,\n","    end_date=end_date,\n","    directory=ftse_100_path\n",")\n","\n","download_and_save_FTSE_stocks(\n","    tickers=FTSE_250_tickers,\n","    start_date=start_date,\n","    end_date=end_date,\n","    directory=ftse_250_path\n",")"],"metadata":{"id":"dnZIuB7oXMSA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749863111009,"user_tz":-60,"elapsed":15539,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}},"outputId":"49d0aa1f-6150-4b46-b03a-20a7a46d04c3"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting download for 15 tickers from 2014-01-01 to 2024-12-31...\n"]},{"output_type":"stream","name":"stderr","text":["[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","Downloading Stocks: 100%|██████████| 15/15 [00:08<00:00,  1.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Download process completed.\n","Starting download for 15 tickers from 2014-01-01 to 2024-12-31...\n"]},{"output_type":"stream","name":"stderr","text":["[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","Downloading Stocks: 100%|██████████| 15/15 [00:06<00:00,  2.15it/s]"]},{"output_type":"stream","name":"stdout","text":["Download process completed.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# Creating a function to load stocks and pre-process into a dataframe\n","\n","def load_and_structure_stock_data(folder_path: str) -> pd.DataFrame:\n","    \"\"\"\n","    Loads historical stock data from CSV files in a specified folder,\n","    cleans and processes each DataFrame, and consolidates them into\n","    a single structured DataFrame with a MultiIndex (Date, Ticker).\n","\n","    Args:\n","        folder_path (str): The path to the directory containing stock data CSVs.\n","\n","    Returns:\n","        pd.DataFrame: A single DataFrame containing data for all tickers,\n","                      indexed by 'Date' and 'Ticker', sorted by Date and then Ticker.\n","                      Returns an empty DataFrame if no data is loaded.\n","    \"\"\"\n","    all_dfs = []\n","\n","    if not os.path.exists(folder_path):\n","        print(f\"❌ Error: Folder not found at {folder_path}\")\n","        return pd.DataFrame() # Return empty DataFrame if folder doesn't exist\n","\n","    # Use os.scandir for potentially better performance with many files\n","    # and tqdm for a progress bar if there are many files.\n","    file_list = [f.name for f in os.scandir(folder_path) if f.name.endswith(\".csv\")]\n","\n","    if not file_list:\n","        print(f\"⚠️ No CSV files found in {folder_path}\")\n","        return pd.DataFrame()\n","\n","    print(f\"Loading data from {len(file_list)} CSV files in {folder_path}...\")\n","\n","    for filename in tqdm(file_list, desc=\"Processing Stock Files\"):\n","        file_path = os.path.join(folder_path, filename)\n","\n","        try:\n","            # Read CSV: use first row as header, skip second row (which often contains ticker name repeated)\n","            # The original yfinance CSV header is often 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'\n","            # and sometimes it puts the ticker symbol in the second row for some tools/configs,\n","            # so skiprows=[1] is a good precaution if that's how yfinance saves it for you.\n","            df = pd.read_csv(file_path, header=0, skiprows=[1], encoding='utf-8-sig')\n","\n","            # --- Data Cleaning and Preparation ---\n","\n","            # Rename the first column to 'Date' if it's not already, and handle potential variants\n","            if df.columns[0].lower().strip() != 'date':\n","                df.rename(columns={df.columns[0]: 'Date'}, inplace=True)\n","\n","            # Normalize column names to lowercase and remove leading/trailing spaces\n","            df.columns = [col.strip().lower() for col in df.columns]\n","\n","            # Clean and prepare the 'date' column: ensure string, strip spaces, drop 'Date' literals\n","            df['date'] = df['date'].astype(str).str.strip()\n","            df = df[df['date'].str.lower() != 'date'] # Drop any rows where the date column contains the string 'Date'\n","\n","            # Convert to datetime, coercing errors will turn unparseable dates into NaT\n","            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n","            df.dropna(subset=['date'], inplace=True) # Drop rows where date conversion failed (NaT)\n","\n","            # Set datetime index\n","            df.set_index('date', inplace=True)\n","\n","            # Drop rows missing essential data (e.g., open price). Assuming 'open' is critical.\n","            if 'open' in df.columns:\n","                df.dropna(subset=['open'], inplace=True)\n","            else:\n","                print(f\"⚠️ 'open' column not found in {filename}. Skipping this file.\")\n","                continue  # Skip this file if 'open' is missing\n","\n","            # --- Add Ticker Information ---\n","            # Extract ticker from filename (assuming format like TICKER_STARTDATE_ENDDATE.csv)\n","            # Example: 'TSCO_20140101_20241231.csv' -> 'TSCO'\n","            ticker_symbol = filename.split('_')[0]\n","            df['ticker'] = ticker_symbol # Add a new 'ticker' column to this DataFrame\n","\n","            all_dfs.append(df)\n","\n","        except pd.errors.EmptyDataError:\n","            print(f\"⚠️ {filename} is empty. Skipping.\")\n","        except pd.errors.ParserError as e:\n","            print(f\"❌ Error parsing {filename}: {type(e).__name__} - {e}. Skipping.\")\n","        except Exception as e:\n","            print(f\"❌ Error processing {filename}: {type(e).__name__} - {e}. Skipping.\")\n","\n","    if not all_dfs:\n","        print(\"No valid stock data loaded.\")\n","        return pd.DataFrame()\n","\n","    # Concatenate all individual DataFrames into one\n","    combined_df = pd.concat(all_dfs)\n","\n","    # Set a MultiIndex: primary index is 'date', secondary is 'ticker'\n","    # Ensure 'ticker' is part of the index for efficient slicing and grouping\n","    combined_df.set_index('ticker', append=True, inplace=True)\n","    combined_df.index.names = ['Date', 'Ticker']\n","\n","    # Sort the MultiIndex for better performance and consistency\n","    combined_df.sort_index(inplace=True)\n","\n","    print(\"All stock data loaded and structured successfully.\")\n","    return combined_df"],"metadata":{"id":"0vnfndB7yYQ7","executionInfo":{"status":"ok","timestamp":1749863857260,"user_tz":-60,"elapsed":47,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["#Calling the function\n","\n","structured_ftse_data = load_and_structure_stock_data(ftse_100_path)\n","\n","# Display the first few rows and info of the combined DataFrame\n","if not structured_ftse_data.empty:\n","  print(\"\\nCombined DataFrame Head:\")\n","  print(structured_ftse_data.head())\n","  print(\"\\nCombined DataFrame Info:\")\n","  structured_ftse_data.info()\n","  print(\"\\nNumber of unique tickers:\", structured_ftse_data.index.get_level_values('Ticker').nunique())"],"metadata":{"id":"XktAAHheAscj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add .\n","!git commit -m \"Added a new function to load and process stocks into a dataframe\"\n","!git push origin main\n"],"metadata":{"id":"CX47oDUYDIPX"},"execution_count":null,"outputs":[]}]}