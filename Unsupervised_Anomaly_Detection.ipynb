{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m7jN0d3qJZZK","outputId":"4ce1c5b0-903b-4978-c4a9-92b8678f5848","executionInfo":{"status":"ok","timestamp":1750196929229,"user_tz":-60,"elapsed":28229,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Importing drive model from the google.colab package\n","from google.colab import drive\n","\n","#Mounting the google drive to a specific path\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_8kLZ9NLAgy","outputId":"67e35ab1-1586-4278-bbc7-063febf463ff","executionInfo":{"status":"ok","timestamp":1750196943034,"user_tz":-60,"elapsed":421,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection\n"]}]},{"cell_type":"code","source":["# Configuring Git user details\n","!git config --global user.email \"dorothy.sarpongk@gmail.com\"\n","!git config --global user.name \"01DorSarpong\""],"metadata":{"id":"sdbbjCzE0tWT","executionInfo":{"status":"ok","timestamp":1750196951015,"user_tz":-60,"elapsed":1015,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Importing libraries for code\n","\n","import pandas as pd\n","import numpy  as np\n","import yfinance as yf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import IsolationForest\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from tqdm import tqdm\n","from typing import Tuple, Union\n"],"metadata":{"id":"jQ_wBKNkmV25","executionInfo":{"status":"ok","timestamp":1750208379074,"user_tz":-60,"elapsed":425,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# A function to download and save FTSE 100 and FTSE 250 stocks\n","\n","def download_and_save_FTSE_stocks(tickers: list, start_date: str, end_date: str, directory: str):\n","  \"\"\"\n","  This function downloads historical stock data for a list of tickers and saves it to CSV files.\n","\n","  Args:\n","    tickers_list (list): A list of stock ticker symbols (e.g., ['TSCO.L', 'BARC.L']).\n","    start_date (str): The start date for data download in 'YYYY-MM-DD' format.\n","    end_date (str): The end date for data download in 'YYYY-MM-DD' format.\n","    directory (str): The path to the directory where CSV files will be saved.\n","  \"\"\"\n","\n","  # Ensure the save directory exists\n","  if not os.path.exists(directory):\n","    os.makedirs(directory)\n","    print(f\"Created directory: {directory}\")\n","\n","  print(f\"Starting download for {len(tickers)} tickers from {start_date} to {end_date}...\")\n","\n","  for ticker in tqdm(tickers, desc=\"Downloading Stocks\"):\n","    # Format the filename: remove '.L' and add date range for clarity\n","    cleaned_ticker = ticker.replace('.L', '')\n","    file_name = f\"{cleaned_ticker}_{start_date.replace('-', '')}_{end_date.replace('-', '')}.csv\"\n","    full_file_path = os.path.join(directory, file_name)\n","\n","    try:\n","      df = yf.download(ticker, start=start_date, end=end_date, auto_adjust=False)\n","\n","      if not df.empty:\n","        df.to_csv(full_file_path)\n","        # print(f\"✅ Saved data for {ticker} to {full_file_path}\") # Optional: uncomment for more verbose output\n","      else:\n","        print(f\"⚠️ No data available for {ticker} for the specified period.\")\n","    except Exception as e:\n","      print(f\"❌ Error downloading or saving data for {ticker}: {e}\")\n","\n","  print(\"Download process completed.\")\n"],"metadata":{"id":"6Iby76j7PtZR","executionInfo":{"status":"ok","timestamp":1750115644198,"user_tz":-60,"elapsed":3,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#Creating a list of FTSE 100 and FTSE 250 tickers\n","\n","FTSE_100_tickers = [\"AZN.L\", \"HSBA.L\", \"ULVR.L\", \"REL.L\", \"BATS.L\", \"BP.L\", \"GSK.L\", \"DGE.L\",\n","                   \"RR.L\", \"NG.L\", \"BARC.L\", \"TSCO.L\", \"PRU.L\", \"BHP.L\", \"BT-A.L\",]\n","\n","FTSE_250_tickers = [\"BWY.L\", \"EMG.L\", \"JUST.L\", \"SXS.L\", \"CKN.L\", \"LRE.L\", \"RAT.L\", \"THG.L\",\n","                    \"JDW.L\", \"SCT.L\", \"DOM.L\", \"SRE.L\", \"HIK.L\", \"ICGT.L\", \"HSX.L\"]"],"metadata":{"id":"2L4EdTEsuCu8","executionInfo":{"status":"ok","timestamp":1750197324979,"user_tz":-60,"elapsed":10,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#Defining the period for stocks range\n","start_date = \"2014-01-01\"\n","end_date = \"2024-12-31\""],"metadata":{"id":"yESfV6rzUuQ1","executionInfo":{"status":"ok","timestamp":1750197326598,"user_tz":-60,"elapsed":4,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Defining the path to save the CSVs\n","\n","ftse_100_path = '/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection/FTSE_100'\n","ftse_250_path = '/content/drive/MyDrive/GitHub_Projects/FTSE_350_Anomaly_Detection/FTSE_250'\n"],"metadata":{"id":"w9mFm0wLVH6v","executionInfo":{"status":"ok","timestamp":1750197329927,"user_tz":-60,"elapsed":4,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Calling the function for FTSE 100 and FTSE 250 tickers\n","\n","download_and_save_FTSE_stocks(\n","    tickers=FTSE_100_tickers,\n","    start_date=start_date,\n","    end_date=end_date,\n","    directory=ftse_100_path\n",")\n","\n","download_and_save_FTSE_stocks(\n","    tickers=FTSE_250_tickers,\n","    start_date=start_date,\n","    end_date=end_date,\n","    directory=ftse_250_path\n",")"],"metadata":{"id":"dnZIuB7oXMSA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a function to load stocks and pre-process into a dataframe\n","\n","def load_and_structure_stock_data(folder_path: str) -> pd.DataFrame:\n","    \"\"\"\n","    Loads historical stock data from CSV files in a specified folder,\n","    cleans, processes, filters to weekdays, fills NaNs/gaps, and\n","    consolidates them into a single structured DataFrame with a MultiIndex.\n","\n","    Args:\n","        folder_path (str): The path to the directory containing stock data CSVs.\n","\n","    Returns:\n","        pd.DataFrame: A single DataFrame containing data for all tickers,\n","                      indexed by 'Date' and 'Ticker', sorted by Date and then Ticker.\n","                      Returns an empty DataFrame if no data is loaded or processed.\n","    \"\"\"\n","    all_dfs = []\n","\n","    if not os.path.exists(folder_path):\n","        print(f\"❌ Error: Folder not found at {folder_path}\")\n","        return pd.DataFrame() # Return empty DataFrame if folder doesn't exist\n","\n","    # Get list of CSV files to process\n","    file_list = [f.name for f in os.scandir(folder_path) if f.name.endswith(\".csv\")]\n","\n","    if not file_list:\n","        print(f\"⚠️ No CSV files found in {folder_path}\")\n","        return pd.DataFrame()\n","\n","    print(f\"Loading and processing data from {len(file_list)} CSV files in {folder_path}...\")\n","\n","    # Define columns that typically contain numerical stock data to be filled\n","    numerical_cols_to_fill = ['open', 'high', 'low', 'close', 'adj close', 'volume']\n","\n","    for filename in tqdm(file_list, desc=\"Processing Stock Files\"):\n","        file_path = os.path.join(folder_path, filename)\n","\n","        try:\n","            # Read CSV: use first row as header, skip second row (often contains ticker name repeated)\n","            df = pd.read_csv(file_path, header=0, skiprows=[1], encoding='utf-8-sig')\n","\n","            # --- Initial Cleaning and Date Conversion ---\n","            # Rename the first column to 'Date' if it's not already\n","            if df.columns[0].strip().lower() != 'date':\n","                df.rename(columns={df.columns[0]: 'Date'}, inplace=True)\n","\n","            # Normalize all column names to lowercase and remove leading/trailing spaces\n","            df.columns = [col.strip().lower() for col in df.columns]\n","\n","            # Clean and prepare the 'date' column\n","            df['date'] = df['date'].astype(str).str.strip()\n","            df = df[df['date'].str.lower() != 'date'] # Drop any rows where the date column contains the string 'Date'\n","\n","            # Convert to datetime, coercing errors will turn unparseable dates into NaT\n","            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n","            df.dropna(subset=['date'], inplace=True) # Drop rows where date conversion failed (NaT)\n","\n","            # --- Add Ticker Information ---\n","            # Extract ticker from filename (e.g., 'TSCO_20140101_20241231.csv' -> 'TSCO')\n","            ticker_symbol = filename.split('_')[0]\n","            df['ticker'] = ticker_symbol\n","\n","            # --- Set Date as Index (temporarily for filtering/filling) ---\n","            df.set_index('date', inplace=True)\n","\n","            # --- Filter for Weekdays Only ---\n","            # .dayofweek returns Monday=0, ..., Sunday=6. Keep only 0 to 4.\n","            df = df[df.index.dayofweek < 5]\n","\n","            # --- Fill NaNs/Gaps for Numerical Columns ---\n","            # Identify columns to fill that actually exist in the current DataFrame\n","            existing_numerical_cols = [col for col in numerical_cols_to_fill if col in df.columns]\n","\n","            if not existing_numerical_cols:\n","                # print(f\"⚠️ No numerical columns found for {filename}. Skipping NaN filling for this file.\")\n","                pass # This is fine, just means the file might only have non-numerical data or very specific columns\n","\n","            # Apply forward-fill then backward-fill for numerical columns within this ticker's data\n","            # This handles gaps for individual stock series\n","            df[existing_numerical_cols] = df[existing_numerical_cols].ffill().bfill()\n","\n","            # --- Final Check for essential data after filling ---\n","            # If 'open' column is vital and still has NaNs (e.g., entire series was NaN), drop those rows\n","            if 'open' in df.columns:\n","                df.dropna(subset=['open'], inplace=True)\n","            else:\n","                print(f\"⚠️ 'open' column not found in {filename}. Skipping this file as essential data is missing.\")\n","                continue # Skip this file if 'open' is genuinely missing\n","\n","            if df.empty:\n","                print(f\"⚠️ No valid weekday data remaining for {filename} after filtering. Skipping.\")\n","                continue\n","\n","            all_dfs.append(df)\n","\n","        except pd.errors.EmptyDataError:\n","            print(f\"⚠️ {filename} is empty. Skipping.\")\n","        except pd.errors.ParserError as e:\n","            print(f\"❌ Error parsing {filename}: {type(e).__name__} - {e}. Skipping.\")\n","        except Exception as e:\n","            print(f\"❌ Error processing {filename}: {type(e).__name__} - {e}. Skipping.\")\n","\n","    if not all_dfs:\n","        print(\"No valid stock data loaded after processing. Returning empty DataFrame.\")\n","        return pd.DataFrame()\n","\n","    # --- Consolidate all DataFrames into one structured DataFrame ---\n","    combined_df = pd.concat(all_dfs)\n","\n","    # Set a MultiIndex: primary index is 'Date', secondary is 'ticker'\n","    # 'ticker' was added as a regular column inside the loop, now it becomes part of the index\n","    combined_df.set_index('ticker', append=True, inplace=True)\n","    combined_df.index.names = ['Date', 'Ticker']\n","\n","    # Sort the MultiIndex for better performance and consistency\n","    combined_df.sort_index(inplace=True)\n","\n","    print(\"All stock data loaded, structured, filtered, and filled successfully.\")\n","    return combined_df\n","\n"],"metadata":{"id":"oQt_Z87EH2L9","executionInfo":{"status":"ok","timestamp":1750197482200,"user_tz":-60,"elapsed":13,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#Calling the function to load and structure df for FTSE 100 stocks\n","\n","ready_ftse100_data = load_and_structure_stock_data(ftse_100_path)\n","\n","if not ready_ftse100_data.empty:\n","  print(\"\\n--- Final Structured and Cleaned DataFrame ---\")\n","  print(ready_ftse100_data.head(15)) # Show more rows to see multiple dates/tickers\n","  print(\"\\nDataFrame Info:\")\n","  ready_ftse100_data.info()\n","  print(\"\\nSample of weekdays (should only be Mon-Fri):\")\n","  print(ready_ftse100_data.index.get_level_values('Date').day_name().value_counts())\n","  print(\"\\nNaNs after processing (should be very few or none in numerical columns):\")\n","  print(ready_ftse100_data.isnull().sum())\n","else:\n"," print(\"No data was successfully loaded and processed.\")"],"metadata":{"id":"gK9gOL3vIK_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calling the function to load and structure df for FTSE 250 stocks\n","\n","ready_ftse250_data = load_and_structure_stock_data(ftse_250_path)\n","\n","if not ready_ftse250_data.empty:\n","  print(\"\\n--- Final Structured and Cleaned DataFrame ---\")\n","  print(ready_ftse250_data.head(15)) # Show more rows to see multiple dates/tickers\n","  print(\"\\nDataFrame Info:\")\n","  ready_ftse250_data.info()\n","  print(\"\\nSample of weekdays (should only be Mon-Fri):\")\n","  print(ready_ftse250_data.index.get_level_values('Date').day_name().value_counts())\n","  print(\"\\nNaNs after processing (should be very few or none in numerical columns):\")\n","  print(ready_ftse250_data.isnull().sum())\n","else:\n"," print(\"No data was successfully loaded and processed.\")"],"metadata":{"id":"XktAAHheAscj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to genereate the stock features needed to build ML model\n","\n","def generate_all_stock_features(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Generates a comprehensive set of time-series and cross-sectional numerical features\n","    for stock data, suitable for unsupervised anomaly detection models.\n","\n","    This function assumes the input DataFrame has a MultiIndex (Date, Ticker)\n","    and contains cleaned base columns like 'open', 'high', 'low', 'close',\n","    'adj close', and 'volume', with no critical NaNs.\n","\n","    Args:\n","        df (pd.DataFrame): Your clean DataFrame with MultiIndex (Date, Ticker)\n","                           and base stock price/volume data.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with all original and newly engineered numerical\n","                      features. NaNs introduced by calculations (e.g., at the start\n","                      of rolling windows) will be present.\n","    \"\"\"\n","    if df.empty:\n","        print(\"Input DataFrame is empty for feature generation. Returning empty DataFrame.\")\n","        return pd.DataFrame()\n","\n","    processed_df = df.copy() # Always work on a copy to keep the original untouched\n","\n","    # --- Ensure critical columns are numerical for calculations ---\n","    for col in ['open', 'high', 'low', 'close', 'adj close', 'volume']:\n","        if col in processed_df.columns:\n","            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')\n","    processed_df.dropna(subset=['open','close','volume'], inplace=True)\n","    if processed_df.empty:\n","        print(\"No data remaining after ensuring essential columns are numerical and not NaN.\")\n","        return pd.DataFrame()\n","\n","\n","    grouped_by_ticker = processed_df.groupby(level='Ticker')\n","    grouped_by_date = processed_df.groupby(level='Date')\n","    epsilon = 1e-9\n","\n","    print(\"Generating Time-Series Based Features (per stock ticker)...\")\n","\n","    # 1. Return-Based Features:\n","    processed_df['log_return'] = grouped_by_ticker['close'].transform(\n","        lambda x: np.log(x / x.shift(1).replace(0, epsilon)).replace([-np.inf, np.inf], np.nan)\n","    )\n","    processed_df['simple_return'] = grouped_by_ticker['close'].transform(\n","        lambda x: (x / x.shift(1).replace(0, epsilon)) - 1\n","    )\n","    if 'adj close' in processed_df.columns:\n","        processed_df['log_adj_close_return'] = grouped_by_ticker['adj close'].transform(\n","            lambda x: np.log(x / x.shift(1).replace(0, epsilon)).replace([-np.inf, np.inf], np.nan)\n","        )\n","    processed_df['return_5d'] = grouped_by_ticker['close'].transform(lambda x: x.pct_change(periods=5))\n","    processed_df['return_20d'] = grouped_by_ticker['close'].transform(lambda x: x.pct_change(periods=20))\n","\n","    print(\"Generating Volatility Measures...\")\n","    # 2. Volatility Measures:\n","    processed_df['rolling_std_5d_log_return'] = grouped_by_ticker['log_return'].transform(lambda x: x.rolling(window=5).std())\n","    processed_df['rolling_std_20d_log_return'] = grouped_by_ticker['log_return'].transform(lambda x: x.rolling(window=20).std())\n","\n","    if 'high' in processed_df.columns and 'low' in processed_df.columns:\n","        processed_df['daily_range_norm'] = (processed_df['high'] - processed_df['low']) / (processed_df['close'] + epsilon)\n","\n","        log_high_div_low = np.log((processed_df['high'] / processed_df['low'].replace(0, epsilon)).clip(lower=epsilon))\n","        log_close_div_open = np.log((processed_df['close'] / processed_df['open'].replace(0, epsilon)).clip(lower=epsilon))\n","\n","        gk_term = 0.5 * (log_high_div_low)**2 - (2 * np.log(2) - 1) * (log_close_div_open)**2\n","        gk_term[gk_term < 0] = np.nan # Set negative values before sqrt to NaN\n","        processed_df['garman_klass_vol'] = np.sqrt(gk_term)\n","\n","        # Explicitly replace infs/NaNs that might result from sqrt or division by zero, and fill\n","        processed_df['garman_klass_vol'] = processed_df['garman_klass_vol'].replace([-np.inf, np.inf], np.nan)\n","        processed_df['garman_klass_vol'].fillna(0, inplace=True) # Fill with 0 for Garman-Klass specific NaNs\n","    else:\n","        print(\"⚠️ Missing 'high' or 'low' columns for some volatility features. Skipping.\")\n","\n","    print(\"Generating Volume-Based Features...\")\n","    # 3. Volume-Based Features:\n","    processed_df['volume_change'] = grouped_by_ticker['volume'].transform(lambda x: x.pct_change(periods=1))\n","    processed_df['avg_volume_20d'] = grouped_by_ticker['volume'].transform(lambda x: x.rolling(window=20).mean())\n","    processed_df['relative_volume'] = processed_df['volume'] / (processed_df['avg_volume_20d'] + epsilon)\n","\n","    print(\"Generating Momentum/Trend Indicators...\")\n","    # 4. Momentum/Trend Indicators:\n","    processed_df['sma_5d'] = grouped_by_ticker['close'].transform(lambda x: x.rolling(window=5).mean())\n","    processed_df['sma_20d'] = grouped_by_ticker['close'].transform(lambda x: x.rolling(window=20).mean())\n","    processed_df['deviation_from_sma_20d'] = (processed_df['close'] - processed_df['sma_20d']) / (processed_df['sma_20d'] + epsilon)\n","\n","    print(\"Generating Price-Volume Interaction Features...\")\n","    # 5. Price-Volume Interaction Features:\n","    if 'high' in processed_df.columns and 'low' in processed_df.columns:\n","        processed_df['typical_price'] = (processed_df['high'] + processed_df['low'] + processed_df['close']) / 3\n","    else:\n","        print(\"⚠️ Missing 'high' or 'low' columns. Skipping 'typical_price'.\")\n","\n","    print(\"Generating Cross-Sectional Features (comparing stocks on the same day)...\")\n","    # 6. Cross-Sectional Features:\n","    if 'log_return' in processed_df.columns:\n","        processed_df['daily_market_mean_log_return'] = grouped_by_date['log_return'].transform('mean')\n","        processed_df['daily_market_median_log_return'] = grouped_by_date['log_return'].transform('median')\n","\n","        processed_df['deviation_from_daily_mean_return'] = processed_df['log_return'] - processed_df['daily_market_mean_log_return']\n","        processed_df['deviation_from_daily_median_return'] = processed_df['log_return'] - processed_df['daily_market_median_log_return']\n","        processed_df['daily_return_rank_pct'] = grouped_by_date['log_return'].rank(pct=True, method='average')\n","    else:\n","        print(\"⚠️ 'log_return' column not available for cross-sectional feature generation. Skipping these features.\")\n","\n","    # --- FINAL CLEANUP: Replace any remaining inf/-inf with NaN across all numerical columns ---\n","    print(\"Finalizing features: cleaning up any remaining inf/-inf values...\")\n","    numerical_cols_after_gen = processed_df.select_dtypes(include=np.number).columns\n","    for col in numerical_cols_after_gen:\n","        processed_df[col] = processed_df[col].replace([np.inf, -np.inf], np.nan)\n","\n","    print(\"Feature generation complete. NaNs from calculations are present and will need further handling.\")\n","    return processed_df.sort_index()"],"metadata":{"id":"CX47oDUYDIPX","executionInfo":{"status":"ok","timestamp":1750197571750,"user_tz":-60,"elapsed":10,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["\n","# Call the feature generation function on the FTSE 100 stock\n","\n","df_with_all_100_features = generate_all_stock_features(ready_ftse100_data)\n","\n","if not df_with_all_100_features.empty:\n","    print(\"\\n--- NEWLY GENERATED DataFrame with ALL Engineered Features (Head) ---\")\n","    # To ensure you see all columns, even if there are many:\n","    pd.set_option('display.max_columns', None) # Display all columns\n","    pd.set_option('display.width', 1000)      # Ensure wide display in terminal/Colab output\n","\n","    # Now, print the head of the *NEW* DataFrame\n","    print(df_with_all_100_features.head(15))\n","\n","    print(\"\\n--- NEWLY GENERATED DataFrame Info (should show many more columns) ---\")\n","    # Now, print the info of the *NEW* DataFrame\n","    df_with_all_100_features.info()\n","\n","    print(\"\\n--- ALL Column Names in the NEW DataFrame ---\")\n","    # This will explicitly list ALL column names, proving they are there\n","    print(df_with_all_100_features.columns.tolist())\n","\n","    print(\"\\n--- Count of NaNs per column (expect some NaNs from rolling/shifting, esp. at start of series) ---\")\n","    print(df_with_all_100_features.isnull().sum().sort_values(ascending=False).head(20))\n","else:\n","    print(\"The feature generation function returned an empty DataFrame.\")"],"metadata":{"id":"GNB55xrl1RAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Call the feature generation function on the FTSE 100 stock\n","\n","df_with_all_250_features = generate_all_stock_features(ready_ftse250_data)\n","\n","if not df_with_all_250_features.empty:\n","    print(\"\\n--- NEWLY GENERATED DataFrame with ALL Engineered Features (Head) ---\")\n","    # To ensure you see all columns, even if there are many:\n","    pd.set_option('display.max_columns', None) # Display all columns\n","    pd.set_option('display.width', 1000)      # Ensure wide display in terminal/Colab output\n","\n","    # Now, print the head of the *NEW* DataFrame\n","    print(df_with_all_250_features.head(15))\n","\n","    print(\"\\n--- NEWLY GENERATED DataFrame Info (should show many more columns) ---\")\n","    # Now, print the info of the *NEW* DataFrame\n","    df_with_all_250_features.info()\n","\n","    print(\"\\n--- ALL Column Names in the NEW DataFrame ---\")\n","    # This will explicitly list ALL column names, proving they are there\n","    print(df_with_all_250_features.columns.tolist())\n","\n","    print(\"\\n--- Count of NaNs per column (expect some NaNs from rolling/shifting, esp. at start of series) ---\")\n","    print(df_with_all_250_features.isnull().sum().sort_values(ascending=False).head(20))\n","else:\n","    print(\"The feature generation function returned an empty DataFrame.\")"],"metadata":{"id":"pkgU-aLTsJr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function to remove all NaNs from features and scales values using standard scaler\n","#from sklearn.preprocessing import StandardScaler\n","\n","def preprocess_features_for_model(\n","    df_with_all_features: pd.DataFrame,\n","    columns_to_exclude_from_features: list = None,\n","    apply_scaling: bool = True,\n","    scaler_obj: StandardScaler = None # Optional: provide a pre-fitted scaler for consistent scaling\n",") -> Tuple[pd.DataFrame, Union[StandardScaler, None]]:\n","    \"\"\"\n","    Handles NaN removal, feature selection, and feature scaling for a DataFrame\n","    containing engineered stock features. This prepares the data for anomaly\n","    detection models.\n","\n","    Args:\n","        df_with_engineered_features (pd.DataFrame): The DataFrame containing\n","                                                    all engineered features, with a MultiIndex.\n","                                                    (Output of `generate_all_stock_features`).\n","        columns_to_exclude_from_features (list, optional): A list of column names\n","                                                        that should NOT be treated as features\n","                                                        for the model (e.g., raw 'open', 'close',\n","                                                        or non-numeric helper columns like 'weekday').\n","                                                        If None, a default list is used.\n","        apply_scaling (bool): Whether to apply StandardScaler to numerical features.\n","                              Defaults to True. Highly recommended for most ML models.\n","        scaler_obj (StandardScaler, optional): An pre-fitted StandardScaler object.\n","                                               If `apply_scaling` is True and `scaler_obj` is None,\n","                                               a new scaler will be fitted. Useful for consistent\n","                                               scaling between train/test datasets.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - pd.DataFrame: The DataFrame with selected, cleaned, and optionally scaled\n","                            numerical features, ready for an anomaly detection model.\n","                            Retains the MultiIndex.\n","            - StandardScaler or None: The fitted or used StandardScaler object if\n","                                      scaling was applied, else None.\n","    \"\"\"\n","    if df_with_all_features.empty:\n","        print(\"Input DataFrame is empty for preprocessing. Returning empty DataFrame.\")\n","        return pd.DataFrame(), None\n","\n","    processed_df = df_with_all_features.copy()\n","    print(\"\\n--- Starting Feature Preprocessing for Model ---\")\n","\n","    # --- 1. Feature Selection (Identify Numerical Features to Use) ---\n","    # Default list of columns that are typically not features, but base data or helpers\n","    if columns_to_exclude_from_features is None:\n","        columns_to_exclude_from_features = ['open', 'high', 'low', 'close', 'adj close', 'volume', 'weekday']\n","\n","    # Get all numerical columns from the DataFrame\n","    all_numerical_cols = processed_df.select_dtypes(include=np.number).columns.tolist()\n","\n","    # Filter out the columns that should be excluded\n","    features_for_model_names = [\n","        col for col in all_numerical_cols\n","        if col not in columns_to_exclude_from_features\n","    ]\n","\n","    if not features_for_model_names:\n","        print(\"⚠️ No valid numerical features identified after exclusion. Using all numerical original columns.\")\n","        features_for_model_names = all_numerical_cols # Fallback to all if custom exclusion leads to empty list\n","\n","    print(f\"Selected {len(features_for_model_names)} features for the model.\")\n","    df_features_only = processed_df[features_for_model_names].copy()\n","\n","\n","    # --- 2. NaN Removal (Final Handling for Model Input) ---\n","    # Drop rows where any of the *selected features* have NaNs.\n","    # This is critical as most ML models cannot handle NaNs.\n","    original_rows_count = df_features_only.shape[0]\n","    df_features_only.dropna(inplace=True)\n","    rows_after_nan_drop = df_features_only.shape[0]\n","\n","    if original_rows_count > rows_after_nan_drop:\n","        print(f\"Dropped {original_rows_count - rows_after_nan_drop} rows due to NaNs in selected features.\")\n","    if df_features_only.empty:\n","        print(\"DataFrame is empty after NaN removal. Cannot proceed with preprocessing.\")\n","        return pd.DataFrame(), None\n","    print(f\"Data shape after NaN removal: {df_features_only.shape}\")\n","\n","\n","    # --- 3. Feature Scaling ---\n","    scaler = None\n","    if apply_scaling:\n","        print(\"Applying StandardScaler to features...\")\n","        scaler = scaler_obj if scaler_obj is not None else StandardScaler()\n","\n","        # Fit and/or transform the features\n","        X_scaled = scaler.fit_transform(df_features_only) if scaler_obj is None else scaler.transform(df_features_only)\n","\n","        # Convert scaled array back to DataFrame, retaining index and column names\n","        df_scaled_features = pd.DataFrame(X_scaled, index=df_features_only.index, columns=df_features_only.columns)\n","        print(f\"Features scaled. Scaler: {'New' if scaler_obj is None else 'Existing'}.\")\n","    else:\n","        df_scaled_features = df_features_only.copy()\n","        print(\"Skipping feature scaling.\")\n","\n","    print(\"--- Feature Preprocessing for Model Complete ---\")\n","    return df_scaled_features, scaler"],"metadata":{"id":"IULteCpM6AMz","executionInfo":{"status":"ok","timestamp":1750197722161,"user_tz":-60,"elapsed":8,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#Calling the preprocess_features_for_model function on the FTSE 100 and FTSE 250 stocks\n","\n","final_processed_FTSE100_df, fitted_scaler = preprocess_features_for_model(\n","    df_with_all_100_features.copy(), # Pass a copy to avoid modifying the original df_with_all_features\n","    columns_to_exclude_from_features=None, # Use default exclusion (removes original price/volume/weekday)\n","    apply_scaling=True,                     # Apply StandardScaler (best for all 3 models)\n","    scaler_obj=None                         # Fit a new scaler\n",")\n","if not final_processed_FTSE100_df.empty:\n","    print(\"\\n--- Processed DataFrame (Head) after fixing infs ---\")\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    print(final_processed_FTSE100_df.head(15))\n","\n","    print(\"\\n--- Processed DataFrame (Info) after fixing infs ---\")\n","    final_processed_FTSE100_df.info()\n","\n","    print(\"\\n--- NaNs in Processed DataFrame (should be 0) ---\")\n","    print(final_processed_FTSE100_df.isnull().sum().sum()) # Should be 0 NaNs if successful\n","else:\n","    print(\"Preprocessing returned an empty DataFrame.\")\n","\n","\n","\n","final_processed_FTSE250_df, fitted_scaler = preprocess_features_for_model(\n","    df_with_all_250_features.copy(), # Pass a copy to avoid modifying the original df_with_all_features\n","    columns_to_exclude_from_features=None, # Use default exclusion (removes original price/volume/weekday)\n","    apply_scaling=True,                     # Apply StandardScaler (best for all 3 models)\n","    scaler_obj=None                         # Fit a new scaler\n",")\n","if not final_processed_FTSE250_df.empty:\n","    print(\"\\n--- Processed DataFrame (Head) after fixing infs ---\")\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 1000)\n","    print(final_processed_FTSE100_df.head(15))\n","\n","    print(\"\\n--- Processed DataFrame (Info) after fixing infs ---\")\n","    final_processed_FTSE250_df.info()\n","\n","    print(\"\\n--- NaNs in Processed DataFrame (should be 0) ---\")\n","    print(final_processed_FTSE250_df.isnull().sum().sum()) # Should be 0 NaNs if successful\n","else:\n","    print(\"Preprocessing returned an empty DataFrame.\")"],"metadata":{"id":"W9dD2nPsraNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data visualization\n","\n","\n","def visualize_engineered_features(df: pd.DataFrame, num_sample_tickers: int = 3, num_top_features_corr: int = 15):\n","    \"\"\"\n","    Generates various visualizations for the engineered stock features.\n","\n","    Assumes input DataFrame has a MultiIndex (Date, Ticker) and contains\n","    numerical features (optionally scaled).\n","\n","    Args:\n","        df (pd.DataFrame): The DataFrame with engineered features, MultiIndex (Date, Ticker).\n","        num_sample_tickers (int): Number of random tickers to sample for time-series plots.\n","                                  Defaults to 3.\n","        num_top_features_corr (int): Number of top correlated features to display in the\n","                                     correlation heatmap. Defaults to 15.\n","    \"\"\"\n","    if df.empty:\n","        print(\"DataFrame is empty. No visualizations to generate.\")\n","        return\n","\n","    print(\"\\n--- Starting Data Visualization for Engineered Features ---\")\n","\n","    # Set plot style\n","    sns.set_style(\"whitegrid\")\n","    plt.rcParams['figure.figsize'] = (12, 6) # Default figure size\n","\n","    # --- 1. Overall DataFrame Information ---\n","    print(\"\\n1. DataFrame Overview:\")\n","    print(f\"Total entries: {df.shape[0]}\")\n","    print(f\"Number of features: {df.shape[1]}\")\n","    print(f\"Number of unique tickers: {df.index.get_level_values('Ticker').nunique()}\")\n","    print(f\"Date range: {df.index.get_level_values('Date').min().date()} to {df.index.get_level_values('Date').max().date()}\")\n","\n","    # --- 2. Distribution of Key Features ---\n","    print(\"\\n2. Distribution of Selected Key Features (Histograms/KDE):\")\n","    # Select a few representative features to visualize their distribution\n","    # Adjust these feature names based on your actual generated features\n","    key_features = [\n","        'log_return', 'rolling_std_20d_log_return', 'relative_volume',\n","        'deviation_from_daily_median_return', 'daily_return_rank_pct'\n","    ]\n","\n","    # Filter for features that actually exist in the DataFrame\n","    existing_key_features = [f for f in key_features if f in df.columns]\n","\n","    if existing_key_features:\n","        fig, axes = plt.subplots(len(existing_key_features), 1, figsize=(10, 4 * len(existing_key_features)))\n","        if len(existing_key_features) == 1: axes = [axes] # Ensure axes is iterable for single plot\n","\n","        for i, feature in enumerate(existing_key_features):\n","            sns.histplot(df[feature], kde=True, ax=axes[i], bins=50)\n","            axes[i].set_title(f'Distribution of {feature}')\n","            axes[i].set_xlabel(feature)\n","            axes[i].set_ylabel('Frequency')\n","        plt.tight_layout()\n","        plt.show()\n","    else:\n","        print(\"⚠️ No key features found to plot distributions. Ensure feature names are correct.\")\n","\n","    # --- 3. Time Series Plots for Sample Tickers ---\n","    print(f\"\\n3. Time Series Plots for {num_sample_tickers} Sample Tickers:\")\n","    unique_tickers = df.index.get_level_values('Ticker').unique()\n","    if len(unique_tickers) > num_sample_tickers:\n","        sample_tickers = np.random.choice(unique_tickers, num_sample_tickers, replace=False)\n","    else:\n","        sample_tickers = unique_tickers # Use all if fewer than requested samples\n","\n","    features_to_plot_ts = ['log_return', 'rolling_std_20d_log_return', 'relative_volume']\n","    existing_features_to_plot_ts = [f for f in features_to_plot_ts if f in df.columns]\n","\n","    if existing_features_to_plot_ts and sample_tickers.size > 0:\n","        for ticker in sample_tickers:\n","            ticker_df = df.loc[(slice(None), ticker), :] # Select all dates for this ticker\n","            ticker_df = ticker_df.droplevel('Ticker') # Drop Ticker level from index for cleaner plot\n","\n","            fig, axes = plt.subplots(len(existing_features_to_plot_ts), 1, figsize=(15, 3 * len(existing_features_to_plot_ts)), sharex=True)\n","            if len(existing_features_to_plot_ts) == 1: axes = [axes] # Ensure axes is iterable\n","\n","            for i, feature in enumerate(existing_features_to_plot_ts):\n","                if feature in ticker_df.columns:\n","                    axes[i].plot(ticker_df.index, ticker_df[feature], label=f'{ticker} - {feature}')\n","                    axes[i].set_title(f'{ticker} - {feature} over Time')\n","                    axes[i].set_ylabel(feature)\n","                    axes[i].legend()\n","            plt.tight_layout()\n","            plt.show()\n","    else:\n","        print(\"⚠️ Not enough data or features to plot time series for sample tickers.\")\n","\n","    # --- 4. Correlation Matrix of Features ---\n","    print(f\"\\n4. Correlation Matrix (Top {num_top_features_corr} Most Correlated Features):\")\n","    # Calculate the correlation matrix\n","    corr_matrix = df.corr()\n","\n","    # Find the top N most correlated features (by sum of absolute correlations)\n","    # Exclude self-correlation (diagonal) when summing\n","    np.fill_diagonal(corr_matrix.values, np.nan) # Temporarily set diagonal to NaN for sum\n","    sum_abs_corr = corr_matrix.abs().sum().sort_values(ascending=False)\n","\n","    # Restore diagonal for display\n","    np.fill_diagonal(corr_matrix.values, 1.0)\n","\n","    top_features = sum_abs_corr.head(num_top_features_corr).index.tolist()\n","\n","    if len(top_features) > 1:\n","        plt.figure(figsize=(num_top_features_corr * 0.7, num_top_features_corr * 0.7))\n","        sns.heatmap(corr_matrix.loc[top_features, top_features], annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n","        plt.title(f'Correlation Matrix of Top {num_top_features_corr} Features')\n","        plt.show()\n","    else:\n","        print(\"⚠️ Not enough features or no strong correlations to plot heatmap.\")\n","\n","    # --- 5. Box Plots of Features (Overall) ---\n","    print(\"\\n5. Box Plots of All Features:\")\n","    plt.figure(figsize=(15, 8))\n","    # It's better to melt the DataFrame for box plots if you want all features on one plot\n","    # Or plot each feature individually if the number of features is small\n","    df_melted = df.reset_index().melt(id_vars=['Date', 'Ticker'], var_name='Feature', value_name='Value')\n","\n","    # Plotting for numerical features only\n","    numerical_features_only = df_melted[df_melted['Feature'].isin(df.select_dtypes(include=np.number).columns)]\n","\n","    if not numerical_features_only.empty:\n","        sns.boxplot(data=numerical_features_only, x='Feature', y='Value', orient='v')\n","        plt.title('Box Plot of Engineered Features (Overall)')\n","        plt.xticks(rotation=90)\n","        plt.tight_layout()\n","        plt.show()\n","    else:\n","        print(\"⚠️ No numerical features to plot box plots.\")\n","\n","    # --- 6. Pair Plots (for selected features - can be very slow for many features) ---\n","    print(\"\\n6. Pair Plots for a subset of features (Warning: Can be very slow for many features/rows):\")\n","    # It's impractical to plot all features. Select a very small, representative subset.\n","    pair_plot_features = [\n","        'log_return', 'rolling_std_5d_log_return', 'daily_return_rank_pct'\n","    ]\n","    existing_pair_plot_features = [f for f in pair_plot_features if f in df.columns]\n","\n","    if len(existing_pair_plot_features) >= 2 and df.shape[0] < 5000: # Limit for performance\n","        print(f\"Plotting pair plots for: {', '.join(existing_pair_plot_features)}. This may take a while.\")\n","        sns.pairplot(df[existing_pair_plot_features].sample(min(500, df.shape[0]))) # Sample a subset for faster plotting\n","        plt.suptitle('Pair Plots of Selected Features', y=1.02)\n","        plt.show()\n","    elif len(existing_pair_plot_features) < 2:\n","        print(\"⚠️ Not enough selected features for pair plot.\")\n","    else:\n","        print(\"⚠️ Too many rows for pair plot (sampling for speed). Skipped pair plot due to potentially large data.\")\n","\n","\n","    print(\"\\n--- Data Visualization Complete ---\")\n","\n","# Call the visualization function\n","visualize_engineered_features(final_processed_FTSE100_df)"],"metadata":{"id":"4jQhYp7IKmZl","executionInfo":{"status":"ok","timestamp":1750199586127,"user_tz":-60,"elapsed":37,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# A function to split data into training and validation\n","\n","def perform_chronological_split(df: pd.DataFrame, split_date: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","    \"\"\"\n","    Splits the input DataFrame into training and evaluation sets based on a chronological date.\n","\n","    The training set will contain data up to and including the split_date.\n","    The evaluation set will contain data strictly after the split_date.\n","\n","    Args:\n","        df (pd.DataFrame): The input DataFrame with a MultiIndex (Date, Ticker),\n","                           and numerical features. This is your 'processed_features_df'.\n","        split_date (str): The date string in 'YYYY-MM-DD' format to use as the split point.\n","                          Data up to this date goes into the training set.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - pd.DataFrame: The training data (X_train_normal).\n","            - pd.DataFrame: The evaluation data (X_eval).\n","            Returns empty DataFrames if the input is empty or split is not possible.\n","    \"\"\"\n","    if df.empty:\n","        print(\"Input DataFrame is empty for chronological split. Returning empty DataFrames.\")\n","        return pd.DataFrame(), pd.DataFrame()\n","\n","    try:\n","        parsed_split_date = pd.to_datetime(split_date)\n","    except ValueError:\n","        print(f\"❌ Error: Invalid split_date format '{split_date}'. Please use 'YYYY-MM-DD'.\")\n","        return pd.DataFrame(), pd.DataFrame()\n","\n","    print(f\"\\n--- Performing Chronological Data Split at {parsed_split_date.date()} ---\")\n","\n","    # X_train_normal: Data from the beginning up to the split_date (inclusive)\n","    X_train_normal = df.loc[df.index.get_level_values('Date') <= parsed_split_date].copy()\n","\n","    # X_eval: Data strictly after the split_date\n","    X_eval = df.loc[df.index.get_level_values('Date') > parsed_split_date].copy()\n","\n","    print(f\"Data split complete:\")\n","    print(f\"  Training data (X_train_normal) shape: {X_train_normal.shape} (Dates up to {parsed_split_date.date()})\")\n","    print(f\"  Evaluation data (X_eval) shape: {X_eval.shape} (Dates after {parsed_split_date.date()})\")\n","\n","    # Basic verification of the split\n","    if not X_train_normal.empty and not X_eval.empty:\n","        max_train_date = X_train_normal.index.get_level_values('Date').max()\n","        min_eval_date = X_eval.index.get_level_values('Date').min()\n","        print(f\"  Verification: Max train date: {max_train_date.date()}, Min eval date: {min_eval_date.date()}\")\n","        if max_train_date >= parsed_split_date and min_eval_date <= parsed_split_date: # This condition might be too strict if split_date is not present in data\n","            pass # The check is more about ensuring non-overlap in the general sense.\n","    elif X_train_normal.empty:\n","        print(\"⚠️ Warning: Training data is empty after split. Check split_date or input data.\")\n","    elif X_eval.empty:\n","        print(\"⚠️ Warning: Evaluation data is empty after split. Check split_date or input data.\")\n","\n","\n","    return X_train_normal, X_eval\n"],"metadata":{"id":"dzKR4yIIAwlk","executionInfo":{"status":"ok","timestamp":1750209690799,"user_tz":-60,"elapsed":48,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Calling the function to split the data into training and testing\n","\n","# Define your desired chronological split point (e.g., end of 2020)\n","# A common split is 70-80% for training. For 2014-2024 data, 2020-12-31 is a good point.\n","my_split_date = \"2020-12-31\"\n","\n","# Call the function to perform the split on the FTSE 100 stocks\n","X1_train_normal, X1_eval = perform_chronological_split(final_processed_FTSE100_df, my_split_date)\n","\n","#Call the function to perform the split on the FTSE 250 stocks\n","X2_train_normal, X2_eval = perform_chronological_split(final_processed_FTSE250_df, my_split_date)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4n-YDY4hJBRx","executionInfo":{"status":"ok","timestamp":1750210572216,"user_tz":-60,"elapsed":41,"user":{"displayName":"dorothy sarpong","userId":"04133103870765976145"}},"outputId":"7397a6d6-7fb8-4b83-c09b-75f6ee5e5c5b"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Performing Chronological Data Split at 2020-12-31 ---\n","Data split complete:\n","  Training data (X_train_normal) shape: (26261, 21) (Dates up to 2020-12-31)\n","  Evaluation data (X_eval) shape: (15104, 21) (Dates after 2020-12-31)\n","  Verification: Max train date: 2020-12-31, Min eval date: 2021-01-04\n","\n","--- Performing Chronological Data Split at 2020-12-31 ---\n","Data split complete:\n","  Training data (X_train_normal) shape: (24060, 21) (Dates up to 2020-12-31)\n","  Evaluation data (X_eval) shape: (15104, 21) (Dates after 2020-12-31)\n","  Verification: Max train date: 2020-12-31, Min eval date: 2021-01-04\n"]}]}]}